{"metadata":{"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8613427,"sourceType":"datasetVersion","datasetId":5155031}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q clu","metadata":{"id":"gS6euWNvHFye","outputId":"45b149a7-9450-439c-da67-ab8678a3b0d0","execution":{"iopub.status.busy":"2024-06-10T06:23:14.289165Z","iopub.execute_input":"2024-06-10T06:23:14.289501Z","iopub.status.idle":"2024-06-10T06:23:31.117311Z","shell.execute_reply.started":"2024-06-10T06:23:14.289472Z","shell.execute_reply":"2024-06-10T06:23:31.116210Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.2.1 which is incompatible.\ntensorflow 2.15.0 requires ml-dtypes~=0.2.0, but you have ml-dtypes 0.4.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# # We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt","metadata":{"id":"7jjCLfuUHFyg","outputId":"dfe048f0-dd44-40ef-edf3-2fa56558672f","execution":{"iopub.status.busy":"2024-06-10T06:23:31.119204Z","iopub.execute_input":"2024-06-10T06:23:31.119507Z","iopub.status.idle":"2024-06-10T06:23:31.123820Z","shell.execute_reply.started":"2024-06-10T06:23:31.119477Z","shell.execute_reply":"2024-06-10T06:23:31.122834Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from functools import partial\nimport jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\nfrom jax.nn.initializers import lecun_normal, normal\nfrom jax.numpy.linalg import eigh, inv, matrix_power\nfrom jax.scipy.signal import convolve\n\nimport torch\n\nfrom dataclasses import dataclass\n\nfrom typing import Union\n\nimport matplotlib.pyplot as plt\nimport seaborn\n\nfrom clu import metrics\nfrom flax.training import train_state  # Useful dataclass to keep train state\nfrom flax import struct                # Flax dataclasses\nimport optax                           # Common loss functions and optimizers\nfrom tqdm import tqdm","metadata":{"id":"YXSCJzupHFyh","execution":{"iopub.status.busy":"2024-06-10T06:23:31.125106Z","iopub.execute_input":"2024-06-10T06:23:31.125463Z","iopub.status.idle":"2024-06-10T06:23:36.886077Z","shell.execute_reply.started":"2024-06-10T06:23:31.125430Z","shell.execute_reply":"2024-06-10T06:23:36.885234Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# read it in to inspect it\nwith open('/kaggle/input/shak-new-input/input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()","metadata":{"id":"KpJoV3KQHFyh","execution":{"iopub.status.busy":"2024-06-10T06:23:36.888489Z","iopub.execute_input":"2024-06-10T06:23:36.889315Z","iopub.status.idle":"2024-06-10T06:23:36.924950Z","shell.execute_reply.started":"2024-06-10T06:23:36.889278Z","shell.execute_reply":"2024-06-10T06:23:36.924108Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)","metadata":{"id":"PsWxZqyRHFyi","outputId":"b1730724-647e-45cd-edfa-97af24995830","execution":{"iopub.status.busy":"2024-06-10T06:23:36.926219Z","iopub.execute_input":"2024-06-10T06:23:36.926605Z","iopub.status.idle":"2024-06-10T06:23:36.974786Z","shell.execute_reply.started":"2024-06-10T06:23:36.926569Z","shell.execute_reply":"2024-06-10T06:23:36.973829Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"\n !\"&',-.:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n64\n","output_type":"stream"}]},{"cell_type":"code","source":"# from transformers import AutoTokenizer\n\n# tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Phi-3-mini-4k-instruct\", padding_side=\"left\")","metadata":{"execution":{"iopub.status.busy":"2024-06-10T06:23:36.976383Z","iopub.execute_input":"2024-06-10T06:23:36.977038Z","iopub.status.idle":"2024-06-10T06:23:36.983916Z","shell.execute_reply.started":"2024-06-10T06:23:36.977000Z","shell.execute_reply":"2024-06-10T06:23:36.982851Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# text_inputs = tokenizer(text, return_tensors=\"np\")\n# data = jnp.array(text_inputs['input_ids'][0])","metadata":{"execution":{"iopub.status.busy":"2024-06-10T06:23:36.985127Z","iopub.execute_input":"2024-06-10T06:23:36.985715Z","iopub.status.idle":"2024-06-10T06:23:36.994340Z","shell.execute_reply.started":"2024-06-10T06:23:36.985680Z","shell.execute_reply":"2024-06-10T06:23:36.993267Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# vocab_size = tokenizer.vocab_size\n# print(vocab_size)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T06:23:36.996281Z","iopub.execute_input":"2024-06-10T06:23:36.996676Z","iopub.status.idle":"2024-06-10T06:23:37.003916Z","shell.execute_reply.started":"2024-06-10T06:23:36.996642Z","shell.execute_reply":"2024-06-10T06:23:37.002969Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# print(tokenizer.decode((text_inputs['input_ids'][0][0:100]).tolist()))","metadata":{"execution":{"iopub.status.busy":"2024-06-10T06:23:37.005172Z","iopub.execute_input":"2024-06-10T06:23:37.005530Z","iopub.status.idle":"2024-06-10T06:23:37.013048Z","shell.execute_reply.started":"2024-06-10T06:23:37.005497Z","shell.execute_reply":"2024-06-10T06:23:37.012319Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# create a mapping from characters to integers\nstoi = { ch: i for i,ch in enumerate(chars) }\nitos = { i: ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))","metadata":{"id":"S-mzLOk1HFyi","outputId":"f56e2f85-5a1c-4099-87df-436ba39f4363","execution":{"iopub.status.busy":"2024-06-10T06:23:37.016611Z","iopub.execute_input":"2024-06-10T06:23:37.016851Z","iopub.status.idle":"2024-06-10T06:23:37.024665Z","shell.execute_reply.started":"2024-06-10T06:23:37.016830Z","shell.execute_reply":"2024-06-10T06:23:37.023824Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"[45, 46, 46, 1, 57, 45, 42, 55, 42]\nhii there\n","output_type":"stream"}]},{"cell_type":"code","source":"data = jnp.array(encode(text), dtype=jnp.int32)\nprint(data.shape, data.dtype)\nprint(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this","metadata":{"id":"HImuqDd8HFyj","outputId":"91dcd15f-f068-4551-ad29-e6e41e52fd91","execution":{"iopub.status.busy":"2024-06-10T06:23:37.025797Z","iopub.execute_input":"2024-06-10T06:23:37.026122Z","iopub.status.idle":"2024-06-10T06:23:42.248934Z","shell.execute_reply.started":"2024-06-10T06:23:37.026098Z","shell.execute_reply":"2024-06-10T06:23:42.247970Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"(2643247,) int32\n[17 46 55 56 57  1 14 46 57 46 63 42 51  9  0 13 42 43 52 55 42  1 60 42\n  1 53 55 52 40 42 42 41  1 38 51 62  1 43 58 55 57 45 42 55  6  1 45 42\n 38 55  1 50 42  1 56 53 42 38 48  8  0  0 12 49 49  9  0 30 53 42 38 48\n  6  1 56 53 42 38 48  8  0  0 17 46 55 56 57  1 14 46 57 46 63 42 51  9\n  0 36 52 58  1 38 55 42  1 38 49 49  1 55 42 56 52 49 59 42 41  1 55 38\n 57 45 42 55  1 57 52  1 41 46 42  1 57 45 38 51  1 57 52  1 43 38 50 46\n 56 45 11  0  0 12 49 49  9  0 29 42 56 52 49 59 42 41  8  1 55 42 56 52\n 49 59 42 41  8  0  0 17 46 55 56 57  1 14 46 57 46 63 42 51  9  0 17 46\n 55 56 57  6  1 62 52 58  1 48 51 52 60  1 14 38 46 58 56  1 24 38 55 40\n 46 58 56  1 46 56  1 40 45 46 42 43  1 42 51 42 50 62  1 57 52  1 57 45\n 42  1 53 42 52 53 49 42  8  0  0 12 49 49  9  0 34 42  1 48 51 52 60  5\n 57  6  1 60 42  1 48 51 52 60  5 57  8  0  0 17 46 55 56 57  1 14 46 57\n 46 63 42 51  9  0 23 42 57  1 58 56  1 48 46 49 49  1 45 46 50  6  1 38\n 51 41  1 60 42  5 49 49  1 45 38 59 42  1 40 52 55 51  1 38 57  1 52 58\n 55  1 52 60 51  1 53 55 46 40 42  8  0 20 56  5 57  1 38  1 59 42 55 41\n 46 40 57 11  0  0 12 49 49  9  0 25 52  1 50 52 55 42  1 57 38 49 48 46\n 51 44  1 52 51  5 57 10  1 49 42 57  1 46 57  1 39 42  1 41 52 51 42  9\n  1 38 60 38 62  6  1 38 60 38 62  2  0  0 30 42 40 52 51 41  1 14 46 57\n 46 63 42 51  9  0 26 51 42  1 60 52 55 41  6  1 44 52 52 41  1 40 46 57\n 46 63 42 51 56  8  0  0 17 46 55 56 57  1 14 46 57 46 63 42 51  9  0 34\n 42  1 38 55 42  1 38 40 40 52 58 51 57 42 41  1 53 52 52 55  1 40 46 57\n 46 63 42 51 56  6  1 57 45 42  1 53 38 57 55 46 40 46 38 51 56  1 44 52\n 52 41  8  0 34 45 38 57  1 38 58 57 45 52 55 46 57 62  1 56 58 55 43 42\n 46 57 56  1 52 51  1 60 52 58 49 41  1 55 42 49 46 42 59 42  1 58 56  9\n  1 46 43  1 57 45 42 62  0 60 52 58 49 41  1 62 46 42 49 41  1 58 56  1\n 39 58 57  1 57 45 42  1 56 58 53 42 55 43 49 58 46 57 62  6  1 60 45 46\n 49 42  1 46 57  1 60 42 55 42  0 60 45 52 49 42 56 52 50 42  6  1 60 42\n  1 50 46 44 45 57  1 44 58 42 56 56  1 57 45 42 62  1 55 42 49 46 42 59\n 42 41  1 58 56  1 45 58 50 38 51 42 49 62 10  0 39 58 57  1 57 45 42 62\n  1 57 45 46 51 48  1 60 42  1 38 55 42  1 57 52 52  1 41 42 38 55  9  1\n 57 45 42  1 49 42 38 51 51 42 56 56  1 57 45 38 57  0 38 43 43 49 46 40\n 57 56  1 58 56  6  1 57 45 42  1 52 39 47 42 40 57  1 52 43  1 52 58 55\n  1 50 46 56 42 55 62  6  1 46 56  1 38 56  1 38 51  0 46 51 59 42 51 57\n 52 55 62  1 57 52  1 53 38 55 57 46 40 58 49 38 55 46 56 42  1 57 45 42\n 46 55  1 38 39 58 51 41 38 51 40 42 10  1 52 58 55  0 56 58 43 43 42 55\n 38 51 40 42  1 46 56  1 38  1 44 38 46 51  1 57 52  1 57 45 42 50  1 23\n 42 57  1 58 56  1 55 42 59 42 51 44 42  1 57 45 46 56  1 60 46 57 45  0\n 52 58 55  1 53 46 48 42 56  6  1 42 55 42  1 60 42  1 39 42 40 52 50 42\n  1 55 38 48 42 56  9  1 43 52 55  1 57 45 42  1 44 52 41 56  1 48 51 52\n 60  1 20  0 56 53 42 38 48  1 57 45 46 56  1 46 51  1 45 58 51 44 42 55\n  1 43 52 55  1 39 55 42 38 41  6  1 51 52 57  1 46 51  1 57 45 46 55 56\n 57  1 43 52 55  1 55 42 59 42 51 44 42  8  0  0]\n","output_type":"stream"}]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2024-06-10T06:23:42.250833Z","iopub.execute_input":"2024-06-10T06:23:42.251150Z","iopub.status.idle":"2024-06-10T06:23:42.260314Z","shell.execute_reply.started":"2024-06-10T06:23:42.251122Z","shell.execute_reply":"2024-06-10T06:23:42.259414Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"Array([17, 46, 55, ..., 38, 62,  8], dtype=int32)"},"metadata":{}}]},{"cell_type":"code","source":"len(data)/64/32","metadata":{"execution":{"iopub.status.busy":"2024-06-10T06:23:42.261508Z","iopub.execute_input":"2024-06-10T06:23:42.261795Z","iopub.status.idle":"2024-06-10T06:23:42.268885Z","shell.execute_reply.started":"2024-06-10T06:23:42.261769Z","shell.execute_reply":"2024-06-10T06:23:42.268032Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"1290.64794921875"},"metadata":{}}]},{"cell_type":"code","source":"# train_test_split = 0.9\n# n = int(train_test_split*len(data))\n# train_data = data[:n]\n# test_data = data[n:]\n\ntrain_test_split = 0.9\nn = int(train_test_split*len(data))\ntrain_data = data[:n]\ntest_data = data[n:]","metadata":{"id":"pXrAqMxRHFyj","execution":{"iopub.status.busy":"2024-06-10T06:23:42.269952Z","iopub.execute_input":"2024-06-10T06:23:42.270277Z","iopub.status.idle":"2024-06-10T06:23:42.432963Z","shell.execute_reply.started":"2024-06-10T06:23:42.270249Z","shell.execute_reply":"2024-06-10T06:23:42.432087Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"block_size = 8\ntrain_data[:block_size+1]","metadata":{"id":"ahhKyiAzHFyj","outputId":"98306c96-5082-4dfa-ba66-915051831fc8","execution":{"iopub.status.busy":"2024-06-10T06:23:42.434153Z","iopub.execute_input":"2024-06-10T06:23:42.434512Z","iopub.status.idle":"2024-06-10T06:23:42.513905Z","shell.execute_reply.started":"2024-06-10T06:23:42.434478Z","shell.execute_reply":"2024-06-10T06:23:42.513000Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"Array([17, 46, 55, 56, 57,  1, 14, 46, 57], dtype=int32)"},"metadata":{}}]},{"cell_type":"code","source":"x = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")","metadata":{"id":"HIpsznQmHFyk","outputId":"be9d197b-0b79-43ed-f3a9-e74295d51c79","execution":{"iopub.status.busy":"2024-06-10T06:23:42.515208Z","iopub.execute_input":"2024-06-10T06:23:42.515552Z","iopub.status.idle":"2024-06-10T06:23:43.165077Z","shell.execute_reply.started":"2024-06-10T06:23:42.515523Z","shell.execute_reply":"2024-06-10T06:23:43.164106Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"when input is [17] the target: 46\nwhen input is [17 46] the target: 55\nwhen input is [17 46 55] the target: 56\nwhen input is [17 46 55 56] the target: 57\nwhen input is [17 46 55 56 57] the target: 1\nwhen input is [17 46 55 56 57  1] the target: 14\nwhen input is [17 46 55 56 57  1 14] the target: 46\nwhen input is [17 46 55 56 57  1 14 46] the target: 57\n","output_type":"stream"}]},{"cell_type":"code","source":"batch_size = 128 # how many independent sequences will we process in parallel?\nblock_size = 64 # what is the maximum context length for predictions?\nmax_iters = 15000\nlearning_rate = 5e-4\n# device = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 100\nn_embd = 256\nexpans = 2\nn_heads = 1\nchannel_size = n_embd // n_heads\nn_layers = 6\ndropout = 0.2\nconv_k_size = 3\nn_latent_dim = 16\nn_tokens = 1\n\nrng_key = jax.random.PRNGKey(1564)\n\ndynamic_slice_vmap = jax.vmap(jax.lax.dynamic_slice, in_axes=(None, 0, None))\n\n@jax.jit\ndef get_batch(random_key, data):\n    \"\"\"Prepares a random batch of training data.\n\n    Args:\n      random_key: A random seed for sampling a batch.\n      data: The complete training dataset.\n\n    Returns:\n      x: Input sequences.\n      y: Target sequences (shifted inputs).\n    \"\"\"\n    ix = jax.random.randint(\n      random_key, shape=(batch_size, 1), minval=0, maxval=len(data) - block_size\n    )\n    x = dynamic_slice_vmap(data, ix, (block_size,))\n    y = dynamic_slice_vmap(data, ix + n_tokens, (block_size,))\n    return x, y\n\nxb, yb = get_batch(rng_key, train_data)\ntrain_shape = xb.shape\nprint('inputs:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\n# print('----')\n\n# for b in range(batch_size): # batch dimension\n#     for t in range(block_size): # time dimension\n#         context = xb[b, :t+1]\n#         target = yb[b,t]\n#         print(f\"when input is {context} the target: {target}\")","metadata":{"id":"UuAjtqPeHFyk","outputId":"6a88fb2b-b798-4ee9-9f4f-f38ce898d576","execution":{"iopub.status.busy":"2024-06-10T06:23:43.166306Z","iopub.execute_input":"2024-06-10T06:23:43.166653Z","iopub.status.idle":"2024-06-10T06:23:43.527400Z","shell.execute_reply.started":"2024-06-10T06:23:43.166612Z","shell.execute_reply":"2024-06-10T06:23:43.526456Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"inputs:\n(128, 64)\n[[31 16 30 ... 20 25 14]\n [44 43 58 ...  1 38 44]\n [42 56  6 ... 52 55 56]\n ...\n [ 6  1 39 ... 25 52 57]\n [46 41 46 ... 56 60 52]\n [57 52  1 ... 49 52 55]]\ntargets:\n(128, 64)\n[[16 30 30 ... 25 14 16]\n [43 58 49 ... 38 44 38]\n [56  6  1 ... 55 56 42]\n ...\n [ 1 39 42 ... 52 57 45]\n [41 46 58 ... 60 52 55]\n [52  1 50 ... 52 55 41]]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(xb[0])\nprint(yb[0])","metadata":{"execution":{"iopub.status.busy":"2024-06-10T06:23:43.528636Z","iopub.execute_input":"2024-06-10T06:23:43.528933Z","iopub.status.idle":"2024-06-10T06:23:43.628537Z","shell.execute_reply.started":"2024-06-10T06:23:43.528906Z","shell.execute_reply":"2024-06-10T06:23:43.627623Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"[31 16 30 30  9  0 34 45 52  1 48 51 52 40 48 56  1 56 52  1 49 52 58 41\n  1 38 57  1 41 52 52 55 11  1 23 52 52 48  1 57 52  1 57 45  5  1 41 52\n 52 55  1 57 45 42 55 42  6  0  0 27 29 20 25 14]\n[16 30 30  9  0 34 45 52  1 48 51 52 40 48 56  1 56 52  1 49 52 58 41  1\n 38 57  1 41 52 52 55 11  1 23 52 52 48  1 57 52  1 57 45  5  1 41 52 52\n 55  1 57 45 42 55 42  6  0  0 27 29 20 25 14 16]\n","output_type":"stream"}]},{"cell_type":"code","source":"# hidden_state = [jnp.zeros((1,n_latent_dim, n_embd * expans)) for _ in range(n_layers)]\n# hidden_state[0].shape","metadata":{"execution":{"iopub.status.busy":"2024-06-10T06:23:43.629813Z","iopub.execute_input":"2024-06-10T06:23:43.630539Z","iopub.status.idle":"2024-06-10T06:23:43.635119Z","shell.execute_reply.started":"2024-06-10T06:23:43.630502Z","shell.execute_reply":"2024-06-10T06:23:43.634191Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# Mamba Block\nDense --> Conv1D --> Silu --> SSM --> Silu -->","metadata":{"id":"yOccqzJlHFym"}},{"cell_type":"code","source":"class Mamba(nn.Module):\n\n    def setup(self):\n        emb_features = n_embd * expans\n        self.in_proj1 = nn.Dense(features=emb_features)\n        self.in_proj2 = nn.Dense(features=emb_features)\n\n        # Adjusted for Flax. Flax does not have nn.Conv1d, so you might need to reshape or use a different approach\n        self.conv1d = nn.Conv(features=emb_features,\n                              kernel_size=conv_k_size,\n                              padding=1,\n                              )\n\n        self.A = -1*self.param('A', nn.initializers.ones, (1, n_latent_dim, emb_features, 1))\n        self.B = 0.1*self.param('B', nn.initializers.ones, (1, n_latent_dim, 1, block_size))\n        self.C = 0.09*self.param('C', jax.random.normal, (1, n_latent_dim, 1, block_size))\n        self.D = 0.1*self.param('D', jax.random.normal, (1, 1,emb_features, block_size))\n        self.delta = 0.05*self.param('delta', jax.random.normal, (1, 1,emb_features, block_size))\n\n        self.out_proj = nn.Dense(n_embd // n_heads)\n        \n        self.hidden_state = self.variable('other_variables','hidden_state', \n                                          jnp.zeros, \n                                          (1,n_latent_dim, emb_features))\n#         self.rms_norm = nn.RMSNorm()\n\n    def __call__(self, embeds):\n        x = self.in_proj1(embeds)\n        x = self.conv1d(x)\n        x = jax.nn.silu(x)\n        x = x.reshape((x.shape[0],1,x.shape[2],x.shape[1]))\n        x = self.ssm(x)\n        x = x.reshape((x.shape[0],x.shape[3],x.shape[2]))\n        x = x*jax.nn.silu(self.in_proj2(embeds))\n\n        x = self.out_proj(x)\n\n#         x = self.rms_norm(x)\n\n        return x\n    def discretize(self):\n        da = self.delta * self.A\n        a_ = jnp.exp(da)\n        b_ = self.B * self.delta\n        return a_, b_\n\n    def ssm(self, x):\n        y = []\n        a_, b_ = self.discretize()\n        h = 0\n        for k in range(x.shape[-1]):\n            h = a_[..., k] * h + b_[..., k] * x[..., k]\n            \n#         for l in range(x.shape[-1]):\n#             print(self.C.shape, h.shape)\n\n        y = ((self.C * jax.lax.expand_dims(h,[3])).sum(1, keepdims=True) + self.D*x)\n        \n#         self.hidden_state.value = jax.nn.standardize(h.mean(0, keepdims=True))\n        return y","metadata":{"id":"4qOdblU5HFyo","execution":{"iopub.status.busy":"2024-06-10T06:23:43.636347Z","iopub.execute_input":"2024-06-10T06:23:43.636652Z","iopub.status.idle":"2024-06-10T06:23:43.653422Z","shell.execute_reply.started":"2024-06-10T06:23:43.636625Z","shell.execute_reply":"2024-06-10T06:23:43.652445Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"class MultiHeadMamba(nn.Module):\n    def setup(self):\n        self.heads = [Mamba() for _ in range(n_heads)]\n        self.rms_norm = nn.RMSNorm()\n\n    def __call__(self, x):\n        out = jnp.concatenate([h(x) for h in self.heads], axis=-1)\n        x = self.rms_norm(out)\n        return x","metadata":{"id":"0bH9vlLZHFyq","execution":{"iopub.status.busy":"2024-06-10T06:23:43.654566Z","iopub.execute_input":"2024-06-10T06:23:43.654900Z","iopub.status.idle":"2024-06-10T06:23:43.667956Z","shell.execute_reply.started":"2024-06-10T06:23:43.654868Z","shell.execute_reply":"2024-06-10T06:23:43.667102Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# class FeedForward(nn.Module):\n#     def setup(self):\n#         self.ffn = nn.Sequential([\n#             nn.Dense(4 * n_embd),\n#             nn.relu,\n#             nn.Dense(n_embd)]\n#         )\n#     def __call__(self, x):\n#         return self.ffn(x)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T06:23:43.669046Z","iopub.execute_input":"2024-06-10T06:23:43.669395Z","iopub.status.idle":"2024-06-10T06:23:43.680094Z","shell.execute_reply.started":"2024-06-10T06:23:43.669363Z","shell.execute_reply":"2024-06-10T06:23:43.679169Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# class MambaBlock(nn.Module):\n#     def setup(self):\n#         self.mamba_block = Mamba()\n#         self.ln1 = nn.RMSNorm()\n#         self.ffn = FeedForward()\n#         self.ln2 = nn.LayerNorm()\n\n#     def __call__(self, x):\n#         x = x + self.mamba_block(self.ln2(x))\n#         x = x + self.ffn(self.ln1(x))\n#         return x\n","metadata":{"id":"UiCxIjoEp2QA","execution":{"iopub.status.busy":"2024-06-10T06:23:43.681223Z","iopub.execute_input":"2024-06-10T06:23:43.681495Z","iopub.status.idle":"2024-06-10T06:23:43.690334Z","shell.execute_reply.started":"2024-06-10T06:23:43.681471Z","shell.execute_reply":"2024-06-10T06:23:43.689462Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# class MambaModel(nn.Module):\n\n#     def setup(self):\n#         self.tok_embeddings = nn.Embed(vocab_size, n_embd)\n#         self.pos_embeddings = nn.Embed(block_size, n_embd)\n#         self.ln = nn.LayerNorm()\n#         self.mamba_layers = [MambaBlock() for _ in range(n_layers)]\n#         self.preds_out = nn.Dense(vocab_size)\n\n#     def __call__(self, x, training: bool):\n#         x = self.tok_embeddings(x) + self.pos_embeddings(jnp.arange(block_size))\n# #         x = self.ln(x)\n#         for layer in self.mamba_layers:\n#             x = layer(x)\n            \n#         return self.preds_out(x)\n\n#     @jax.jit\n#     def generate(self, idx, max_new_tokens, params):\n#     # idx is (B, T) array of indices in the current context\n#         for _ in range(max_new_tokens):\n#             # crop idx to the last block_size tokens\n#             idx_cond = idx[:, -block_size:]\n#             # get the predictions\n#             logits = self.apply(params, idx_cond)\n#             # focus only on the last time step\n#             logits = logits[:, -1, :] # becomes (B, C)\n#             # apply softmax to get probabilities\n#             ##probs = tf.keras.activations.softmax(logits, dim=-1) # (B, C)\n#             # sample from the distribution\n#             idx_next = jax.random.categorical(jax.random.PRNGKey(52), logits) # (B, 1)\n#             # append sampled index to the running sequence\n#             idx = jax.numpy.expand_dims(jnp.concatenate([idx[0], idx_next], axis=0), 0) # (B, T+1)\n#     #         print(idx_next)\n#     #         print(idx)\n\n#         return idx","metadata":{"id":"y4C7OWL8HFyq","execution":{"iopub.status.busy":"2024-06-10T06:23:43.691426Z","iopub.execute_input":"2024-06-10T06:23:43.691730Z","iopub.status.idle":"2024-06-10T06:23:43.700817Z","shell.execute_reply.started":"2024-06-10T06:23:43.691706Z","shell.execute_reply":"2024-06-10T06:23:43.699894Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# model = Mamba()\n# params = model.init(jax.random.key(42), jnp.ones((1,64,256)))\n# # print(params['other_variables']['hidden_state'].shape, params['other_variables']['hidden_state'].min(), params['other_variables']['hidden_state'].max())\n# # print(model.tabulate(jax.random.key(0), jnp.ones((1,64,256)),\n# #                    compute_flops=True, compute_vjp_flops=True))\n# xs = model.apply(params, jnp.ones((1,64,256)), mutable=['other_variables'])\n# # # print(params['other_variables']['hidden_state'].shape, params['other_variables']['hidden_state'].min(), params['other_variables']['hidden_state'].max())\n# xb.shape, xs[0].shape, xs[1].keys()","metadata":{"id":"wTd3jSQWHFyp","execution":{"iopub.status.busy":"2024-06-10T06:23:43.701975Z","iopub.execute_input":"2024-06-10T06:23:43.702373Z","iopub.status.idle":"2024-06-10T06:23:43.713823Z","shell.execute_reply.started":"2024-06-10T06:23:43.702341Z","shell.execute_reply":"2024-06-10T06:23:43.712970Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# print(xs[1]['other_variables']['hidden_state'].shape, xs[1]['other_variables']['hidden_state'].min(), xs[1]['other_variables']['hidden_state'].max())","metadata":{"execution":{"iopub.status.busy":"2024-06-10T06:23:43.714950Z","iopub.execute_input":"2024-06-10T06:23:43.715355Z","iopub.status.idle":"2024-06-10T06:23:43.727445Z","shell.execute_reply.started":"2024-06-10T06:23:43.715320Z","shell.execute_reply":"2024-06-10T06:23:43.726523Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# xfs = model.apply(params, 2*jnp.ones((1,64,256)), mutable=['other_variables'])\n# print(params['other_variables']['hidden_state'].shape, params['other_variables']['hidden_state'].min(), params['other_variables']['hidden_state'].max())\n# print(xfs[1]['other_variables']['hidden_state'].shape, xfs[1]['other_variables']['hidden_state'].min(), xfs[1]['other_variables']['hidden_state'].max())","metadata":{"execution":{"iopub.status.busy":"2024-06-10T06:23:43.728524Z","iopub.execute_input":"2024-06-10T06:23:43.728840Z","iopub.status.idle":"2024-06-10T06:23:43.737077Z","shell.execute_reply.started":"2024-06-10T06:23:43.728816Z","shell.execute_reply":"2024-06-10T06:23:43.736175Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# test_model = Mamba()\n# test_params = test_model.init(jax.random.key(42), xb)\n# n_params = sum(p.size for p in jax.tree_util.tree_leaves(test_params))\n# print(f\"Total number of parameters: {n_params:_}\")\n# # print(fin_model.tabulate(jax.random.key(42), xb,\n# #                    compute_flops=True, compute_vjp_flops=True))\n# xf = test_model.apply(test_params, xb)\n# xb.shape, xf.shape","metadata":{"id":"cm2a0nepHFyq","execution":{"iopub.status.busy":"2024-06-10T06:23:43.745575Z","iopub.execute_input":"2024-06-10T06:23:43.745822Z","iopub.status.idle":"2024-06-10T06:23:43.749823Z","shell.execute_reply.started":"2024-06-10T06:23:43.745800Z","shell.execute_reply":"2024-06-10T06:23:43.748952Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"class NanoLM(nn.Module):\n    \"\"\"NanoLM model.\"\"\"\n    vocab_size: int = 65\n    num_layers: int = 6\n    num_heads: int = 8\n    head_size: int = 32\n    dropout_rate: float = 0.2\n    embed_size: int = 256\n    block_size: int = 64\n\n    @nn.compact\n    def __call__(self, x, training: bool):\n        x = nn.Embed(self.vocab_size, self.embed_size)(x) + nn.Embed(\n            self.block_size, self.embed_size\n        )(jnp.arange(self.block_size))\n        \n        for i in range(self.num_layers):\n#             x = x + nn.MultiHeadDotProductAttention(\n#               num_heads=self.num_heads,\n#               qkv_features=self.head_size,\n#               out_features=self.head_size * self.num_heads,\n#               dropout_rate=self.dropout_rate,\n#             )(\n#               x_norm,\n#               x_norm,\n#               mask=jnp.tril(jnp.ones((x.shape[-2], x.shape[-2]))),\n#               deterministic=not training,\n#             )\n    \n            x = x + MultiHeadMamba()(nn.RMSNorm()(x))\n\n#             x = x + nn.Sequential([\n#               nn.Dense(4 * self.embed_size),\n#               nn.relu,\n#               nn.Dropout(self.dropout_rate, deterministic=not training),\n#               nn.Dense(self.embed_size),\n#             ])(nn.LayerNorm()(x))\n\n        x = nn.Dense(self.vocab_size)(nn.RMSNorm()(x))\n        return x","metadata":{"id":"zuiaFP6WHFyr","execution":{"iopub.status.busy":"2024-06-10T06:23:43.750865Z","iopub.execute_input":"2024-06-10T06:23:43.751135Z","iopub.status.idle":"2024-06-10T06:23:43.761529Z","shell.execute_reply.started":"2024-06-10T06:23:43.751112Z","shell.execute_reply":"2024-06-10T06:23:43.760635Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# key = jax.random.key(42)\n\n# # fin_model = MambaModel()\n# # fin_params = fin_model.init(key, xb, training=False)\n\n\n# fin_model = NanoLM(\n#     vocab_size=vocab_size,\n#     num_layers=n_layers,\n#     num_heads=8,\n#     head_size=32,\n#     dropout_rate=0.2,\n#     embed_size=n_embd,\n#     block_size=block_size,\n# )\n\n# fin_params = fin_model.init(\n#     {'params': key},\n#     jnp.ones((batch_size, block_size), dtype=jnp.int32),\n#     training=False\n# )\n\n# n_params = sum(p.size for p in jax.tree_util.tree_leaves(fin_params))\n# print(f\"Total number of parameters: {n_params:_}\")\n# # print(fin_model.tabulate(jax.random.key(42), xb,\n# #                    compute_flops=True, compute_vjp_flops=True))\n# xf = fin_model.apply(fin_params, xb, training=False)[0]\n# xb.shape, xf.shape","metadata":{"id":"fnUQPyuvHFys","outputId":"f04ebf31-d67f-4488-dd5d-7fd5b20dd1ea","execution":{"iopub.status.busy":"2024-06-10T06:23:43.762683Z","iopub.execute_input":"2024-06-10T06:23:43.763111Z","iopub.status.idle":"2024-06-10T06:23:43.774812Z","shell.execute_reply.started":"2024-06-10T06:23:43.763049Z","shell.execute_reply":"2024-06-10T06:23:43.774099Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"def loss_fun(params, x, y, var_params,dropout_key):\n    logits, updated_variables = model.apply({'params': params, **var_params}, x, training=True, rngs={\"dropout\": dropout_key}, mutable=['other_variables'])\n    accuracy = jnp.mean(jnp.argmax(logits, axis=-1) == y)\n    return optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=y).mean(), (updated_variables, accuracy)\n\n@jax.jit\ndef eval_step(params, x, y, var_params):\n    logits, _ = model.apply({'params': params, **var_params}, x, training=False, mutable=['other_variables'])\n    accuracy = jnp.mean(jnp.argmax(logits, axis=-1) == y)\n    return optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=y).mean(), accuracy","metadata":{"execution":{"iopub.status.busy":"2024-06-10T06:23:43.775977Z","iopub.execute_input":"2024-06-10T06:23:43.776362Z","iopub.status.idle":"2024-06-10T06:23:43.788432Z","shell.execute_reply.started":"2024-06-10T06:23:43.776329Z","shell.execute_reply":"2024-06-10T06:23:43.787538Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"key = jax.random.PRNGKey(42)\nkey, subkey = jax.random.split(key)\n\nmodel = NanoLM(\n    vocab_size=vocab_size,\n    num_layers=n_layers,\n    num_heads=8,\n    head_size=32,\n    dropout_rate=0.2,\n    embed_size=n_embd,\n    block_size=block_size,\n)\n\nvar_params = model.init(\n    key,\n    jnp.ones((batch_size, block_size), dtype=jnp.int32),\n    training=False,\n)\nprint(var_params.keys())\nn_params = sum(p.size for p in jax.tree_util.tree_leaves(var_params))\n\nprint(f\"Total number of parameters: {n_params:_}\")","metadata":{"id":"PKpb3864HFyt","execution":{"iopub.status.busy":"2024-06-10T06:23:43.789544Z","iopub.execute_input":"2024-06-10T06:23:43.789854Z","iopub.status.idle":"2024-06-10T06:23:51.892704Z","shell.execute_reply.started":"2024-06-10T06:23:43.789817Z","shell.execute_reply":"2024-06-10T06:23:51.891734Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"dict_keys(['params', 'other_variables'])\nTotal number of parameters: 7_644_992\n","output_type":"stream"}]},{"cell_type":"code","source":"var_params['params']['Embed_0']['embedding'].shape","metadata":{"execution":{"iopub.status.busy":"2024-06-10T06:23:51.893800Z","iopub.execute_input":"2024-06-10T06:23:51.894093Z","iopub.status.idle":"2024-06-10T06:23:51.900697Z","shell.execute_reply.started":"2024-06-10T06:23:51.894059Z","shell.execute_reply":"2024-06-10T06:23:51.899465Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"(64, 256)"},"metadata":{}}]},{"cell_type":"code","source":"params = var_params.pop('params')","metadata":{"execution":{"iopub.status.busy":"2024-06-10T06:23:51.901809Z","iopub.execute_input":"2024-06-10T06:23:51.902095Z","iopub.status.idle":"2024-06-10T06:23:51.922178Z","shell.execute_reply.started":"2024-06-10T06:23:51.902063Z","shell.execute_reply":"2024-06-10T06:23:51.921368Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"var_params = jax.tree_map(lambda x: jnp.zeros_like(x), var_params)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T06:23:51.923274Z","iopub.execute_input":"2024-06-10T06:23:51.923573Z","iopub.status.idle":"2024-06-10T06:23:51.935232Z","shell.execute_reply.started":"2024-06-10T06:23:51.923543Z","shell.execute_reply":"2024-06-10T06:23:51.934436Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# decay_rate = 0.96\n# learning_rate_schedule = optax.exponential_decay(learning_rate, decay_rate, max_iters//1000)\nopt = optax.adamw(learning_rate=learning_rate)\n\nopt_state = opt.init(params)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T06:23:51.936279Z","iopub.execute_input":"2024-06-10T06:23:51.936667Z","iopub.status.idle":"2024-06-10T06:23:52.230704Z","shell.execute_reply.started":"2024-06-10T06:23:51.936641Z","shell.execute_reply":"2024-06-10T06:23:52.229742Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"%%time\n\nall_train_losses = []\nall_eval_losses = []\n\nall_train_accuracy =  []\nall_test_accuracy = []\n\n# we define one iteration of the optimizer and JIT this function\n@jax.jit\ndef step(key, params, var_params, opt_state):\n    key, subkey = jax.random.split(key)\n    xb, yb = get_batch(key, train_data)\n    (loss, aux_data), grad = jax.value_and_grad(loss_fun, has_aux=True)(params, xb, yb, var_params, subkey)\n    var_params, train_accuracy = aux_data\n    updates, opt_state = opt.update(grad, opt_state, params)\n    params = optax.apply_updates(params, updates)\n    return params, key, opt_state, loss, var_params, train_accuracy\n\n# for i in tqdm(range(max_iters)):\ncounter = 0\nloss = 10\nwhile counter<max_iters: # and loss > 1.0:\n\n    params, key, opt_state, loss, var_params, train_accuracy = step(key, params, var_params, opt_state)\n    \n\n    # once every N_FREQ_EVAL we compute loss on the validation set\n    if counter % eval_iters == 0:\n        key, subkey = jax.random.split(key)\n        eval_loss, eval_accuracy = eval_step(params, *get_batch(subkey, test_data), var_params)\n        all_train_losses.append(loss)\n        all_eval_losses.append(eval_loss)\n        all_train_accuracy.append(train_accuracy)\n        all_test_accuracy.append(eval_accuracy)\n        print('##########################################################')\n        print(\"Step: \", counter,\"\\t Train Loss: \", loss,\"\\t Train Accuracy: \", format(train_accuracy, \".2%\"))\n        print(\"Step: \", counter,\"\\t Eval Loss: \", eval_loss,\"\\t Eval Accuracy: \", format(eval_accuracy, \".2%\"))\n        \n    counter += 1\n        ","metadata":{"execution":{"iopub.status.busy":"2024-06-10T06:23:52.232108Z","iopub.execute_input":"2024-06-10T06:23:52.232886Z","iopub.status.idle":"2024-06-10T06:55:57.206680Z","shell.execute_reply.started":"2024-06-10T06:23:52.232849Z","shell.execute_reply":"2024-06-10T06:55:57.205620Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"##########################################################\nStep:  0 \t Train Loss:  4.649929 \t Train Accuracy:  1.57%\nStep:  0 \t Eval Loss:  4.241928 \t Eval Accuracy:  4.80%\n##########################################################\nStep:  100 \t Train Loss:  0.14932057 \t Train Accuracy:  97.46%\nStep:  100 \t Eval Loss:  0.15681136 \t Eval Accuracy:  97.42%\n##########################################################\nStep:  200 \t Train Loss:  0.051119924 \t Train Accuracy:  98.90%\nStep:  200 \t Eval Loss:  0.06282723 \t Eval Accuracy:  98.57%\n##########################################################\nStep:  300 \t Train Loss:  0.044627473 \t Train Accuracy:  98.95%\nStep:  300 \t Eval Loss:  0.049876653 \t Eval Accuracy:  98.62%\n##########################################################\nStep:  400 \t Train Loss:  0.040717054 \t Train Accuracy:  98.91%\nStep:  400 \t Eval Loss:  0.042327553 \t Eval Accuracy:  98.95%\n##########################################################\nStep:  500 \t Train Loss:  0.033634283 \t Train Accuracy:  99.04%\nStep:  500 \t Eval Loss:  0.03761807 \t Eval Accuracy:  98.94%\n##########################################################\nStep:  600 \t Train Loss:  0.037874393 \t Train Accuracy:  98.91%\nStep:  600 \t Eval Loss:  0.03976752 \t Eval Accuracy:  98.95%\n##########################################################\nStep:  700 \t Train Loss:  0.03564897 \t Train Accuracy:  99.04%\nStep:  700 \t Eval Loss:  0.038962692 \t Eval Accuracy:  98.95%\n##########################################################\nStep:  800 \t Train Loss:  0.032338347 \t Train Accuracy:  99.08%\nStep:  800 \t Eval Loss:  0.037071038 \t Eval Accuracy:  99.02%\n##########################################################\nStep:  900 \t Train Loss:  0.03198741 \t Train Accuracy:  99.10%\nStep:  900 \t Eval Loss:  0.032631874 \t Eval Accuracy:  99.05%\n##########################################################\nStep:  1000 \t Train Loss:  0.030391112 \t Train Accuracy:  99.06%\nStep:  1000 \t Eval Loss:  0.036820084 \t Eval Accuracy:  98.95%\n##########################################################\nStep:  1100 \t Train Loss:  0.035092883 \t Train Accuracy:  99.06%\nStep:  1100 \t Eval Loss:  0.034664355 \t Eval Accuracy:  98.97%\n##########################################################\nStep:  1200 \t Train Loss:  0.031960294 \t Train Accuracy:  99.13%\nStep:  1200 \t Eval Loss:  0.033551876 \t Eval Accuracy:  99.07%\n##########################################################\nStep:  1300 \t Train Loss:  0.033153743 \t Train Accuracy:  99.06%\nStep:  1300 \t Eval Loss:  0.03391379 \t Eval Accuracy:  99.01%\n##########################################################\nStep:  1400 \t Train Loss:  0.028052604 \t Train Accuracy:  99.10%\nStep:  1400 \t Eval Loss:  0.0335251 \t Eval Accuracy:  98.91%\n##########################################################\nStep:  1500 \t Train Loss:  0.032070614 \t Train Accuracy:  99.10%\nStep:  1500 \t Eval Loss:  0.032910652 \t Eval Accuracy:  99.07%\n##########################################################\nStep:  1600 \t Train Loss:  0.030055687 \t Train Accuracy:  99.10%\nStep:  1600 \t Eval Loss:  0.031445224 \t Eval Accuracy:  99.10%\n##########################################################\nStep:  1700 \t Train Loss:  0.031032793 \t Train Accuracy:  99.08%\nStep:  1700 \t Eval Loss:  0.030037928 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  1800 \t Train Loss:  0.033705164 \t Train Accuracy:  99.13%\nStep:  1800 \t Eval Loss:  0.028210336 \t Eval Accuracy:  99.10%\n##########################################################\nStep:  1900 \t Train Loss:  0.03448022 \t Train Accuracy:  98.94%\nStep:  1900 \t Eval Loss:  0.036650464 \t Eval Accuracy:  99.02%\n##########################################################\nStep:  2000 \t Train Loss:  0.030290253 \t Train Accuracy:  99.11%\nStep:  2000 \t Eval Loss:  0.033276625 \t Eval Accuracy:  99.10%\n##########################################################\nStep:  2100 \t Train Loss:  0.030304076 \t Train Accuracy:  99.17%\nStep:  2100 \t Eval Loss:  0.032465704 \t Eval Accuracy:  99.04%\n##########################################################\nStep:  2200 \t Train Loss:  0.0272877 \t Train Accuracy:  99.26%\nStep:  2200 \t Eval Loss:  0.030659785 \t Eval Accuracy:  99.15%\n##########################################################\nStep:  2300 \t Train Loss:  0.025747387 \t Train Accuracy:  99.22%\nStep:  2300 \t Eval Loss:  0.03598552 \t Eval Accuracy:  98.94%\n##########################################################\nStep:  2400 \t Train Loss:  0.028963683 \t Train Accuracy:  99.15%\nStep:  2400 \t Eval Loss:  0.037131682 \t Eval Accuracy:  98.95%\n##########################################################\nStep:  2500 \t Train Loss:  0.027844375 \t Train Accuracy:  99.16%\nStep:  2500 \t Eval Loss:  0.030441735 \t Eval Accuracy:  99.10%\n##########################################################\nStep:  2600 \t Train Loss:  0.030113854 \t Train Accuracy:  99.10%\nStep:  2600 \t Eval Loss:  0.033372 \t Eval Accuracy:  99.00%\n##########################################################\nStep:  2700 \t Train Loss:  0.026797138 \t Train Accuracy:  99.16%\nStep:  2700 \t Eval Loss:  0.029678188 \t Eval Accuracy:  99.17%\n##########################################################\nStep:  2800 \t Train Loss:  0.0286717 \t Train Accuracy:  99.18%\nStep:  2800 \t Eval Loss:  0.03194927 \t Eval Accuracy:  99.08%\n##########################################################\nStep:  2900 \t Train Loss:  0.02528717 \t Train Accuracy:  99.28%\nStep:  2900 \t Eval Loss:  0.036916997 \t Eval Accuracy:  98.97%\n##########################################################\nStep:  3000 \t Train Loss:  0.027331468 \t Train Accuracy:  99.26%\nStep:  3000 \t Eval Loss:  0.033765897 \t Eval Accuracy:  99.06%\n##########################################################\nStep:  3100 \t Train Loss:  0.02910247 \t Train Accuracy:  99.23%\nStep:  3100 \t Eval Loss:  0.03168217 \t Eval Accuracy:  99.12%\n##########################################################\nStep:  3200 \t Train Loss:  0.03062004 \t Train Accuracy:  99.12%\nStep:  3200 \t Eval Loss:  0.03195411 \t Eval Accuracy:  99.13%\n##########################################################\nStep:  3300 \t Train Loss:  0.027491415 \t Train Accuracy:  99.19%\nStep:  3300 \t Eval Loss:  0.03151872 \t Eval Accuracy:  99.06%\n##########################################################\nStep:  3400 \t Train Loss:  0.02862098 \t Train Accuracy:  99.16%\nStep:  3400 \t Eval Loss:  0.030326407 \t Eval Accuracy:  99.17%\n##########################################################\nStep:  3500 \t Train Loss:  0.027703144 \t Train Accuracy:  99.13%\nStep:  3500 \t Eval Loss:  0.027569242 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  3600 \t Train Loss:  0.028518485 \t Train Accuracy:  99.11%\nStep:  3600 \t Eval Loss:  0.023178002 \t Eval Accuracy:  99.32%\n##########################################################\nStep:  3700 \t Train Loss:  0.027655242 \t Train Accuracy:  99.13%\nStep:  3700 \t Eval Loss:  0.030312013 \t Eval Accuracy:  99.10%\n##########################################################\nStep:  3800 \t Train Loss:  0.031046882 \t Train Accuracy:  99.10%\nStep:  3800 \t Eval Loss:  0.031547464 \t Eval Accuracy:  99.06%\n##########################################################\nStep:  3900 \t Train Loss:  0.029740095 \t Train Accuracy:  99.08%\nStep:  3900 \t Eval Loss:  0.031185554 \t Eval Accuracy:  99.06%\n##########################################################\nStep:  4000 \t Train Loss:  0.02575926 \t Train Accuracy:  99.23%\nStep:  4000 \t Eval Loss:  0.02826136 \t Eval Accuracy:  99.17%\n##########################################################\nStep:  4100 \t Train Loss:  0.02669277 \t Train Accuracy:  99.15%\nStep:  4100 \t Eval Loss:  0.03185065 \t Eval Accuracy:  99.10%\n##########################################################\nStep:  4200 \t Train Loss:  0.023982972 \t Train Accuracy:  99.23%\nStep:  4200 \t Eval Loss:  0.029652487 \t Eval Accuracy:  99.15%\n##########################################################\nStep:  4300 \t Train Loss:  0.028858617 \t Train Accuracy:  99.18%\nStep:  4300 \t Eval Loss:  0.031879276 \t Eval Accuracy:  99.07%\n##########################################################\nStep:  4400 \t Train Loss:  0.028101828 \t Train Accuracy:  99.07%\nStep:  4400 \t Eval Loss:  0.03009581 \t Eval Accuracy:  99.13%\n##########################################################\nStep:  4500 \t Train Loss:  0.026873713 \t Train Accuracy:  99.27%\nStep:  4500 \t Eval Loss:  0.029080827 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  4600 \t Train Loss:  0.028056543 \t Train Accuracy:  99.15%\nStep:  4600 \t Eval Loss:  0.03325148 \t Eval Accuracy:  99.08%\n##########################################################\nStep:  4700 \t Train Loss:  0.027363943 \t Train Accuracy:  99.23%\nStep:  4700 \t Eval Loss:  0.024970734 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  4800 \t Train Loss:  0.027021334 \t Train Accuracy:  99.23%\nStep:  4800 \t Eval Loss:  0.027829964 \t Eval Accuracy:  99.13%\n##########################################################\nStep:  4900 \t Train Loss:  0.02874453 \t Train Accuracy:  99.15%\nStep:  4900 \t Eval Loss:  0.032553453 \t Eval Accuracy:  99.07%\n##########################################################\nStep:  5000 \t Train Loss:  0.029283691 \t Train Accuracy:  99.10%\nStep:  5000 \t Eval Loss:  0.026811369 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  5100 \t Train Loss:  0.026528997 \t Train Accuracy:  99.19%\nStep:  5100 \t Eval Loss:  0.0345166 \t Eval Accuracy:  99.02%\n##########################################################\nStep:  5200 \t Train Loss:  0.025443364 \t Train Accuracy:  99.29%\nStep:  5200 \t Eval Loss:  0.034250338 \t Eval Accuracy:  99.01%\n##########################################################\nStep:  5300 \t Train Loss:  0.027079146 \t Train Accuracy:  99.15%\nStep:  5300 \t Eval Loss:  0.030906338 \t Eval Accuracy:  99.13%\n##########################################################\nStep:  5400 \t Train Loss:  0.02757854 \t Train Accuracy:  99.19%\nStep:  5400 \t Eval Loss:  0.02458229 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  5500 \t Train Loss:  0.028166626 \t Train Accuracy:  99.18%\nStep:  5500 \t Eval Loss:  0.030957099 \t Eval Accuracy:  99.16%\n##########################################################\nStep:  5600 \t Train Loss:  0.029217731 \t Train Accuracy:  99.17%\nStep:  5600 \t Eval Loss:  0.026799686 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  5700 \t Train Loss:  0.026174668 \t Train Accuracy:  99.22%\nStep:  5700 \t Eval Loss:  0.029661473 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  5800 \t Train Loss:  0.024909977 \t Train Accuracy:  99.19%\nStep:  5800 \t Eval Loss:  0.027927777 \t Eval Accuracy:  99.13%\n##########################################################\nStep:  5900 \t Train Loss:  0.027504928 \t Train Accuracy:  99.22%\nStep:  5900 \t Eval Loss:  0.02606704 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  6000 \t Train Loss:  0.024998676 \t Train Accuracy:  99.24%\nStep:  6000 \t Eval Loss:  0.028197203 \t Eval Accuracy:  99.16%\n##########################################################\nStep:  6100 \t Train Loss:  0.026147956 \t Train Accuracy:  99.21%\nStep:  6100 \t Eval Loss:  0.02862004 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  6200 \t Train Loss:  0.026421111 \t Train Accuracy:  99.23%\nStep:  6200 \t Eval Loss:  0.028305605 \t Eval Accuracy:  99.32%\n##########################################################\nStep:  6300 \t Train Loss:  0.024046041 \t Train Accuracy:  99.28%\nStep:  6300 \t Eval Loss:  0.027010586 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  6400 \t Train Loss:  0.025172597 \t Train Accuracy:  99.22%\nStep:  6400 \t Eval Loss:  0.02809804 \t Eval Accuracy:  99.17%\n##########################################################\nStep:  6500 \t Train Loss:  0.028237637 \t Train Accuracy:  99.18%\nStep:  6500 \t Eval Loss:  0.028940257 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  6600 \t Train Loss:  0.024329422 \t Train Accuracy:  99.28%\nStep:  6600 \t Eval Loss:  0.028369263 \t Eval Accuracy:  99.16%\n##########################################################\nStep:  6700 \t Train Loss:  0.024771836 \t Train Accuracy:  99.28%\nStep:  6700 \t Eval Loss:  0.033791408 \t Eval Accuracy:  98.99%\n##########################################################\nStep:  6800 \t Train Loss:  0.023489933 \t Train Accuracy:  99.29%\nStep:  6800 \t Eval Loss:  0.029505871 \t Eval Accuracy:  99.11%\n##########################################################\nStep:  6900 \t Train Loss:  0.026659792 \t Train Accuracy:  99.24%\nStep:  6900 \t Eval Loss:  0.02624935 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  7000 \t Train Loss:  0.029525068 \t Train Accuracy:  99.11%\nStep:  7000 \t Eval Loss:  0.030526388 \t Eval Accuracy:  99.02%\n##########################################################\nStep:  7100 \t Train Loss:  0.02760928 \t Train Accuracy:  99.15%\nStep:  7100 \t Eval Loss:  0.03181102 \t Eval Accuracy:  99.07%\n##########################################################\nStep:  7200 \t Train Loss:  0.025725774 \t Train Accuracy:  99.17%\nStep:  7200 \t Eval Loss:  0.028647505 \t Eval Accuracy:  99.10%\n##########################################################\nStep:  7300 \t Train Loss:  0.023728805 \t Train Accuracy:  99.22%\nStep:  7300 \t Eval Loss:  0.033710584 \t Eval Accuracy:  99.08%\n##########################################################\nStep:  7400 \t Train Loss:  0.022880184 \t Train Accuracy:  99.32%\nStep:  7400 \t Eval Loss:  0.029110108 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  7500 \t Train Loss:  0.028735649 \t Train Accuracy:  99.21%\nStep:  7500 \t Eval Loss:  0.029561775 \t Eval Accuracy:  99.15%\n##########################################################\nStep:  7600 \t Train Loss:  0.026843587 \t Train Accuracy:  99.13%\nStep:  7600 \t Eval Loss:  0.028792549 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  7700 \t Train Loss:  0.027486734 \t Train Accuracy:  99.19%\nStep:  7700 \t Eval Loss:  0.03255316 \t Eval Accuracy:  99.08%\n##########################################################\nStep:  7800 \t Train Loss:  0.027154341 \t Train Accuracy:  99.18%\nStep:  7800 \t Eval Loss:  0.030373238 \t Eval Accuracy:  99.12%\n##########################################################\nStep:  7900 \t Train Loss:  0.024255205 \t Train Accuracy:  99.24%\nStep:  7900 \t Eval Loss:  0.028215023 \t Eval Accuracy:  99.13%\n##########################################################\nStep:  8000 \t Train Loss:  0.02536894 \t Train Accuracy:  99.30%\nStep:  8000 \t Eval Loss:  0.027570719 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  8100 \t Train Loss:  0.029491019 \t Train Accuracy:  99.17%\nStep:  8100 \t Eval Loss:  0.025735773 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  8200 \t Train Loss:  0.028483259 \t Train Accuracy:  99.12%\nStep:  8200 \t Eval Loss:  0.032212444 \t Eval Accuracy:  99.06%\n##########################################################\nStep:  8300 \t Train Loss:  0.028438428 \t Train Accuracy:  99.12%\nStep:  8300 \t Eval Loss:  0.02647661 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  8400 \t Train Loss:  0.028367769 \t Train Accuracy:  99.07%\nStep:  8400 \t Eval Loss:  0.027385555 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  8500 \t Train Loss:  0.021772671 \t Train Accuracy:  99.27%\nStep:  8500 \t Eval Loss:  0.02787839 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  8600 \t Train Loss:  0.02366691 \t Train Accuracy:  99.32%\nStep:  8600 \t Eval Loss:  0.028938819 \t Eval Accuracy:  99.12%\n##########################################################\nStep:  8700 \t Train Loss:  0.025148299 \t Train Accuracy:  99.24%\nStep:  8700 \t Eval Loss:  0.026636971 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  8800 \t Train Loss:  0.02509411 \t Train Accuracy:  99.32%\nStep:  8800 \t Eval Loss:  0.028889162 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  8900 \t Train Loss:  0.025098016 \t Train Accuracy:  99.23%\nStep:  8900 \t Eval Loss:  0.026316158 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  9000 \t Train Loss:  0.022727488 \t Train Accuracy:  99.34%\nStep:  9000 \t Eval Loss:  0.02622197 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  9100 \t Train Loss:  0.024660934 \t Train Accuracy:  99.16%\nStep:  9100 \t Eval Loss:  0.027834201 \t Eval Accuracy:  99.15%\n##########################################################\nStep:  9200 \t Train Loss:  0.02791458 \t Train Accuracy:  99.15%\nStep:  9200 \t Eval Loss:  0.028424531 \t Eval Accuracy:  99.16%\n##########################################################\nStep:  9300 \t Train Loss:  0.024228347 \t Train Accuracy:  99.23%\nStep:  9300 \t Eval Loss:  0.024290692 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  9400 \t Train Loss:  0.027686387 \t Train Accuracy:  99.17%\nStep:  9400 \t Eval Loss:  0.024735864 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  9500 \t Train Loss:  0.027747225 \t Train Accuracy:  99.15%\nStep:  9500 \t Eval Loss:  0.029410228 \t Eval Accuracy:  99.12%\n##########################################################\nStep:  9600 \t Train Loss:  0.026149973 \t Train Accuracy:  99.24%\nStep:  9600 \t Eval Loss:  0.02939859 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  9700 \t Train Loss:  0.02505791 \t Train Accuracy:  99.21%\nStep:  9700 \t Eval Loss:  0.02287756 \t Eval Accuracy:  99.35%\n##########################################################\nStep:  9800 \t Train Loss:  0.02286194 \t Train Accuracy:  99.24%\nStep:  9800 \t Eval Loss:  0.028193608 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  9900 \t Train Loss:  0.0262927 \t Train Accuracy:  99.18%\nStep:  9900 \t Eval Loss:  0.02588626 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  10000 \t Train Loss:  0.02302699 \t Train Accuracy:  99.30%\nStep:  10000 \t Eval Loss:  0.030345539 \t Eval Accuracy:  99.10%\n##########################################################\nStep:  10100 \t Train Loss:  0.019037249 \t Train Accuracy:  99.45%\nStep:  10100 \t Eval Loss:  0.026806163 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  10200 \t Train Loss:  0.027444229 \t Train Accuracy:  99.13%\nStep:  10200 \t Eval Loss:  0.030592524 \t Eval Accuracy:  99.13%\n##########################################################\nStep:  10300 \t Train Loss:  0.024200613 \t Train Accuracy:  99.22%\nStep:  10300 \t Eval Loss:  0.029579736 \t Eval Accuracy:  99.05%\n##########################################################\nStep:  10400 \t Train Loss:  0.024155 \t Train Accuracy:  99.29%\nStep:  10400 \t Eval Loss:  0.031363234 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  10500 \t Train Loss:  0.024220085 \t Train Accuracy:  99.27%\nStep:  10500 \t Eval Loss:  0.027500868 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  10600 \t Train Loss:  0.024190228 \t Train Accuracy:  99.26%\nStep:  10600 \t Eval Loss:  0.025631066 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  10700 \t Train Loss:  0.024953889 \t Train Accuracy:  99.26%\nStep:  10700 \t Eval Loss:  0.03260441 \t Eval Accuracy:  99.10%\n##########################################################\nStep:  10800 \t Train Loss:  0.026590016 \t Train Accuracy:  99.27%\nStep:  10800 \t Eval Loss:  0.028181665 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  10900 \t Train Loss:  0.027882082 \t Train Accuracy:  99.23%\nStep:  10900 \t Eval Loss:  0.027984165 \t Eval Accuracy:  99.05%\n##########################################################\nStep:  11000 \t Train Loss:  0.02378374 \t Train Accuracy:  99.34%\nStep:  11000 \t Eval Loss:  0.03221024 \t Eval Accuracy:  99.10%\n##########################################################\nStep:  11100 \t Train Loss:  0.025690015 \t Train Accuracy:  99.17%\nStep:  11100 \t Eval Loss:  0.033297032 \t Eval Accuracy:  99.15%\n##########################################################\nStep:  11200 \t Train Loss:  0.022906493 \t Train Accuracy:  99.30%\nStep:  11200 \t Eval Loss:  0.032043293 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  11300 \t Train Loss:  0.02717853 \t Train Accuracy:  99.22%\nStep:  11300 \t Eval Loss:  0.024749298 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  11400 \t Train Loss:  0.02399507 \t Train Accuracy:  99.16%\nStep:  11400 \t Eval Loss:  0.023682792 \t Eval Accuracy:  99.34%\n##########################################################\nStep:  11500 \t Train Loss:  0.027939001 \t Train Accuracy:  99.22%\nStep:  11500 \t Eval Loss:  0.026876774 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  11600 \t Train Loss:  0.02793857 \t Train Accuracy:  99.23%\nStep:  11600 \t Eval Loss:  0.026078038 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  11700 \t Train Loss:  0.025640242 \t Train Accuracy:  99.21%\nStep:  11700 \t Eval Loss:  0.02711096 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  11800 \t Train Loss:  0.024883468 \t Train Accuracy:  99.24%\nStep:  11800 \t Eval Loss:  0.02710045 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  11900 \t Train Loss:  0.027382998 \t Train Accuracy:  99.24%\nStep:  11900 \t Eval Loss:  0.023795132 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  12000 \t Train Loss:  0.022214703 \t Train Accuracy:  99.40%\nStep:  12000 \t Eval Loss:  0.029886035 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  12100 \t Train Loss:  0.025021132 \t Train Accuracy:  99.22%\nStep:  12100 \t Eval Loss:  0.029893652 \t Eval Accuracy:  99.10%\n##########################################################\nStep:  12200 \t Train Loss:  0.0206898 \t Train Accuracy:  99.34%\nStep:  12200 \t Eval Loss:  0.028121417 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  12300 \t Train Loss:  0.026194531 \t Train Accuracy:  99.21%\nStep:  12300 \t Eval Loss:  0.031247385 \t Eval Accuracy:  99.07%\n##########################################################\nStep:  12400 \t Train Loss:  0.022552375 \t Train Accuracy:  99.29%\nStep:  12400 \t Eval Loss:  0.029376905 \t Eval Accuracy:  99.17%\n##########################################################\nStep:  12500 \t Train Loss:  0.025130711 \t Train Accuracy:  99.21%\nStep:  12500 \t Eval Loss:  0.022951005 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  12600 \t Train Loss:  0.023262247 \t Train Accuracy:  99.28%\nStep:  12600 \t Eval Loss:  0.029537382 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  12700 \t Train Loss:  0.025818005 \t Train Accuracy:  99.22%\nStep:  12700 \t Eval Loss:  0.026007071 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  12800 \t Train Loss:  0.029447433 \t Train Accuracy:  99.15%\nStep:  12800 \t Eval Loss:  0.024941357 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  12900 \t Train Loss:  0.025415702 \t Train Accuracy:  99.17%\nStep:  12900 \t Eval Loss:  0.027607547 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  13000 \t Train Loss:  0.022509728 \t Train Accuracy:  99.29%\nStep:  13000 \t Eval Loss:  0.02613913 \t Eval Accuracy:  99.30%\n##########################################################\nStep:  13100 \t Train Loss:  0.024610376 \t Train Accuracy:  99.30%\nStep:  13100 \t Eval Loss:  0.02890534 \t Eval Accuracy:  99.16%\n##########################################################\nStep:  13200 \t Train Loss:  0.02256557 \t Train Accuracy:  99.24%\nStep:  13200 \t Eval Loss:  0.025876056 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  13300 \t Train Loss:  0.02083386 \t Train Accuracy:  99.37%\nStep:  13300 \t Eval Loss:  0.025945751 \t Eval Accuracy:  99.16%\n##########################################################\nStep:  13400 \t Train Loss:  0.026819646 \t Train Accuracy:  99.16%\nStep:  13400 \t Eval Loss:  0.022979382 \t Eval Accuracy:  99.33%\n##########################################################\nStep:  13500 \t Train Loss:  0.022677869 \t Train Accuracy:  99.29%\nStep:  13500 \t Eval Loss:  0.029097693 \t Eval Accuracy:  99.12%\n##########################################################\nStep:  13600 \t Train Loss:  0.02702978 \t Train Accuracy:  99.19%\nStep:  13600 \t Eval Loss:  0.030891106 \t Eval Accuracy:  99.12%\n##########################################################\nStep:  13700 \t Train Loss:  0.02462468 \t Train Accuracy:  99.34%\nStep:  13700 \t Eval Loss:  0.025047151 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  13800 \t Train Loss:  0.021715153 \t Train Accuracy:  99.32%\nStep:  13800 \t Eval Loss:  0.028197728 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  13900 \t Train Loss:  0.021855827 \t Train Accuracy:  99.34%\nStep:  13900 \t Eval Loss:  0.027003273 \t Eval Accuracy:  99.17%\n##########################################################\nStep:  14000 \t Train Loss:  0.02419103 \t Train Accuracy:  99.21%\nStep:  14000 \t Eval Loss:  0.026882078 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  14100 \t Train Loss:  0.02045138 \t Train Accuracy:  99.33%\nStep:  14100 \t Eval Loss:  0.02575152 \t Eval Accuracy:  99.13%\n##########################################################\nStep:  14200 \t Train Loss:  0.023735605 \t Train Accuracy:  99.33%\nStep:  14200 \t Eval Loss:  0.030872177 \t Eval Accuracy:  99.17%\n##########################################################\nStep:  14300 \t Train Loss:  0.025924668 \t Train Accuracy:  99.16%\nStep:  14300 \t Eval Loss:  0.025704622 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  14400 \t Train Loss:  0.021948248 \t Train Accuracy:  99.34%\nStep:  14400 \t Eval Loss:  0.027804159 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  14500 \t Train Loss:  0.021378249 \t Train Accuracy:  99.39%\nStep:  14500 \t Eval Loss:  0.02135804 \t Eval Accuracy:  99.39%\n##########################################################\nStep:  14600 \t Train Loss:  0.020970952 \t Train Accuracy:  99.35%\nStep:  14600 \t Eval Loss:  0.028179051 \t Eval Accuracy:  99.16%\n##########################################################\nStep:  14700 \t Train Loss:  0.026313659 \t Train Accuracy:  99.21%\nStep:  14700 \t Eval Loss:  0.02848512 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  14800 \t Train Loss:  0.021831926 \t Train Accuracy:  99.30%\nStep:  14800 \t Eval Loss:  0.028259147 \t Eval Accuracy:  99.12%\n##########################################################\nStep:  14900 \t Train Loss:  0.024799256 \t Train Accuracy:  99.30%\nStep:  14900 \t Eval Loss:  0.03205876 \t Eval Accuracy:  99.13%\nCPU times: user 19min 55s, sys: 12min 12s, total: 32min 8s\nWall time: 32min 4s\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt  # Visualization\n\n# Plot loss and accuracy in subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\nax1.set_title('Loss')\nax2.set_title('Accuracy')\n\n\n\nax1.plot(all_train_losses, label='train_loss')\nax1.plot(all_eval_losses, label='eval_loss')\n\nax2.plot(all_train_accuracy, label='train_accuracy')\nax2.plot(all_test_accuracy, label='eval_accuracy')\n\nax1.legend()\nax2.legend()\nplt.show()\nplt.clf()","metadata":{"execution":{"iopub.status.busy":"2024-06-10T06:55:57.207775Z","iopub.execute_input":"2024-06-10T06:55:57.208041Z","iopub.status.idle":"2024-06-10T06:55:57.738629Z","shell.execute_reply.started":"2024-06-10T06:55:57.208016Z","shell.execute_reply":"2024-06-10T06:55:57.737495Z"},"trusted":true},"execution_count":38,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1500x500 with 2 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAABLEAAAHDCAYAAADbbYg5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAACGZklEQVR4nOzdeXxU9b3/8fc5s2eZ7AlbIGyiiIIL+sPdiqKt1Npa13sBtbYu3KrUqrTuVrFWqbvYRaktXFyqvbaoLWLRuitK674gyBqSANkz6/n+/pjMQGRLIBA483o+HseQmTNzPmcm8Xzzmc/387WMMUYAAAAAAADAbszu6QAAAAAAAACAbSGJBQAAAAAAgN0eSSwAAAAAAADs9khiAQAAAAAAYLdHEgsAAAAAAAC7PZJYAAAAAAAA2O2RxAIAAAAAAMBujyQWAAAAAAAAdnsksQAAAAAAALDbI4kFAAAAAACA3R5JLADdZubMmbIsS++8805PhwIAAIB2DzzwgCzL0qGHHtrToQDADiGJBQAAAAAuNmvWLFVVVemtt97SF1980dPhAMB2I4kFAAAAAC61ZMkSvfbaa5o+fbrKyso0a9asng5ps1paWno6BAB7AJJYAHap9957TyeddJLC4bDy8vJ03HHH6Y033uiwTzwe14033qihQ4cqGAyqpKRERxxxhObNm5fZp7q6Wueee6769eunQCCg3r1765RTTtHSpUt38RkBAADsvmbNmqWioiJ961vf0mmnnbbZJFZ9fb0uv/xyVVVVKRAIqF+/fpowYYLq6uoy+0QiEd1www3aa6+9FAwG1bt3b333u9/V4sWLJUkLFiyQZVlasGBBh+deunSpLMvSzJkzM7dNmjRJeXl5Wrx4sb75zW8qPz9f55xzjiTpX//6l77//e+rf//+CgQCqqys1OWXX662trZN4v7kk090+umnq6ysTKFQSMOGDdPPf/5zSdI///lPWZalp59+epPHzZ49W5Zl6fXXX+/y6wmgZ3l7OgAA2ePDDz/UkUceqXA4rCuvvFI+n08PPfSQjjnmGL300kuZPg033HCDpk2bph/84Ac65JBD1NjYqHfeeUfvvvuujj/+eEnS9773PX344Yf6n//5H1VVVammpkbz5s3TsmXLVFVV1YNnCQAAsPuYNWuWvvvd78rv9+uss87Sgw8+qLffflujR4+WJDU3N+vII4/Uxx9/rPPOO08HHnig6urq9Mwzz2jFihUqLS1VMpnUySefrPnz5+vMM8/UpZdeqqamJs2bN08ffPCBBg8e3OW4EomExo0bpyOOOEJ33HGHcnJyJElPPPGEWltbddFFF6mkpERvvfWW7r33Xq1YsUJPPPFE5vH/+c9/dOSRR8rn8+mHP/yhqqqqtHjxYv31r3/VLbfcomOOOUaVlZWaNWuWTj311E1ek8GDB2vMmDE78MoC6BEGALrJI488YiSZt99+e7P3f+c73zF+v98sXrw4c9uqVatMfn6+OeqoozK3jRw50nzrW9/a4nHWr19vJJlf/epX3Rc8AACAy7zzzjtGkpk3b54xxhjHcUy/fv3MpZdemtnnuuuuM5LMU089tcnjHccxxhjz8MMPG0lm+vTpW9znn//8p5Fk/vnPf3a4f8mSJUaSeeSRRzK3TZw40UgyV1999SbP19rauslt06ZNM5Zlma+++ipz21FHHWXy8/M73LZxPMYYM3XqVBMIBEx9fX3mtpqaGuP1es3111+/yXEA7P6YTghgl0gmk/rHP/6h73znOxo0aFDm9t69e+vss8/WK6+8osbGRklSYWGhPvzwQ33++eebfa5QKCS/368FCxZo/fr1uyR+AACAPc2sWbNUUVGhY489VpJkWZbOOOMMzZkzR8lkUpL05z//WSNHjtykWim9f3qf0tJS/c///M8W99keF1100Sa3hUKhzL9bWlpUV1enww47TMYYvffee5Kk2tpavfzyyzrvvPPUv3//LcYzYcIERaNRPfnkk5nbHnvsMSUSCf3Xf/3XdscNoOeQxAKwS9TW1qq1tVXDhg3b5L599tlHjuNo+fLlkqSbbrpJ9fX12muvvbTffvvppz/9qf7zn/9k9g8EAvrlL3+p5557ThUVFTrqqKN0++23q7q6epedDwAAwO4smUxqzpw5OvbYY7VkyRJ98cUX+uKLL3TooYdqzZo1mj9/viRp8eLFGjFixFafa/HixRo2bJi83u7rRuP1etWvX79Nbl+2bJkmTZqk4uJi5eXlqaysTEcffbQkqaGhQZL05ZdfStI249577701evToDn3AZs2apf/3//6fhgwZ0l2nAmAXIokFYLdz1FFHafHixXr44Yc1YsQI/e53v9OBBx6o3/3ud5l9LrvsMn322WeaNm2agsGgrr32Wu2zzz6ZT+gAAACy2YsvvqjVq1drzpw5Gjp0aGY7/fTTJanbVyncUkVWuuLr6wKBgGzb3mTf448/XnPnztVVV12lv/zlL5o3b16mKbzjOF2Oa8KECXrppZe0YsUKLV68WG+88QZVWMAejMbuAHaJsrIy5eTk6NNPP93kvk8++US2bauysjJzW3Fxsc4991yde+65am5u1lFHHaUbbrhBP/jBDzL7DB48WD/5yU/0k5/8RJ9//rlGjRqlO++8U3/60592yTkBAADsrmbNmqXy8nLdf//9m9z31FNP6emnn9aMGTM0ePBgffDBB1t9rsGDB+vNN99UPB6Xz+fb7D5FRUWSUisdbuyrr77qdMzvv/++PvvsM/3hD3/QhAkTMrdvvEK1pExrim3FLUlnnnmmpkyZov/93/9VW1ubfD6fzjjjjE7HBGD3QiUWgF3C4/HohBNO0P/93/9p6dKlmdvXrFmj2bNn64gjjlA4HJYkrV27tsNj8/LyNGTIEEWjUUlSa2urIpFIh30GDx6s/Pz8zD4AAADZqq2tTU899ZROPvlknXbaaZtskydPVlNTk5555hl973vf07///W89/fTTmzyPMUZSalXouro63XfffVvcZ8CAAfJ4PHr55Zc73P/AAw90Om6Px9PhOdP/vvvuuzvsV1ZWpqOOOkoPP/ywli1bttl40kpLS3XSSSfpT3/6k2bNmqUTTzxRpaWlnY4JwO6FSiwA3e7hhx/W888/v8ntN9xwg+bNm6cjjjhCF198sbxerx566CFFo1Hdfvvtmf2GDx+uY445RgcddJCKi4v1zjvv6Mknn9TkyZMlSZ999pmOO+44nX766Ro+fLi8Xq+efvpprVmzRmeeeeYuO08AAIDd0TPPPKOmpiZ9+9vf3uz9/+///T+VlZVp1qxZmj17tp588kl9//vf13nnnaeDDjpI69at0zPPPKMZM2Zo5MiRmjBhgh599FFNmTJFb731lo488ki1tLTohRde0MUXX6xTTjlFBQUF+v73v697771XlmVp8ODB+tvf/qaamppOx7333ntr8ODBuuKKK7Ry5UqFw2H9+c9/3uxCPvfcc4+OOOIIHXjggfrhD3+ogQMHaunSpZo7d64WLVrUYd8JEybotNNOkyTdfPPNnX8hAex+enJpRADu8sgjjxhJW9yWL19u3n33XTNu3DiTl5dncnJyzLHHHmtee+21Ds/zi1/8whxyyCGmsLDQhEIhs/fee5tbbrnFxGIxY4wxdXV15pJLLjF77723yc3NNQUFBebQQw81jz/+eE+cNgAAwG5l/PjxJhgMmpaWli3uM2nSJOPz+UxdXZ1Zu3atmTx5sunbt6/x+/2mX79+ZuLEiaauri6zf2trq/n5z39uBg4caHw+n+nVq5c57bTTzOLFizP71NbWmu9973smJyfHFBUVmR/96Efmgw8+MJLMI488ktlv4sSJJjc3d7NxffTRR2bs2LEmLy/PlJaWmgsuuMD8+9//3uQ5jDHmgw8+MKeeeqopLCw0wWDQDBs2zFx77bWbPGc0GjVFRUWmoKDAtLW1dfJVBLA7soz5Wr0lAAAAAAAukUgk1KdPH40fP16///3vezocADuAnlgAAAAAANf6y1/+otra2g7N4gHsmajEAgAAAAC4zptvvqn//Oc/uvnmm1VaWqp33323p0MCsIOoxAIAAAAAuM6DDz6oiy66SOXl5Xr00Ud7OhwA3YBKLAAAAAAAAOz2qMQCAAAAAADAbo8kFgAAAAAAAHZ73l19QMdxtGrVKuXn58uyrF19eAAAsAcyxqipqUl9+vSRbfMZ3O6KcR4AAOiqrozzdnkSa9WqVaqsrNzVhwUAAC6wfPly9evXr6fDwBYwzgMAANurM+O8XZ7Eys/Pl5QKLhwO7+rDAwCAPVBjY6MqKysz4wjsnhjnAQCArurKOG+XJ7HSpeXhcJjBDQAA6BKmqO3eGOcBAIDt1ZlxHk0lAAAAAAAAsNsjiQUAAAAAAIDdHkksAAAAAAAA7PZ2eU8sAAB2hmQyqXg83tNhYDv5fD55PJ6eDgMAAAC7MZJYAIA9mjFG1dXVqq+v7+lQsIMKCwvVq1cvmrcDAABgs0hiAQD2aOkEVnl5uXJyckiA7IGMMWptbVVNTY0kqXfv3j0cEQAAAHZHJLEAAHusZDKZSWCVlJT0dDjYAaFQSJJUU1Oj8vJyphYCAABgEzR2BwDssdI9sHJycno4EnSH9PtIbzMAAABsDkksAMAejymE7sD72L1efvlljR8/Xn369JFlWfrLX/6yzccsWLBABx54oAKBgIYMGaKZM2fu9DgBAAA6iyQWAACAC7W0tGjkyJG6//77O7X/kiVL9K1vfUvHHnusFi1apMsuu0w/+MEP9Pe//30nRwoAANA5JLEAANjDVVVV6a677uqW51qwYIEsy2K1Rxc46aST9Itf/EKnnnpqp/afMWOGBg4cqDvvvFP77LOPJk+erNNOO02//vWvd3KkAAAAnUMSCwCAHnDMMcfosssu65bnevvtt/XDH/6wW54L2ev111/X2LFjO9w2btw4vf766z0UEQAAQEesTggAwG7IGKNkMimvd9uX6rKysl0QEdyuurpaFRUVHW6rqKhQY2Oj2traMitIbiwajSoajWa+b2xs3OlxAgCA7OWqSqxV9W165fM6fbyaARQAYPc1adIkvfTSS7r77rtlWZYsy9LMmTNlWZaee+45HXTQQQoEAnrllVe0ePFinXLKKaqoqFBeXp5Gjx6tF154ocPzfX06oWVZ+t3vfqdTTz1VOTk5Gjp0qJ555pntjvfPf/6z9t13XwUCAVVVVenOO+/scP8DDzygoUOHKhgMqqKiQqeddlrmvieffFL77befQqGQSkpKNHbsWLW0tGx3LNi9TJs2TQUFBZmtsrKyp0MCgKwQSzj6oqZZ/15er6RjejqcXcYYo8ZIXMZkzzl3t0QiqUg0tse+hq6qxHr+g2rd9LePdPL+vXXf2Qf2dDgAgF3MGKO2eLJHjh3yeTq9ut7dd9+tzz77TCNGjNBNN90kSfrwww8lSVdffbXuuOMODRo0SEVFRVq+fLm++c1v6pZbblEgENCjjz6q8ePH69NPP1X//v23eIwbb7xRt99+u371q1/p3nvv1TnnnKOvvvpKxcXFXTqvhQsX6vTTT9cNN9ygM844Q6+99pouvvhilZSUaNKkSXrnnXf04x//WH/84x912GGHad26dfrXv/4lSVq9erXOOuss3X777Tr11FPV1NSkf/3rX3vsoMntevXqpTVr1nS4bc2aNQqHw5utwpKkqVOnasqUKZnvGxsbsy6RlfqDKqGWaEIFIZ9y/J3/f4FirVLDCqlhmdRcq2SoRK05fdUc7KWSoiL5vR0/bzbGqL41rrUtMa1riWldS1TrWuJqalyvREO1WkIV8gdylOP3KBz0qTjXr5I8v3L8XiUdo6RjZFlSOOhTQcinvKBXLdGEGhvWK7b6Q0WMX83+crV6w1rfmlB1Y0SrG9rUFEnI57Hl99rye2wFvLYCHqOwWlSeY6t32Kfe+T4lvTmqS+ZqXVtS0URSOaZN4Wi18qJrFIrXKxhfJ390vfyx9fJG1soTqZcTrlSk76Fq7X2okkVD5PPaCng8qWN5bXlsS0nHaFltg1Z89YXqq5fKE8hVXnGFwiW9lXCM1q9bq8b6tYoljTx5pQrkFSkc8qt3QUB9cx3lmRatrK7R4uUrtLK6Vo4/VwXlA1TaZ4BCplWJ5e/KW71IdludGoL9VB8aoMacSoUKy1VYVKKy/BwV+B2FY9XKbatWxApopX+gVrd5VN0Q0eqGiKob2tQcTWhASa72Kg1qr1Cj7LY6xRtr5bSslSNbjj9fji9Plscrn4nLY+KyklHFohHFYxHF43ElAoVygsVyggUKNK9UXsNnym/6QkljaV2wv+oClWrz5qvAaVCBU6+8xDr52urkj6yVP96gNiukJrtADVaBWn1FSgSLZEIl8gTzlOszCnkt2ZLWRqSaNqO6iEervP3UZufIcSQjI8dIlpNQpbNCQ5wlGhBfrDw7oVj/I5W7z3HqU16hWNJRJJ5UayyptnhSrbHEhu/bb2tsalFk7VKZ9cvliTUolFug/MIiFYfDKvHHVWS3qsBqVcQX1jp/X9V4eilqBeRTUl4nKsUjam1pUmtrs9ramhVra1G0rUXJWJsKQz5VhIPqFQ4qt6hcntJBChX2UsBjyVu/WN6a9+Wt/0p+0yZPIiITb1NTJPW7s74lpmpTrK/sfvrC9FXMk6M+gYgq/BHlWjE1RxJqjMTlxKOq9KxVP3utSp11alRIKxJF+jJWoESgSL3Ky1XZu1y9isLyKSmPicsk4mpsaVFjS6ta29qU40kq1yvleh15QmE5uRWy8ivUFkuqsW6FIutXyd+0QuXRr1TStkS50Ro1BftqbahKtYFK+W0prGblmRY12mF9Yqr0TqSvPqr3yGlYod6mVoVqUTJUrH2HDtIhI/aWk1Oq2jZLa1tialxfp/Cat1Sx7m35E81q8xWoxVukhCeoHBNRjmmRT0mtD1aqNmew1udUqShWrd4tH6tX62eKeXO1svBgrS44QA0Jn1bUt2nl+jY1R+Lapyyg/cs82qvYUmNDvWrWrtX6+nqFTEQlvpiKfTGVab1K46tUEFkhX6xBMW+eInae2uw8Rb15injyFPXkKenNkePLkXwhyThyYq0y0VbZyTb5km3yORElE3Eti+bqs5YcLY3mqS1QqtJelerbr0qeRKuaa79SfP0qGcuSt2Sg8nsNVkF+vpoa6xVdv0p2S7X6ehrU265XqdYrooDWmXzVOHnymoTK7QaVmHrlq0UB2yjgMfJ6PEr48hTzhRWxcxVpXKtEY7U8rXVqUo5WePtrubdSEW+ByrxtKvW0yG/iWhHxa2mLX7Vt0pj8Gh0cXKH+yeVqDZTpi8C+Wmj20lpfb5WFg6rIDyqUqFd05fvy1X2iUKRaDVaB1nnL1OApUZvxK+pYijmWigNGA/KS6hdKyPJ4tCyary/acrSi1S+/ogqYqHKtqIbkRTU0L6J+/ja1RGNa05TQ6qaYQpFaDXSWaoiWK6CEXjdD9bZG6DPf3upT4NegfEd9chw1mVwtj+dpcSRPrUmPbDmyTVIeJfWzs09UWUFu919Mu8BVSSyPnRowMDYGgOzUFk9q+HU9s5LaRzeNU46/c5fVgoIC+f1+5eTkqFevXpKkTz75RJJ000036fjjj8/sW1xcrJEjR2a+v/nmm/X000/rmWee0eTJk7d4jEmTJumss86SJN16662655579NZbb+nEE0/s0nlNnz5dxx13nK699lpJ0l577aWPPvpIv/rVrzRp0iQtW7ZMubm5Ovnkk5Wfn68BAwbogAMOkJRKYiUSCX33u9/VgAEDJEn77bdfl46PXWfMmDF69tlnO9w2b948jRkzZouPCQQCCgQCOzu0bappiujdr9arKZJQczShSNxRwGurPL5C/er+pbbWZtW3xtXQGpfPY6kgx6/CHJ9sy1J9a0z1rXFFIm0KJeqVl6hXntMov+Lym5i8SqpBeVprl2itXaq4JyCficunhELJFhUla1WhdSqymlRtQmqw8tRqh1WrIq1MFmplslBFatTe9nINs5ar3KpPPbcS8lkdk+4eSfntW5MJaa23UMlgsSwZeSPrlJesV56JK6lcWSZX+fLpUGudiqxmSVLUePWBGah3nL30uSmUR448Miq1GjTEWqkh9kqVqkG1KtRiU6x6k6vB1ioNtDsmL6PGp2pTpGoVa7UpVqsJqshqUrHVpGI1qdhqVJGaZVubDrorjaV65cqSMnFt3Rvyf/SEwpLixqM2+dWmgBqNT0aSLEt+xdVf9Rq4meNtTtx41KqAchWR13IkSf3at4yPOvVUShpLLQopbLVmbvNJ2ltSjlMmvylXX9lKyCO/4qq0atXXqsscd0+xxKnQJ6a/chVRf6tGfa26TX4+tfoJxd/w6ENTJUkKKaqQoiq2Ypl/W5KSspWUraAV7/j4ekkrN3/8Ae1fY8Yj/9eP20ktJiBLUo4V3eQ+S1K4fRu4Xc+ecnD6HxFJDZI+34En24LcyBr1qn93s/ftL+n09De+je5ISvqkfZPUaEJqMHnqY9XJ08nfmy1a+ahixqMVpkwhK6ZcRZSjiLxNjvRl154qJKlgB0IZnf6HX5KRtLp9+7pmSV+lfiZyN/PzsEs1tW+SCiX10Qs6qjOPi2/mtqikzkw6+9r/ekds/M1Gn7EcZn2kw/RR6udnXfu2Dasb3pEKhnYiiJ3HVUms9hyWHLJYAIA91MEHH9zh++bmZt1www2aO3duJinU1tamZcuWbfV59t9//8y/c3NzFQ6HVVNT0+V4Pv74Y51yyikdbjv88MN11113KZlM6vjjj9eAAQM0aNAgnXjiiTrxxBMz0xhHjhyp4447Tvvtt5/GjRunE044QaeddpqKioq6HAe6rrm5WV988UXm+yVLlmjRokUqLi5W//79NXXqVK1cuVKPPvqoJOnCCy/UfffdpyuvvFLnnXeeXnzxRT3++OOaO3duT51Cp7REE/rufa/I27hUeWpTvtWmwdYqnep5RQfZW/gLs34bT2p97Wua075tbKNiqQqr/YnNRvdto3lHkwlppSlVrSlQqdWgflad8q3UeeQn26SWr/2FZkmlalSp1fEvmYTlV0AxHWR9vuXzbtdPdepn1XW4rVZF8liOik2DAlZcA6waDdC2/5+RSlh4lDSWQlZMtmVUvNFfUE1WnmqsUq1TgdaafNWZfNUm81Xn5KnB5GqovUKH2p/oAOsLBay4fGpTWG2bvvaSYvKpwVcmrxNVXrJBPiUkSY4sRe2QLBkFnTb5rKQKtCHpFDceNStHEW+e5M9XINmsvFit/O1/Ja7w9NPK0DC1BctVElul0thyFUdXKWAi8lhG4fbnajUBrTIlyrdaVWHVq79dq/6q3ezrEpNX661CNdoFavEUyJZRyLQq5LTKVlJxeRWTT3H55Hj8MrZftu1RKNmk3GS9cpNNqveWaJV/oKqDg+S1pV7xFSqPLZffaVWTXagGu0D1dpHa/MWKBkqU8Bcoz4oobBqVn2yQP7ZO/sg6BWLrZScjShhbcWPLSAraSQWthEJOi3JjdRpor9FAdUxmxj05qs3dS6ty9lJbLKnBDa+rT3KVRlmLt/ozYSspn1KJqLgdUGtOXyUCRXJiLbJjzfIk2tSskBpMruqdoErUqEqtUa5aN0lgxSy/ElZACU9AjicoxxuSPAHFHCkadxRNJBROrle5qcskK1pNQB+b/vrC6asmhdSmgCLGL9u21asgqD5hv/qoViVtS1TQ/KXsZFRRb1itnnzF7KA8Hlte25bl8arRV65aT7lqVKxCu00V1joVJeqktvVKtDXKjjXLcuKKy6O4vEpYPhnbJ8vjkzx+xeVRxPEomrQUMi0qctarxKyXkaUGT7Fa/SVqDZRrtb+/lnsHqFbF6mPWqF9imcrjKxU1XtWbXK13QirXeg1xlqiibbE8Ji4nWCyrqFImWKzm+holm2qUl1gvnxIKW20KW22SpHXB/lpTcqiiub3lj6aqIe1km6J2jiJ2rpJGKm77SmWti5Ufr1XEk6dVOXtrdWiYgol6DWp6R0XxNRpkVW/2/Y4ooJgdUsKXK/nzlPDmqE1BtZhUpdNyVWhxoly1Tp5KfXGV+iIq8rQp37Qox2lWjtMir9MmbzIib7JNsmwlPCE53qASnhwl7IBinpBs21Zvb7PK1KDceJ2cxjXytK6Rx4ml3nd/iRK5vWWZpAJNy+RPtmR+JmJ2SG3BcjX5SrXWLladKVDQiqtYjQo7jXIsr9Z7irRWRVqXDKk5YaklZhRNJFVotarAalHYapMJFsjKq5C/sEKFpkkFzYsVbv5SnkSr2jz5arbzFJdfBXar8kyz/MmI6oL99ZEzQG+1lKu/XacDrc80KPKBQvH6Da+hFdLa3MGKFu8tX0mVvJF18raslq91jWwn3l4JlVBMfjWZkOqTAVlOQqVWg8KJdfInm+V4QnK8ISU8ITVY+apJ5mlFNCS/36+yXK9KQ7ZChWWyK/aVt/cIeb0+OUv+Jc9X/5Kn9iO1ya8mJ6TGpE95pkWFznrlxtfK48RlLI8c2yNZtgpDPZ9C6vkIulG6dDub5gQDADYI+Tz66KZxPXbs7pCb27FE+4orrtC8efN0xx13aMiQIQqFQjrttNMUi8W2+jw+n6/D95ZlyXG6vzIgPz9f7777rhYsWKB//OMfuu6663TDDTfo7bffVmFhoebNm6fXXntN//jHP3Tvvffq5z//ud58800NHLgjn4WjM9555x0de+yxme/T0/4mTpyomTNnavXq1R2SoQMHDtTcuXN1+eWX6+6771a/fv30u9/9TuPG9czvVGfd+/x/9Ou2qRod+GyT+xzZ+iB4gFqDvZQX8Cov4FXCcdQcTag5kpSRydweCgZl5ZbIk1sqb16J7ECOLG9Alscnp2WdnIaVUuNKKRFNJR08Acmfp0BxX+WU9pcvr1SRlga1NNQp0lCnQKRGwbY18rdWywQLlCjdR4mSfZQI95PxBOV4fHK8uUr685UrS7mS8gJeBfwemUSjVq5YpsVffaXVq1bIyFJ5r77qX9lfleVFCiaapbb1Ujwi5feSCivlDYSl9UukZW9KK96Sok2S7ZUsjxQqlEr3ksr2lvIrpOZaJepXKNZYI1/ZEPn6jlRZbmnqRUtEpaZqqXFV6nybVkvRZimnRMotkXJKpdzS1NecYnk8PmX+75dMpOJqrUtNjSisVH4gX/mSBn/9vXGMYklHtmXJ57FkJWNS61qZWKsSkRbFY22KJ40SSUeO7VFxryr588pVZrdnBY2Roo2SZcv25SqUvj0ekdrWpWIO5KnZylVdxKM+RTkq2nh6pjFS6zrJ41O/YLhjlVZaIiq11UuRBimnRP5Aocraq/wUq5dqPky9Vk5SMsnUa11YKRUNlD+/typsWxWbe95OypM2H5ek8h143k20rJWq/yPVfCwFw1JRlVQ0UL783upj2+qz8b7rvlR8xXvy+oOy/LmSL6d9C6W+WnbqtXASki9HvpwSFWxmem3h188t/X4kIqnn8gYlb1B+25a/M+eQiEr1yyVJOcUDdZDt0SjHpH7Xo6mpjv2Lc+TzbD6rnNO+fV2xpKrOHH87bHy8LtUoJ+NSMibbnxovpKvMJKVex0i91FKX2gr7q7igrzrdSCDapKAvV4NsW4M2fs71S1P/TwjkSf48yZ+b2nw5CtoeBbsSf3dK/3/AG1KO19/x9tZ1qf8f5VfIH8iXX6kqsC39Tu3oRPiwtNnf977t2/GbuS8t2L7PtuQo9buzrVjz2p/vgG09Yb99pSMvlCQF2p97cyxt+Dxmd0gg7Q4xdJv0dEJyWACQnSzL6vSUvp7m9/uVTG57ysSrr76qSZMm6dRTT5WUqq5ZunTpTo5ug3322UevvvrqJjHttdde8nhSf7p6vV6NHTtWY8eO1fXXX6/CwkK9+OKL+u53vyvLsnT44Yfr8MMP13XXXacBAwbo6aef7tBHCTvHMcccs9X+YzNnztzsY957772dGFX3+mBFvYa+c51Gez6TY/tk55VLgfxUwmXvb8kecZr2z9+RNELXhNq3HeYrUr+hReo3dOS2991Y8aDUNuqsre9XVCVv5ejN/yHgDUhFA1JbV3m8Ul5ZatsG27YUtDdK/nsDUriPLKVmSfm29MA0y5KCm5mY5AtKvg1plzxJeflbeHxuydaP4Q2kkn7tP0NeSQXe9ph9JdLATk0K2v3llkiDj01t21I8SL7iQdver6s6835sjTcglQ7pcJPHtlQQSvV/cxWPL7VtjmVJoaLUVrodU74Cm/llsSypeGBq291s6f8D6Z+nHfmZwm5rzxjpdxLTCQEAe4qqqiq9+eabWrp0qfLy8rZYJTV06FA99dRTGj9+vCzL0rXXXrtTKqq25Cc/+YlGjx6tm2++WWeccYZef/113XfffXrggQckSX/729/05Zdf6qijjlJRUZGeffZZOY6jYcOG6c0339T8+fN1wgknqLy8XG+++aZqa2u1zz777LL44V5Jx+hf/3ubLvL8S45s2f/9lHuSCgAAYLO2MUt/z2Jb6UosklgAgN3bFVdcIY/Ho+HDh6usrGyLPa6mT5+uoqIiHXbYYRo/frzGjRunAw/cdSvwHnjggXr88cc1Z84cjRgxQtddd51uuukmTZo0SZJUWFiop556St/4xje0zz77aMaMGfrf//1f7bvvvgqHw3r55Zf1zW9+U3vttZeuueYa3XnnnTrppJN2Wfxwr+ee+z+d3/wbSVLLUdeSwAIAIAtYZhevc93Y2KiCggI1NDQoHA5v+wFd8OeFK/STJ/6tI4eW6o/nH9qtzw0A2P1EIhEtWbJEAwcOVDDYYx0Z0E229n7uzPEDus+uep8ikYjWTxuu3tZafdXrBA340eOp6SMAAGCP05Xxg6sqsdI9sSjEAgAAcK/6muXqba1V3HhUOfH3JLAAAMgSrkpiWfTEAgBgqy688ELl5eVtdrvwwgt7OjygU9J94eLyyg5RmQcAQLZwWWP3VBYryfKEAABs1k033aQrrrhis/cxTQ97CuMkJEmOuz6PBQAA2+CqJBbTCQEA2Lry8nKVl5f3dBjADklXYjlMIwQAIKu46uOr9hyWkmSxAAAAXMtJJiVJRiSxAADIJi5LYqUGMvTEAgAAcDEnlcRy5OnhQAAAwK7kziQWPbEAAABcy8n0xKISCwCAbOKqJFa6JxY5LAAAAPdKf2BJY3cAALKLq6786d6eTCcEAABwr/TqhPTEAgAgu7gqiZWeTpikFAsAkOVmzpypwsLCTu17ww03aNSoUTs1HqA7mfTqhPTEAgAgq7gqiZWeTkghFgAAgHuZdGN3i0osAACyiauSWOlxTJIsFgAAgGs5mdUJXTWUBQAA2+CqK78nvTohSSwAwG7OcRxNmzZNAwcOVCgU0siRI/Xkk0/KcRz169dPDz74YIf933vvPdm2ra+++kqSNH36dO23337Kzc1VZWWlLr74YjU3N3dbbDfddJP69eunQCCgUaNG6fnnn8/cH4vFNHnyZPXu3VvBYFADBgzQtGnTJEnGGN1www3q37+/AoGA+vTpox//+MfdEheQ0Z7EMu4aygIAgG3w9nQA3clOr05ITywAyE7GSPHWnjm2L2dDSXAnTJs2TX/60580Y8YMDR06VC+//LL+67/+S3//+9911llnafbs2brooosy+8+aNUuHH364BgwYIEmybVv33HOPBg4cqC+//FIXX3yxrrzySj3wwAM7fCp333237rzzTj300EM64IAD9PDDD+vb3/62PvzwQw0dOlT33HOPnnnmGT3++OPq37+/li9fruXLl0uS/vznP+vXv/615syZo3333VfV1dX697//vcMxARvL9MSySGIBAJBN3JXEylRi9XAgAICeEW+Vbu3TM8f+2SrJn9upXaPRqG699Va98MILGjNmjCRp0KBBeuWVV/TQQw/pyiuv1J133qlly5apf//+chxHc+bM0TXXXJN5jssuuyzz76qqKv3iF7/QhRde2C1JrDvuuENXXXWVzjzzTEnSL3/5S/3zn//UXXfdpfvvv1/Lli3T0KFDdcQRR8iyrExiTZKWLVumXr16aezYsfL5fOrfv78OOeSQHY4J2JiTqcSiJxYAANnEVR9ftRdiMZ0QALBb++KLL9Ta2qrjjz9eeXl5me3RRx/V4sWLNWrUKO2zzz6aPXu2JOmll15STU2Nvv/972ee44UXXtBxxx2nvn37Kj8/X//93/+ttWvXqrV1xyrRGhsbtWrVKh1++OEdbj/88MP18ccfS5ImTZqkRYsWadiwYfrxj3+sf/zjH5n9vv/976utrU2DBg3SBRdcoKefflqJRGKHYgI2kemJxeqEAABkE3dWYlGKBQDZyZeTqojqqWN3Urp31dy5c9W3b98O9wUCAUnSOeeco9mzZ+vqq6/W7NmzdeKJJ6qkpESStHTpUp188sm66KKLdMstt6i4uFivvPKKzj//fMViMeXkdD6W7XHggQdqyZIleu655/TCCy/o9NNP19ixY/Xkk0+qsrJSn376qV544QXNmzdPF198sX71q1/ppZdeks/n26lxIXs4pr0Si9UJAQDIKq5KYnlsphMCQFazrE5P6etJw4cPVyAQ0LJly3T00Udvdp+zzz5b11xzjRYuXKgnn3xSM2bMyNy3cOFCOY6jO++8U7adKqp+/PHHuyW2cDisPn366NVXX+0Q26uvvtphWmA4HNYZZ5yhM844Q6eddppOPPFErVu3TsXFxQqFQho/frzGjx+vSy65RHvvvbfef/99HXjggd0SI5DuiUVjdwAAsourkljpD+OSTCcEAOzG8vPzdcUVV+jyyy+X4zg64ogj1NDQoFdffVXhcFgTJ05UVVWVDjvsMJ1//vlKJpP69re/nXn8kCFDFI/Hde+992r8+PF69dVXOyS5dtRPf/pTXX/99Ro8eLBGjRqlRx55RIsWLdKsWbMkpVZG7N27tw444ADZtq0nnnhCvXr1UmFhoWbOnKlkMqlDDz1UOTk5+tOf/qRQKNShbxaww1idEACArOSqJFa6EsuQxAIA7OZuvvlmlZWVadq0afryyy9VWFioAw88UD/72c8y+5xzzjm6+OKLNWHCBIVCocztI0eO1PTp0/XLX/5SU6dO1VFHHaVp06ZpwoQJ3RLbj3/8YzU0NOgnP/mJampqNHz4cD3zzDMaOnSopFQS7vbbb9fnn38uj8ej0aNH69lnn5Vt2yosLNRtt92mKVOmKJlMar/99tNf//rXzFRIoDuwOiEAANnJMrs449PY2KiCggI1NDQoHA5363N/tqZJJ/z6ZRXn+vXutcd363MDAHY/kUhES5Ys0cCBAxUMBns6HOygrb2fO3P8gO6zq96nRfP+pFGvXqJPvPto72ve2GnHAQAAO19Xxg+u+vgqvTphkqZYAAAArpXpiUUlFgAAWcVVV/5g3Yea5Hleh5l3ezoUAAB2G/vuu6/y8vI2u6X7XAF7FHpiAQCQlVzVEyu0+k3d4HtUz5oxkq7u6XAAANgtPPvss4rH45u9r6KiYhdHA+w4x6SSWPTEAgAgu7gqiWXbntRXOT0cCQAAuw9WBoTrtE8nzCxNDQAAsoK7Pr5KJ7EMSSwAAADXSldiydPDgQAAgF3JVUksqz2JZYnG7gCQTXbxQrvYSXgf0Vkm2d4Ti+mEAABkFVdd+W07dTpMJwSA7ODz+SRJra2tPRwJukP6fUy/r8CWpBOeJLEAAMguruqJtWE6YbKHAwEA7Aoej0eFhYWqqamRJOXk5MiiR84exxij1tZW1dTUqLCwUB4PU8SwDU5CEqsTAgCQbVyVxLIyjd2NjDH8IQMAWaBXr16SlElkYc9VWFiYeT+BrWrvf0olFgAA2cVVSSzb2rA6YdIx8npIYgGA21mWpd69e6u8vFzxeLynw8F28vl8VGCh00x6dUIqsQAAyCquSmJZ7YNfjxw59IYFgKzi8XhIggBZwqQrsWySWAAAZBNXXfkz0wktI4cVjgAAAFzJMvTEAgAgG7nqym/bG6YTksQCAABwp/R0QnpiAQCQXVx15U9XYjGdEAAAwMXapxOKJBYAAFnFVVf+dE8sW0ZJslgAAADulFmdkD54AABkE1clsez2T+NsOTJMJwQAAHAnJ5n6SiUWAABZZYeu/Lfddpssy9Jll13WTeHsGMtOLbbokUMlFgAAgFulK7Hc9XksAADYhu2+8r/99tt66KGHtP/++3dnPDvE3mg6ITksAAAAlzJUYgEAkI2268rf3Nysc845R7/97W9VVFTU3TFtv42mE7I6IQAAgEulVye0SWIBAJBNtuvKf8kll+hb3/qWxo4d293x7JgOqxOSxAIAAHCl9OqETCcEACCreLv6gDlz5ujdd9/V22+/3an9o9GootFo5vvGxsauHrLzrPR0QofphAAAAC5l0j2xbFYnBAAgm3Tp46vly5fr0ksv1axZsxQMBjv1mGnTpqmgoCCzVVZWblegnbLxdEKyWAAAAK5k0RMLAICs1KUr/8KFC1VTU6MDDzxQXq9XXq9XL730ku655x55vV4lk8lNHjN16lQ1NDRktuXLl3db8JtgOiEAAID7tffEIokFAEB26dJ0wuOOO07vv/9+h9vOPfdc7b333rrqqqvk8Wxa0h0IBBQIBHYsys5KTye0jGJUYgEAALhTeyWWoScWAABZpUtJrPz8fI0YMaLDbbm5uSopKdnk9h5hWZLoiQUAAOBq6cbu9MQCACCruOvjK6YTAgAAuJ5lmE4IAEA26vLqhF+3YMGCbgijm2RWJzQksQAAANyKJBYAAFnJXVf+DqsT9nAsAAAA2ClYnRAAgOzkris/0wkBAADcr70Sy9ATCwCArOKuJBbTCQEAANyP6YQAAGQld135N1qdMMnyhAAAAO7UnsSyLCqxAADIJu5KYnWYTtjDsQAAAGCnsOmJBQBAVnLXlZ/phAAAAO6XHufREwsAgKzisiTWxqsTksQCAABwow2rE1o9GwgAANil3JXEYjohAABAFkg3dqcSCwCAbOKuJFZ6OqFl5DhODwcDAACAncFKr07IdEIAALKKy5JYG07HcZI9GAgAAAB2mnQSi8buAABkFXdd+e2NkljJRA8GAgAA0PPuv/9+VVVVKRgM6tBDD9Vbb7211f3vuusuDRs2TKFQSJWVlbr88ssViUR2UbSdZ7U3drdsdw1lAQDA1rnryr9xXwSmEwIAgCz22GOPacqUKbr++uv17rvvauTIkRo3bpxqamo2u//s2bN19dVX6/rrr9fHH3+s3//+93rsscf0s5/9bBdHvm2W0o3dmU4IAEA2cVcSa6O+CEmmEwIAgCw2ffp0XXDBBTr33HM1fPhwzZgxQzk5OXr44Yc3u/9rr72mww8/XGeffbaqqqp0wgkn6Kyzztpm9VaPYDohAABZyV1X/o0GMoYkFgAAyFKxWEwLFy7U2LFjM7fZtq2xY8fq9ddf3+xjDjvsMC1cuDCTtPryyy/17LPP6pvf/OYWjxONRtXY2Nhh2xXs9iSWRWN3AACyirenA+hWG5WUk8QCAADZqq6uTslkUhUVFR1ur6io0CeffLLZx5x99tmqq6vTEUccIWOMEomELrzwwq1OJ5w2bZpuvPHGbo29U9JJLCqxAADIKu668tsksQAAALbHggULdOutt+qBBx7Qu+++q6eeekpz587VzTffvMXHTJ06VQ0NDZlt+fLluyRWS+3TCanEAgAgq7isEovphAAAAKWlpfJ4PFqzZk2H29esWaNevXpt9jHXXnut/vu//1s/+MEPJEn77befWlpa9MMf/lA///nPZW9mJcBAIKBAIND9J7ANVmY6obs+jwUAAFvnriu/ZcmRJUlySGIBAIAs5ff7ddBBB2n+/PmZ2xzH0fz58zVmzJjNPqa1tXWTRJXHk6p0MsbsvGC3g01jdwAAspK7KrEkObJlKyk5Tk+HAgAA0GOmTJmiiRMn6uCDD9Yhhxyiu+66Sy0tLTr33HMlSRMmTFDfvn01bdo0SdL48eM1ffp0HXDAATr00EP1xRdf6Nprr9X48eMzyazdR7oSy3VDWQAAsBWuu/Ib2ZKSMk6ip0MBAADoMWeccYZqa2t13XXXqbq6WqNGjdLzzz+fafa+bNmyDpVX11xzjSzL0jXXXKOVK1eqrKxM48eP1y233NJTp7BFmdUJLauHIwEAALuS65JYjmVLhumEAAAAkydP1uTJkzd734IFCzp87/V6df311+v666/fBZHtGItKLAAAspLrGgmY9CkxnRAAAMCVLHpiAQCQlVx35U83dmd1QgAAAHfaUInluqEsAADYCtdd+Y2VXkWHJBYAAIAb2Zkk1u7WcB4AAOxMrktiOe2nRCUWAACAO1nGpL6SxAIAIKu4LollLHpiAQAAuJml1IeVJLEAAMgu7ktiUYkFAADganamEst1Q1kAALAVrrvyO+2VWA5JLAAAAFey6IkFAEBWcl0SK12JlVl6GQAAAK6SbuwuiyQWAADZxH1JLCs9nTDRw5EAAABgZ0h/WGkznRAAgKziuit/ejqhoRILAADAlWy198TyUIkFAEA2cV0SK3NKSXpiAQAAuJGdXp2Q6YQAAGQV1yWxnPRghkosAAAAV7LaVye0qcQCACCruC6JlemJZajEAgAAcKN0Y3fLct1QFgAAbIULr/ztp+SQxAIAAHAjK53EsqnEAgAgm7guiZWuxJLDdEIAAAA38pDEAgAgK7kuiZVendBhOiEAAIArWaxOCABAVnJdEkvpSiySWAAAAK6U7onlsd03lAUAAFvmuiu/aV+d0KInFgAAgCulk1iyvT0bCAAA2KVcmMRKr05oejgSAAAA7AzpJJbN6oQAAGQVF175WZ0QAADAzez2DyttLz2xAADIJq5LYm1YnZAkFgAAgBulK7EsiyQWAADZxIVJrPbBjHF6NhAAAADsFHb76oS2h55YAABkE9clsVidEAAAwMWMkW21J7Fsq4eDAQAAu5Lrkljp6YQWSSwAAAD32aja3mJ1QgAAsooLk1ip6YTGYTohAACA62zU99S26YkFAEA2cV0Sa8N0QpJYAAAAbmM2qra3PCSxAADIJq5NYjGdEAAAwH2c5IYxnsd231AWAABsmeuu/KxOCAAA4F6Os3ElFj2xAADIJq5LYslmOiEAAIBbJZMb98Ry31AWAABsmeuu/CbTE4vphAAAAG6z8eI9HiqxAADIKq5LYql9OqFFJRYAAIDrJJ1E5t+sTggAQHZxYRKr/ZQcKrEAAADcxrSP8RxjMZ0QAIAs474rP5VYAAAArmUS7UksWbKtHg4GAADsUu5LYrWXlVsiiQUAAOA2yfYPKpOy5SGLBQBAVnFfEovphAAAAK5lkqmeWEaWLIskFgAA2cS1SSymEwIAALiP0/5BZdKFw1gAALB17rv6Z1apIYkFAADgNsZJjfGMC4exAABg69x39c9UYjGdEAAAwG2oxAIAIHu57+rP6oQAAACu5WzUEwsAAGQX9yWx0tMJSWIBAAC4j2MkUYkFAEA2ct3V32qfTmiTxAIAAHCdpEMlFgAA2cp1SSxjpSux6IkFAADgNqa9J5ZjuW4YCwAAtsF1V3/LTvfEMj0cCQAAALpdOonlvmEsAADYBvdd/e1UabklKrEAAADcxnFSLSNIYgEAkH26dPV/8MEHtf/++yscDiscDmvMmDF67rnndlZs22VDJRY9sQAAANzGaa/EoicWAADZp0tJrH79+um2227TwoUL9c477+gb3/iGTjnlFH344Yc7K76us7ypLySxAAAAXMdhOiEAAFnL25Wdx48f3+H7W265RQ8++KDeeOMN7bvvvt0a2PbKVGKJJBYAAIDr0NgdAICs1aUk1saSyaSeeOIJtbS0aMyYMVvcLxqNKhqNZr5vbGzc3kN2jpUqLbdZnRAAAMB1DD2xAADIWl2++r///vvKy8tTIBDQhRdeqKefflrDhw/f4v7Tpk1TQUFBZqusrNyhgLfJbs/LsTohAACA62zoiUUSCwCAbNPlq/+wYcO0aNEivfnmm7rooos0ceJEffTRR1vcf+rUqWpoaMhsy5cv36GAtyU9ndBmOiEAAID7GKYTAgCQrbo8ndDv92vIkCGSpIMOOkhvv/227r77bj300EOb3T8QCCgQCOxYlF1gtU8ntJhOCAAA4DpOMvVBJasTAgCQfXb4IyzHcTr0vOpx7dMJaewOAADgQulKLHl6OBAAALCrdakSa+rUqTrppJPUv39/NTU1afbs2VqwYIH+/ve/76z4uiwzndCQxAIAAHCbdGN3Y1GJBQBAtulSEqumpkYTJkzQ6tWrVVBQoP33319///vfdfzxx++s+LrMslPFZVRiAQAAuI+hsTsAAFmrS0ms3//+9zsrjm5jWalKLItKLAAAANfJVGKRxAIAIOu47+rP6oQAAACuZUwi9ZXphAAAZB3XJbFsemIBAAC4VroSi8buAABkH9clseiJBQAA4F6ZnlhUYgEAkHXcl8TypNp8MZ0QAADAhUx6dUIqsQAAyDbuS2JZqVNiOiEAAIALZVYnpBILAIBs474kVntPLKYTAgCAbHf//ferqqpKwWBQhx56qN56662t7l9fX69LLrlEvXv3ViAQ0F577aVnn312F0XbOU66JxaVWAAAZB1vTwfQ3SxWJwQAANBjjz2mKVOmaMaMGTr00EN11113ady4cfr0009VXl6+yf6xWEzHH3+8ysvL9eSTT6pv37766quvVFhYuOuD35r2SixRiQUAQNZxbxKL6YQAACCLTZ8+XRdccIHOPfdcSdKMGTM0d+5cPfzww7r66qs32f/hhx/WunXr9Nprr8nn80mSqqqqdmXInWIyPbFcN6EAAABsg+uu/jaVWAAAIMvFYjEtXLhQY8eOzdxm27bGjh2r119/fbOPeeaZZzRmzBhdcsklqqio0IgRI3TrrbcqmUxudv8eY9KrE7puGAsAALbBtZVYlkwPRwIAANAz6urqlEwmVVFR0eH2iooKffLJJ5t9zJdffqkXX3xR55xzjp599ll98cUXuvjiixWPx3X99ddv9jHRaFTRaDTzfWNjY/edxJa098QyoicWAADZxnUfYVme1Cl55MgYElkAAACd4TiOysvL9Zvf/EYHHXSQzjjjDP385z/XjBkztviYadOmqaCgILNVVlbu9DhNuieWRU8sAACyjfuSWHaquMyWI3JYAAAgG5WWlsrj8WjNmjUdbl+zZo169eq12cf07t1be+21lzyeDRVO++yzj6qrqxWLxTb7mKlTp6qhoSGzLV++vPtOYksMqxMCAJCtXJfE2tATyyhJFgsAAGQhv9+vgw46SPPnz8/c5jiO5s+frzFjxmz2MYcffri++OILOc6GvqKfffaZevfuLb/fv9nHBAIBhcPhDtvOtqESy3XDWAAAsA2uu/pbduqUbDlySGIBAIAsNWXKFP32t7/VH/7wB3388ce66KKL1NLSklmtcMKECZo6dWpm/4suukjr1q3TpZdeqs8++0xz587VrbfeqksuuaSnTmHz0qsTum8YCwAAtsF9jd09qVPyyJHDAoUAACBLnXHGGaqtrdV1112n6upqjRo1Ss8//3ym2fuyZctk2xsSQZWVlfr73/+uyy+/XPvvv7/69u2rSy+9VFdddVVPncLmsTohAABZy3VJrA3TCanEAgAA2W3y5MmaPHnyZu9bsGDBJreNGTNGb7zxxk6OagelVyckiQUAQNZx3dXfah/Q2BY9sQAAAFzH0BMLAIBs5bqrv+3dMJ3QMJ0QAADAVYyhEgsAgGzluqs/0wkBAADcy2J1QgAAspbrrv72RqsTMp0QAADAXUz7+M5Ynh6OBAAA7GquS2JZ9karE5LEAgAAcBd6YgEAkLXcd/W30tMJTXrxGgAAALiElW56ShILAICs476rv7VhOiGVWAAAAO5i2nti0dgdAIDs476rf3tPLI8cJR2SWAAAAK6SqcSiJxYAANnGfUmsjaYTUogFAADgLlamJ5bVs4EAAIBdzoVJLKYTAgAAuFZ7JRarEwIAkH3cl8SyUwMajxwlSWIBAAC4C43dAQDIWu67+neYTkgSCwAAwE0semIBAJC1XJjEap9OaBklkySxAAAAXCXdE8t23zAWAABsnfuu/vaGT+UcJ9GDgQAAAKDbtVfa0xMLAIDs474k1kb9EYyT7MFAAAAA0N3SqxNa9MQCACDruO/qv9GAxkk6PRgIAAAAuh2N3QEAyFruu/pvNJ3QMJ0QAADAVdKN3Q1JLAAAso77rv4b9UdIMp0QAADAXdqTWJZNTywAALKNC5NYG50SSSwAAABXSffEYjohAADZx31X/41XJ0ySxAIAAHCV9tUJRSUWAABZx31JrI0bu1OJBQAA4CpUYgEAkL3cd/W3LDmyJEmGJBYAAICrWKxOCABA1nLl1d9pPy2SWAAAAO5iqb2xO0ksAACyjiuv/huSWE4PRwIAAIBula7EoicWAABZx9VJLMdJ9HAkAAAA6E7p6YSWRRILAIBs484kVrq8nOmEAAAArpKeTijblcNYAACwFa68+m+oxGI6IQAAgJtkKrFIYgEAkHVcefU3rE4IAADgShtWJ/T2bCAAAGCXc2USK12JJXpiAQAAuEomiWVbPRsIAADY5VyZxDLtjT4dx/RwJAAAAOhO6Z5YNHYHACD7uDKJla7EMlRiAQAAuAqrEwIAkL1cmcQyVnt5OT2xAAAAXCVTieUhiQUAQLZxZRLLUWpQYwxJLAAAADfZ0NjdlcNYAACwFa68+jvtgxon6fRwJAAAAOhO6Uos2VRiAQCQbVyZxJLapxNSiQUAAOAqdnsllm27dBgLAAC2yJVX//R0QnpiAQAAuEumEovG7gAAZB1XJrFMejqhw3RCAAAAN8msTkglFgAAWceVV/90EotKLAAAAHexZFJfbW8PRwIAAHY1VyaxnPbTYnVCAAAAd7Hbx3eWbfVwJAAAYFdzZRKLSiwAAAB3ohILAIDs5eoklqEnFgAAgKvY7Y3dLcuVw1gAALAVrrz6m/RpMZ0QAADAVdKN3W0PqxMCAJBt3JnESi+5TBILAADAVez26YSiEgsAgKzjyqt/ZjphkumEAAAAbmIp3didSiwAALKNO5NYTCcEAABwpXQlFtMJAQDIPu5MYmUau5PEAgAAcBPb0NgdAIBs5cqrfzqJJcN0QgAAADex0qsTMp0QAICs4+okliGJBQAA4Co2SSwAALKWK5NYal+d0GI6IQAAgKtkemKRxAIAIOt0KYk1bdo0jR49Wvn5+SovL9d3vvMdffrppzsrtu1GY3cAAAB32lCJ5c7PYgEAwJZ16er/0ksv6ZJLLtEbb7yhefPmKR6P64QTTlBLS8vOim/7tH8yx3RCAAAAd7HaK7Es29vDkQAAgF2tS1f/559/vsP3M2fOVHl5uRYuXKijjjqqWwPbEemeWEwnBAAAcBdPeyWWx0MlFgAA2WaHPsJqaGiQJBUXF29xn2g0qmg0mvm+sbFxRw7ZKUwnBAAAcCdWJwQAIHtt90dYjuPosssu0+GHH64RI0Zscb9p06apoKAgs1VWVm7vITsv3SPBmJ1/LAAAAOwy3vYkFo3dAQDIPtudxLrkkkv0wQcfaM6cOVvdb+rUqWpoaMhsy5cv395DdpppX51QTCcEAABwj40+oKQSCwCA7LNd0wknT56sv/3tb3r55ZfVr1+/re4bCAQUCAS2K7jtlu6JxXRCAAAA99joA0oqsQAAyD5dSmIZY/Q///M/evrpp7VgwQINHDhwZ8W1Q9KN3emJBQAA4CIbrTxteUhiAQCQbbqUxLrkkks0e/Zs/d///Z/y8/NVXV0tSSooKFAoFNopAW6X9umEhp5YAAAA7mGoxAIAIJt1qSfWgw8+qIaGBh1zzDHq3bt3Znvsscd2Vnzbp70Sy6YSCwAAwD02qsSyqcQCACDrdHk64Z6Axu4AAAAutNHYzrK3e30iAACwh3Ln1T89qNlDkm4AAADYNtNhOuF2rU8EAAD2YO5MYrE6IQAAgO6//35VVVUpGAzq0EMP1VtvvdWpx82ZM0eWZek73/nOzg2wi0xyw3RCDz2xAADIOi5NYrUPakhiAQCALPXYY49pypQpuv766/Xuu+9q5MiRGjdunGpqarb6uKVLl+qKK67QkUceuYsi7bykQ2N3AACymbuTWGI6IQAAyE7Tp0/XBRdcoHPPPVfDhw/XjBkzlJOTo4cffniLj0kmkzrnnHN04403atCgQbsw2s5xnETqq7Fke6wejgYAAOxqLk1itU8npLE7AADIQrFYTAsXLtTYsWMzt9m2rbFjx+r111/f4uNuuukmlZeX6/zzz+/UcaLRqBobGztsO5NxUtMJk7JlWySxAADINu5MYtn0xAIAANmrrq5OyWRSFRUVHW6vqKhQdXX1Zh/zyiuv6Pe//71++9vfdvo406ZNU0FBQWarrKzcobi3xUmmxnaOLJJYAABkIVcmsUymJ5az9R0BAACgpqYm/fd//7d++9vfqrS0tNOPmzp1qhoaGjLb8uXLd2KUkuOkk1h2ZjFqAACQPVy5NrGVqcQiiQUAALJPaWmpPB6P1qxZ0+H2NWvWqFevXpvsv3jxYi1dulTjx4/P3Oa0T93zer369NNPNXjw4E0eFwgEFAgEujn6Ldu4EstLJRYAAFnHnZ9htVdiWWI6IQAAyD5+v18HHXSQ5s+fn7nNcRzNnz9fY8aM2WT/vffeW++//74WLVqU2b797W/r2GOP1aJFi3b6NMHOMu2VWEnZ8pDEAgAg67iyEiuzOqFhdUIAAJCdpkyZookTJ+rggw/WIYccorvuukstLS0699xzJUkTJkxQ3759NW3aNAWDQY0YMaLD4wsLCyVpk9t7Uno6oZElclgAAGQflyaxaOwOAACy2xlnnKHa2lpdd911qq6u1qhRo/T8889nmr0vW7ZM9h7WWMrZqBLLIosFAEDWcWUSy7JTlVg2PbEAAEAWmzx5siZPnrzZ+xYsWLDVx86cObP7A9pBJlOJtWcl3wAAQPdw5wgg/akiSSwAAADXcJKpsV3SpUNYAACwde4cAaQbuzOdEAAAwDWMk0h9FVMJAQDIRq5MYmWmE4pKLAAAALcw7VX2jjuHsAAAYBvcOQLITCdkdUIAAAC3cJKpKnuSWAAAZCdXjgCs9HRCMZ0QAADALdKN3R3LlUNYAACwDe4cAVisTggAAOA2G1YnpCcWAADZyJVJLMuTSmKxOiEAAIB7GIeeWAAAZDN3jgDaS8xp7A4AAOAememELh3CAgCArXPlCCC9OqFl6IkFAADgFg5JLAAAsporRwCZJJZYnRAAAMAtMj2xLHpiAQCQjVyZxMpMJ6QnFgAAgGts6Inl6eFIAABAT3BlEmtDJRbTCQEAANyCSiwAALKbO5NY7asT2obphAAAAG5hDKsTAgCQzVw5ArAsb+orqxMCAAC4hnESqa/uHMICAIBtcOUIwLJTJeY2qxMCAAC4RqYnluXKISwAANgGV44ALDtdicV0QgAAALfI9MQSPbEAAMhGLk1itffEYjohAACAa6R7YhmL1QkBAMhGLk1ipU7LNiSxAAAA3MIkqcQCACCbuTSJlfp0jsbuAAAA7mHa+506VGIBAJCVXJrESvXEYjohAACAizjpsR2VWAAAZCNXJrHs9HRCklgAAACusWF1QiqxAADIRq5MYjGdEAAAwIVMIvWFSiwAALKSq5NYHhq7AwAAuEa6EovVCQEAyE6uTGLZ7UksphMCAAC4SPsHlFRiAQCQnVyZxFJmOqHp4UAAAADQXYyTWp3QWO4cwgIAgK1z5QjA9rRPJ6QSCwAAwDVIYgEAkN1cOQJgOiEAAIALmVSVPT2xAADITq5MYrE6IQAAgAs56dUJXTmEBQAA2+DKEUC6EssjR8bQFwsAAMANTHrlaaYTAgCQlVw5ArA2SmI55LAAAABcwUqvTkgSCwCArOTKEUC6sbsloyRZLAAAAFcwTnsSS1YPRwIAAHqCK5NYlmfjSiySWAAAAG5gTGp1QtHYHQCArOTKJJbHJokFAADgOk4qicV0QgAAspMrRwDWRtMJmU0IAADgEu0fTpLEAgAgO7lyBGDbXkmpSix6YgEAALhEZjqhK4ewAABgG1w5ArDt1Gl5LUeG6YQAAADuwOqEAABkNVeOANKVWJLkUIkFAADgDg6VWAAAZDNXjgBsz4YVa5LJRA9GAgAAgG6TqcRidUIAALKRK5NYG386Z0hiAQAAuEOmJ5bVs3EAAIAe4c4klr1RJVZ6sAMAAIA9W7rXKZVYAABkJXcmsTYa2Jik04OBAAAAoNuwOiEAAFnNnSOAjQY2DtMJAQAA3IGeWAAAZDV3JrE2mk5oHKYTAgAAuIHF6oQAAGQ1d44ANvp0ziGJBQAA4A6ZnljuHMICAICtc+cIYKMVa5wkSSwAAABXoCcWAABZzZ0jAMtSUqlEFtMJAQAA3KJ9wR6bnlgAAGQjdyaxJDlKDW6YTggAAOAOltOexKISCwCArOTaEYCTrsRiOiEAAIA7sDohAABZzcVJrNSpUYkFAADgDlZ7TyzLtraxJwAAcCPXJ7HoiQUAAOASJj2dkEosAACykYuTWDR2BwAAcBPL0BMLAIBs5toRwIbG7k4PRwIAAIDuQSUWAADZrMtJrJdfflnjx49Xnz59ZFmW/vKXv+yEsHbchumEiR6OBAAAAN0h3ROLSiwAALJTl0cALS0tGjlypO6///6dEU+3MRarEwIAALiJZUzqHzaVWAAAZCNvVx9w0kkn6aSTTtoZsXQrGrsDAAC4C5VYAABkN9eOADJJLEMSCwAAwBXaG7tbVGIBAJCVulyJ1VXRaFTRaDTzfWNj484+pCTJUIkFAADgKnZ7Y3eLSiwAALLSTh8BTJs2TQUFBZmtsrJyZx9SkuRY6SQWqxMCAAC4QnsllqESCwCArLTTk1hTp05VQ0NDZlu+fPnOPqQkemIBAAC4jWWoxAIAIJvt9OmEgUBAgUBgZx9mE8YiiQUAAOAmVvt0QlYnBAAgO3X5Y6zm5mYtWrRIixYtkiQtWbJEixYt0rJly7o7th2yoRIr0cORAAAA9Iz7779fVVVVCgaDOvTQQ/XWW29tcd/f/va3OvLII1VUVKSioiKNHTt2q/v3BCqxAADIbl0eAbzzzjs64IADdMABB0iSpkyZogMOOEDXXXddtwe3IzZUYpkejgQAAGDXe+yxxzRlyhRdf/31evfddzVy5EiNGzdONTU1m91/wYIFOuuss/TPf/5Tr7/+uiorK3XCCSdo5cqVuzjyLUsnsajEAgAgO3U5iXXMMcfIGLPJNnPmzJ0Q3vYzVGIBAIAsNn36dF1wwQU699xzNXz4cM2YMUM5OTl6+OGHN7v/rFmzdPHFF2vUqFHae++99bvf/U6O42j+/Pm7OPIts1idEACArObaEUB6OqEMPbEAAEB2icViWrhwocaOHZu5zbZtjR07Vq+//nqnnqO1tVXxeFzFxcU7K8wuoxILAIDsttMbu/cUJzOd0OnhSAAAAHaturo6JZNJVVRUdLi9oqJCn3zySaee46qrrlKfPn06JMK+LhqNKhqNZr5vbGzcvoA7KdMTy3bt57AAAGAr3DsCYHVCAACA7XLbbbdpzpw5evrppxUMBre437Rp01RQUJDZKisrd2pcmemEVGIBAJCVXJvEykwnJIkFAACyTGlpqTwej9asWdPh9jVr1qhXr15bfewdd9yh2267Tf/4xz+0//77b3XfqVOnqqGhIbMtX758h2PfGptKLAAAspprRwDGSn1C5zCdEAAAZBm/36+DDjqoQ1P2dJP2MWPGbPFxt99+u26++WY9//zzOvjgg7d5nEAgoHA43GHbmdKVWLJc2xEDAABshWtHAOmeWBaN3QEAQBaaMmWKJk6cqIMPPliHHHKI7rrrLrW0tOjcc8+VJE2YMEF9+/bVtGnTJEm//OUvdd1112n27NmqqqpSdXW1JCkvL095eXk9dh4bS/fEsqnEAgAgK7k2iZUuMqMnFgAAyEZnnHGGamtrdd1116m6ulqjRo3S888/n2n2vmzZsg7JoAcffFCxWEynnXZah+e5/vrrdcMNN+zK0LfIkmn/Bz2xAADIRq5NYpl0Y3fDdEIAAJCdJk+erMmTJ2/2vgULFnT4funSpTs/oB1kK/XhpOWhEgsAgGzk2hGAo/ZP6JhOCAAA4AqWSVViWVRiAQCQlVybxJJlpb4ynRAAAMAV7PbG7jaVWAAAZCXXjgDSqxMaVicEAABwhQ2rE1KJBQBANnJxEqv91KjEAgAAcAW7vdepZZPEAgAgG7k/iUVjdwAAAFdIr05ok8QCACArZUESi0osAAAAN8isTmi5dggLAAC2wrUjAMPqhAAAAK6SWZ3Q4+3hSAAAQE9wbRJLmZ5YTCcEAABwg/TqhJbt3iEsAADYMteOANLTCQ09sQAAAFxhQxKLnlgAAGQjFyexUoMbi+mEAAAArpBOYtlUYgEAkJXcOwLITCckiQUAAOAG6dUJLZueWAAAZCP3J7GYTggAAOAKnkwlFtMJAQDIRq5NYqWnE5LEAgAAcIcNlViuHcICAICtcO8IwLJSX+mJBQAAsOczJlOJRWN3AACyk2uTWMZON3anEgsAAGCPZ0zmn7aHnlgAAGQj1yaxlJ5OSGN3AACAPd9G1fW2bfVgIAAAoKe4OIlFY3cAAADX2GhMZ1GJBQBAViKJBQAAgN3fRtX1tkVPLAAAspFrk1jp1QktGrsDAADs+Tb6YNL2uHYICwAAtsK9IwAqsQAAANxjow8mLSqxAADISi5OYrE6IQAAgGt0qMQiiQUAQDZybxLLbh/cMJ0QAABgz+dsSGJ5aOwOAEBWcm8Sq306IZVYAAAALrBxJZZl9WAgAACgp7j3Yyx6YgEAALhH+5guaSxZtns/hwWA3UEymVQ8Hu/pMOASPp9Pnm5qBeDeJJbN6oQAAABuYZyELEmObHlsKrEAYGcwxqi6ulr19fU9HQpcprCwUL169ZK1g9XU7k1iMZ0QAADANZLJpLxKJbHIYQHAzpFOYJWXlysnJ2eHEw6AMUatra2qqamRJPXu3XuHns+9SaxMY3eSWAAAAHs6p72xuyNLNlksAOh2yWQyk8AqKSnp6XDgIqFQSJJUU1Oj8vLyHZpa6N6GAkwnBAAAcA3jpMZ0Sdk0dgeAnSDdAysnJ6eHI4EbpX+udrTXmnuTWO3TCW0qsQAAAPZ4jpNIfZXFdEIA2ImYQoidobt+rlybxLKsdHkaSSwAAIA93YbphFRiAQCQrVybxJJNY3cAAAC3cJKp6YSpSiySWACAnaOqqkp33XVXT4eBLXBtY/d0JRY9sQAAAPZ8G/fE8jCfEACwkWOOOUajRo3qluTT22+/rdzc3B0PCjuFa5NYGxq7mx4OBAAAADvKSaZ6YhnZ9MQCAHSJMUbJZFJe77ZTIGVlZbsgop4Ti8Xk9/t7Oozt5t7phO2N3S1RiQUAALCnM07qg8mkbJoOAwAyJk2apJdeekl33323LMuSZVmaOXOmLMvSc889p4MOOkiBQECvvPKKFi9erFNOOUUVFRXKy8vT6NGj9cILL3R4vq9PJ7QsS7/73e906qmnKicnR0OHDtUzzzzTqdiSyaTOP/98DRw4UKFQSMOGDdPdd9+9yX4PP/yw9t13XwUCAfXu3VuTJ0/O3FdfX68f/ehHqqioUDAY1IgRI/S3v/1NknTDDTdo1KhRHZ7rrrvuUlVVVYfX5zvf+Y5uueUW9enTR8OGDZMk/fGPf9TBBx+s/Px89erVS2effbZqamo6PNeHH36ok08+WeFwWPn5+TryyCO1ePFivfzyy/L5fKquru6w/2WXXaYjjzyyU6/N9nJtJZbVXonF6oQAAAB7PuOkK7FIYAHArmCMUVu8Z4pCQj5Ppz+wuPvuu/XZZ59pxIgRuummmySlki+SdPXVV+uOO+7QoEGDVFRUpOXLl+ub3/ymbrnlFgUCAT366KMaP368Pv30U/Xv33+Lx7jxxht1++2361e/+pXuvfdenXPOOfrqq69UXFy81dgcx1G/fv30xBNPqKSkRK+99pp++MMfqnfv3jr99NMlSQ8++KCmTJmi2267TSeddJIaGhr06quvZh5/0kknqampSX/60580ePBgffTRR/J4PFs77Cbmz5+vcDisefPmZW6Lx+O6+eabNWzYMNXU1GjKlCmaNGmSnn32WUnSypUrddRRR+mYY47Riy++qHA4rFdffVWJREJHHXWUBg0apD/+8Y/66U9/mnm+WbNm6fbbb+9SbF3l2iRWejqhSGIBAADs8RyzoScWAGDna4snNfy6v/fIsT+6aZxy/J1LVxQUFMjv9ysnJ0e9evWSJH3yySeSpJtuuknHH398Zt/i4mKNHDky8/3NN9+sp59+Ws8880yH6qevmzRpks466yxJ0q233qp77rlHb731lk488cStxubz+XTjjTdmvh84cKBef/11Pf7445kk1i9+8Qv95Cc/0aWXXprZb/To0ZKkF154QW+99ZY+/vhj7bXXXpKkQYMGbftF+Zrc3Fz97ne/6zCN8Lzzzsv8e9CgQbrnnns0evRoNTc3Ky8vT/fff78KCgo0Z84c+Xw+ScrEIEnnn3++HnnkkUwS669//asikUjmvHYW144C0pVYlkhiAQAA7OlMMjWmoxILANBZBx98cIfvm5ubdcUVV2ifffZRYWGh8vLy9PHHH2vZsmVbfZ79998/8+/c3FyFw+FNpt5tyf3336+DDjpIZWVlysvL029+85vM8WpqarRq1Sodd9xxm33sokWL1K9fvw7Jo+2x3377bdIHa+HChRo/frz69++v/Px8HX300ZKUiW3RokU68sgjMwmsr5s0aZK++OILvfHGG5KkmTNn6vTTT9/pTfFdW4lltffEYjohAADAns9pX53QUdemUAAAtk/I59FHN43rsWN3h68nVK644grNmzdPd9xxh4YMGaJQKKTTTjtNsVhsq8/z9USOZVlynG3nGubMmaMrrrhCd955p8aMGaP8/Hz96le/0ptvvilJCoVCW338tu63bVvma4vZxePxTfb7+uvQ0tKicePGady4cZo1a5bKysq0bNkyjRs3LvNabOvY5eXlGj9+vB555BENHDhQzz33nBYsWLDVx3QH1yaxRCUWAACAa5hMEotKLADYFSzL6vSUvp7m9/uVTG67f9err76qSZMm6dRTT5WUqsxaunTpTovr1Vdf1WGHHaaLL744c9vixYsz/87Pz1dVVZXmz5+vY489dpPH77///lqxYoU+++yzzVZjlZWVqbq6WsaYTA+xRYsWbTOuTz75RGvXrtVtt92myspKSdI777yzybH/8Ic/KB6Pb7Ea6wc/+IHOOuss9evXT4MHD9bhhx++zWPvKPdPJ6QSCwAAYI9n2j/xdizXDl8BANupqqpKb775ppYuXaq6urotVkkNHTpUTz31lBYtWqR///vfOvvssztVUbW9hg4dqnfeeUd///vf9dlnn+naa6/V22+/3WGfG264QXfeeafuueceff7553r33Xd17733SpKOPvpoHXXUUfre976nefPmacmSJXruuef0/PPPS5KOOeYY1dbW6vbbb9fixYt1//3367nnnttmXP3795ff79e9996rL7/8Us8884xuvvnmDvtMnjxZjY2NOvPMM/XOO+/o888/1x//+Ed9+umnmX3GjRuncDisX/ziFzr33HN39OXqFNeOAjLTCanEAgAA2OM5rE4IANiCK664Qh6PR8OHD89Mjduc6dOnq6ioSIcddpjGjx+vcePG6cADD9xpcf3oRz/Sd7/7XZ1xxhk69NBDtXbt2g5VWZI0ceJE3XXXXXrggQe077776uSTT9bnn3+euf/Pf/6zRo8erbPOOkvDhw/XlVdemak622efffTAAw/o/vvv18iRI/XWW2/piiuu2GZcZWVlmjlzpp544gkNHz5ct912m+64444O+5SUlOjFF19Uc3Ozjj76aB100EH67W9/26Eqy7ZtTZo0SclkUhMmTNiRl6rTLPP1CZQ7WWNjowoKCtTQ0KBwOLzTjvPvfz6ukS9doM89QzT02oU77TgAAGDn21XjB+yYnfk+rXxnrvr+7Wx9qioNu+Hf3frcAAApEoloyZIlGjhwoILBYE+Hgz3E+eefr9raWj3zzDNb3W9rP19dGT/sGRNct4NlpaYTUokFAACw56MSCwCA3UdDQ4Pef/99zZ49e5sJrO7k2umEdntPLFYnBAAA2PMZJzV5gJ5YAIDdxYUXXqi8vLzNbhdeeGFPh7dTnXLKKTrhhBN04YUX6vjjj99lx3VtJZbs1ACH1QkBAAD2fCZTiUUSCwCwe7jpppu22IPK7e0PFixY0CPHdW0Sy7JTp0YSCwAAYM9n2qvrHZJYAIDdRHl5ucrLy3s6jKzi2lGA1V6JxXRCAACAPZ9xUisxGYueWAAAZCsXJ7Fo7A4AAOAWjpOuxPL0cCQAAKCnuD6JxXRCAAAAF6ASCwCArOfanlgmkC9JKjHrZZprZeWV9XBEAAAA2F6N5aN1buynyi0s1X09HQwAAOgRrq3E6j1of71vBimguFa/wFAHAABgTxYJluufzgH6xLtPT4cCAAB6iGuTWIW5AS2q/G9JUv77j0jxth6OCAAAANvLGCNJ8jCdEACwi82cOVOFhYU9HQbk4iSWJB144kQtd8qUn2xQw+t/6OlwAAAAsJ2S7UksclgAAGQvVyex9u1XohcKvydJSr52X6YhKAAAAPYsTiqHJY9NFgsAgK6IxWI9HUK3cXUSS5L6feOHajA5Ko4sV/TDuT0dDgAAALaD057FsinFAgB8jeM4mjZtmgYOHKhQKKSRI0fqySeflOM46tevnx588MEO+7/33nuybVtfffWVJGn69Onab7/9lJubq8rKSl188cVqbm7erlgWL16sU045RRUVFcrLy9Po0aP1wgsvdNgnGo3qqquuUmVlpQKBgIYMGaLf//73mfs//PBDnXzyyQqHw8rPz9eRRx6pxYsXS5KOOeYYXXbZZR2e7zvf+Y4mTZqU+b6qqko333yzJkyYoHA4rB/+8IeSpKuuukp77bWXcnJyNGjQIF177bWKx+Mdnuuvf/2rRo8erWAwqNLSUp166qmSpJtuukkjRozY5HxHjRqla6+9drteq+3h+iTWN/YfpP/znSRJapl3q1T9vtRejg4AAIA9g2PSSaweDgQAsoUxUqylZ7Yu/s0+bdo0Pfroo5oxY4Y+/PBDXX755fqv//ov/etf/9JZZ52l2bNnd9h/1qxZOvzwwzVgwABJkm3buueee/Thhx/qD3/4g1588UVdeeWV2/WyNTc365vf/Kbmz5+v9957TyeeeKLGjx+vZcuWZfaZMGGC/vd//1f33HOPPv74Yz300EPKy8uTJK1cuVJHHXWUAoGAXnzxRS1cuFDnnXeeEolEl+K44447NHLkSL333nuZJFN+fr5mzpypjz76SHfffbd++9vf6te//nXmMXPnztWpp56qb37zm3rvvfc0f/58HXLIIZKk8847Tx9//LHefvvtzP7vvfee/vOf/+jcc8/drtdqe3h32ZF6iMe25Pl/P1L0X8+ouPFjacYRagv1VtuAb8hbOUq5/UbK02u4FMjv6VABAACwBenphDZZLADYNeKt0q19eubYP1sl+XM7tWs0GtWtt96qF154QWPGjJEkDRo0SK+88ooeeughXXnllbrzzju1bNky9e/fX47jaM6cObrmmmsyz7FxZVNVVZV+8Ytf6MILL9QDDzzQ5dBHjhypkSNHZr6/+eab9fTTT+uZZ57R5MmT9dlnn+nxxx/XvHnzNHbs2Ey8affff78KCgo0Z84c+Xw+SdJee+3V5Ti+8Y1v6Cc/+UmH2zY+56qqKl1xxRWaM2dOJmF3yy236Mwzz9SNN97Y4XwkqV+/fho3bpweeeQRjR49WpL0yCOP6Oijj+4Q/862XUms+++/X7/61a9UXV2tkSNH6t57781k53ZH4484UBf+6+c6K/lXHWm/r1DbaoU+mSV9MiuzT5OVr+ZAuRJ5vVOr30SbZcebJWOUtANyPAEZT0DGG5C8QRlfjpI5ZUrm9pbyypTjtCgUq1MoWqdYIqGmpF+NCa+iCsgXylUgmCtfMKSEIyWSjhwjhUNeFeUEFA56lbRsNcZtNUQtJSyvAoGgAoGQPP6AEnZQMTuoiEk9Z33Cq6ZIXIGGpQrXf6jCxk/ktxz5wmUKFZTLn1eUitETkOUNKjc3V4FgSJbtkRpXSQ3LpfrlUqxZSkRlEhEpEJZVtpdUupeUVyGnrUGRpjolIi0KhkvkD1dIOSVSMi5Fm1Jb8xqpYUVqi7dIwYINWyD11fHnK+pIkbijWNLI57UUDvrltW3JtiVPQPIGJF8o9ThfzpY7trbVy7SuU1tbqxqbmxWLxeT3euXz++T3+5VXUCIrVJx6rliL1FIrta5LHcefLwXyJFmp2GNNcuJRWb6ALG9I8gUlb/vm8Utt66Xmaqm5JpXg7LVfKr6NGSNFGlKvaUuNlFsmp2CAYp6Q/ErIXveFtOZDqXWtZNmpY3t8Uk5x6rXMLZMK+6fiTXMcqWm1ZByZ3DLFLb9sS/J67NRrH2+VvCHJ6+/235MO52WcVMyWleol17hSql+WOld/rpRbLuWWSnnlHS8uxijRWq9YY60CVlyeZFRyElIgnDrvYKEUqU/9/DUsTz1/uK8U7pN6TWS1v/8bfZVpfz9qUu+plHov0++pv32zN19YGoknlXSMPPFmedZ/KduyZBdXycop6tzr4TipWDb3c2nMzukwnIxLieiG772B1M9OZ5j218s4UqhIsj0bbo82pu6zvZLtSz1nIpr6uUr/bOWVpd6vr5+X40gNy6R1S1K/C+E+qZ/h9PNLqZ+VlrrUz3DbOikRk5IxyYmnfq+8wdS5FPaXCvpv8T3bYenXYOM+iF6/5MuVPN7UuUTqU/skIqn/7/jzUr/rvuCmz5eISpYn9diNJB2jeNJR0OfZ9DFfj0fq2s+Kk0z9fyzemvrqDUr5vXf8NYs0pP6flNdL8ufs2HOlxdtSv5vNtanXdOjY7nle4GuSTCcEAGzGF198odbWVh1//PEdbo/FYjrggAM0atQo7bPPPpo9e7auvvpqvfTSS6qpqdH3v//9zL4vvPCCpk2bpk8++USNjY1KJBKKRCJqbW1VTk7XxkzNzc264YYbNHfuXK1evVqJREJtbW2ZSqxFixbJ4/Ho6KOP3uzjFy1apCOPPDKTwNpeBx988Ca3PfbYY7rnnnu0ePFiNTc3K5FIKBwOdzj2BRdcsMXnvOCCC3Teeedp+vTpsm1bs2fP7lDJtSt0OYn12GOPacqUKZoxY4YOPfRQ3XXXXRo3bpw+/fRTlZeX74wYd1g46NNPf3S+/vnpeM1fs07h1a+osvE9DUgu1d7WMlVY9co3TcqPNEmRxd1yzNIu7m9LKmnfOiNhbHktp4tH2bzNDQVtSd3x540tKdS+dUbceNRg5anWFGmNirTWhNVbazXEWqlya72s9ri2FltStjza9mvT1T8Fl6uX1lglylerwqZZBWpSjiKbPGezCSusFvmtbS8k4MjSGpVqhVWhsGlSf7NaISvVdM+S1GLy1KqACtWiXGvDsRLyKGYFFLGCalNALSYgnxLKVVQ5apNP7fOaTeY/Gx5r+RS3/HJsvyw58jhxeUxCthLymoR82lCmmpQtS5K9ldczYgVVbxUoYSyVmPUKKdojJZ5RK6i45VPc8ikmvyLGq7akRxHjUW9rnSqs+g77N5hc1alAHjnyKiGfkgrYjnyWI5+Ssk0itclRUh5F7By1WTlKylbItCrotMqvuFqtHLXauWqzcyXLI49tybYsWU5clhOXx4nJNgl5nZi8SsiRpXq7UE3eErV6ixV3HCUSCZlkXMWmXuVaq1I1dIg1KVtrPL21xl+pZl+JSpJrVZKoVkGiTnH51GKF1GqCylWrip218pnU+29kKeYvlOMJyBdZJ6/pXEPHuOVTs12gqB1U1ArKMkYViRUKmGiH/ZKyFZNfTvuxgorK24nfPSn1c7PUqtRau1jyBGR7/fJYjvyxegUTDQo6rXJkKyGPEvLIaxn5LCOv5cgjR7YceWQUt7yqV1jrTJ4SslVl1ah3crUCpm2zx01aPtkmIetrvxdpDd5SVfsqVW1XqNhZp96J5SqOV8uWUbOdr3qrQI0mR22OR22OLWMsFdhtKrRblaeIHNuX+dDD60QUTDQpmGySka1Wb1gRb1gJT65su/0PcctWg8lRXSJHaxN+9dJa9TOrVZqoltd0LBmPyadaby+1+IqVZ1qU5zQqmGyWkSXH8sqxvUrKo4S8Ssgjx/LK8vhke33ymphyWlcrkNzQ26HRLlCdXSqvZZSjiEJqU8wbVn3OADXmDJCxfSpqXaKC1qXKjVTLGJPKxxmTyjMrtfk2+rlwZMm6tlZWZ5OuQBeY9oSwhyQWAOwavpxURVRPHbuT0r2r5s6dq759+3a4LxAISJLOOeecTBJr9uzZOvHEE1VSkvrre+nSpTr55JN10UUX6ZZbblFxcbFeeeUVnX/++YrFYl1OYl1xxRWaN2+e7rjjDg0ZMkShUEinnXZaprl6KLT1v463db9t25lrYtrX+1pJUm5ux0q2119/Xeecc45uvPFGjRs3LlPtdeedd3b62OPHj1cgENDTTz8tv9+veDyu0047bauP6W5d/ltz+vTpuuCCCzJzHmfMmKG5c+fq4Ycf1tVXX93tAXaX4X3CGt4nnWFMVY0lHaN1LTF9WLNG1Su+0LpVS9W2brm8Hp8CuWHl5BXK4/UoEW1TMtYmJx5RMhaRiUdkx5uVG1+rcLxW+Yn1alJIa5xCVTuFsj1elQQcFfkTCikmxVtlJSLyJKOyrPQfLlIs4SiacCRj5FVSfiupHE9SfiuRSSwEFFVQcQWtmALa8IPptRzFLL9W+IdoeXComp2AvNF1CsXWK2Ra5FdcAcXlb98CismnpNaYIq0wZVppStWgXMWNVzF5VWQ1abC1SoOtVSq2mtRoclWvXLUqqEI1q9RqUJGalZCtJuWoxQRVpwKtMqVaZUrUYoLKt1oVVmvma9hqUZ7aZMvIkpFltVfVGMmyjDxy2uNLpBIfliOflVSpGlRqNWgfLd0kw9ZiAorKp7h8SloeWcbIkiOfEipQS+YPXEmKGJ/WKixLRnmKKFdtsiQ1K6gWhRQzXvmthAJKvbZBxeSxUv8ziBuPalWgWlOoEqtR/aw6VapalaZ6k5+tdSZPa02Byqx6FVotKrUaJUmNJqRPTH9Vm+LU+csooISKrCYVqUnlVr3yrTb1Vq16m/YKIyt1bCPJbyVVZDWrSJs2FPQqKa9pVY5p7fLvQsDEJNOizuQa0q9lzHi00pRqtSlRjhVVqdWgUjUoaMUVNBH1Mh2TeS0moDal3ivH2JmfCdsycoylNSrSalMsI0u9rHUqV71820j6pV9nIynXiihPbcpVJJPMDZiIAl+LI/NXdrtaE5aRrXKrXgVWiwrU0nF/o6/n/Npfh6RynSblqmmT+3JMq3KSrVKydqvxbyzXqZZi1VInFwnxyFGf5Er1aVspfS03E5SUr/WbfZwlo0Cs431txi9bRgEr9f8Tx1hqVUBt8iuouPKtNvlMXEXJOulrb0nUeLXClCnXiqhM9fJYjkJfS+Q6xlKdCrTO5Csqn2JKJVa8SiqgmEKKqdKqUVAR7W0+T/0cJiR1zI9tagvvjSR1pcjdYzb8f7TJhBSVTzmKKsdKBVCQqFNBok7DNvPYPKdJeRv/DGycCU/HltQmr1uKo3BincKJdZvcUy5p6BbiTb8/AcXlt+Lqm1guJZZv8fw6I2J8ClpxhZ0GhZ2OCdPc+HoVtX0lrd3Gk3ztvYgar+pUoDpToL1amhQKF+9QjMDmJNsH7OSwAGAXsaxOT+nrScOHD1cgENCyZcu2WN109tln65prrtHChQv15JNPasaMGZn7Fi5cKMdxdOedd8pur3p//PHHtzueV199VZMmTco0RG9ubtbSpUsz9++3335yHEcvvfRSZjrhxvbff3/94Q9/UDwe32w1VllZmVavXp35PplM6oMPPtCxxx671bhee+01DRgwQD//+c8zt6Ub22987Pnz52+xx5XX69XEiRP1yCOPyO/368wzz9xm4qu7dSmJFYvFtHDhQk2dOjVzm23bGjt2rF5//fXNPiYajSoa3fDXSWNj43aG2v08tqWy/IDK8vtr38H9eySGpGNU2xRV0GerIORrT/RsgZNMTX2Jt0mJiPx5vTTI49XWZp8aY9QcTai+Na41bXFF4kkF4o76xJMa4veoMMevolyf/B5bsaSjWMLReiPlBb0qC3jl99hqiiS0rjWmZS0R+bxehfwehXweFSccmZaYgi0xRROOcvweBX0e5fg9X/u3V0GfLcuylHSMmiJxrW+NqzXpyO+x5ffaarWkRKRJTmu9TNs6+dtq5W+pli9Sp0RuudoK9lJb4WAFcgtVnOtXkc/T4bWKxJOqbopo7fp1aqpfp6Q/X3YgT16PR5FEUo1tcTW0xuTz2OpdGFKfwpCKc1NT8pJGajFGzcbIaZ/GlfSGZGSr0BjFjfRl61r5az+Q1VqnuK9AcX9YcX+hrHAf+YI5Cng8inhtrYs3yN+8QlFvWOt9veSJJFQcSyqeTCUs48ZovcdWi9fWGlvKTaxXbvNXymlZLoWKZUoGyxQOkM/rky9WL19brZxYqxqVq3VOjhoSfsWjrXKiLXKircqzo8r3xJVnR5WwvGp2gmpIBhQ1Xnk8Hnnt1O+oz2PJtj2yjKNoNKJopFXRthYZ2yuP1y+PL6BgMKi83ByFc0Oyba/WNbVpfXObGqJJRf3Fsm2PbEuZc4nGk8q3oyq3G1VqNSgvYCtU3Fd5JX2VkxuWN5FUPJ5UNJbUynhSi6MxxVvq1WyCanU8isSSaosn9XY8qbZoXEGnTZXFAfUrzFFF2K9EIqm2WFKt8aSS/jzJ9qs9/6tWy1KdpGg8qabmZrU21yve1qSAlVBAcQWsuIoCRsV+owK/I0+4lxJFg+X3h1O/c9FmWfXLZLXWybRPrYs6tlY2JvRVfUzLG+Iytlc+X0A+f0C5Xkf5Vpvy1Sav5Sjhy1XCmyvH9smXaJE33iRPrEWxeFxt8YTaYknZXp8CwZBCwaCCwRzl5OQoNydHHpNU2/pViqxfKadlrUI+r3ICPuUEA1JemRK5vZXI6aWoJ0dtcUexeFKJlrXyrF8sf/1iedvqVO8r0zpPhdZ5SxX2SyW+mIo8UbVZOao2hVqeKFRdS1yRhjo5zbXyOFGV9eqnAf0HaHCfUrXFHK1viaqhpU0JY8uy7fYZk5a8yYhCsbUKxBvlc9rkS7TKklFz3gC15vSTPD4FvLaCHinfWa+Q4gp4LQW8liImoNWJPK1pSao5klDI71FuwCO/x6OWaEINbXE1ReLK90uDPDXqF18qb7Re0UibIpGIkpL8+WXKKShTKL9IthxZTlwmmVAsKbUmjFoTUtyxlDCWEsaWz8RUoCblO42ynZhW2b20JFmhL+IlCgVDKgj5lB/wqLW1Vesb6tXQ2CDL41NRaS/1LQkrP+jT+taY1ja1KdG8Xr2d1eqTWK6ieLVafCWqDfTXal+lAj6vyj1NKlGjCj0R5Xkd5XodeW2pWXmqd0Jan/CpsbVNLS0tirS2yPEGZeUUyRsqlGRkWtdLbevkRJtTv0MJR8ZJqm8gqj6BiEq8ETX7SrXS00dLnQolc0pVUhBWRUFIAdtRfN1yad0SJZtrVW9yVefkal0iJMnIY5LyKqmQx1Ge3yjXa2QScTW3RtQSiag1YSmW10fJvL4K5YZV6ouol6lVcbJWzQlLtVG/aiMeeaNrVRJZptLoMnmcuFb7+2u1t7/WB/qorCBXvcJBleT51RRJqq45qtrmmJL+sEJ5hSrM9asg5NPeOQWbXoyAbpAf9GnvXvkaUNJNU2EBAK6Qn5+vK664Qpdffrkcx9ERRxyhhoYGvfrqqwqHw5o4caKqqqp02GGH6fzzz1cymdS3v/3tzOOHDBmieDyue++9V+PHj9err77aIcnVVUOHDtVTTz2l8ePHy7IsXXvttXKcDRUEVVVVmjhxos477zzdc889GjlypL766ivV1NTo9NNP1+TJk3XvvffqzDPP1NSpU1VQUKA33nhDhxxyiIYNG6ZvfOMbmjJliubOnavBgwdr+vTpqq+v71Rcy5Yt05w5czR69GjNnTtXTz/9dId9rr/+eh133HEaPHiwzjzzTCUSCT377LO66qqrMvv84Ac/0D777CMplbDb5UwXrFy50kgyr732Wofbf/rTn5pDDjlks4+5/vrr05+fd9gaGhq6cmgAAJDFGhoaGD9sh/vuu88MGDDABAIBc8ghh5g333xzq/s//vjjZtiwYSYQCJgRI0aYuXPndul4vE8AsOdqa2szH330kWlra+vpULrMcRxz1113mWHDhhmfz2fKysrMuHHjzEsvvZTZ54EHHjCSzIQJEzZ5/PTp003v3r1NKBQy48aNM48++qiRZNavX2+MMeaRRx4xBQUFnYplyZIl5thjjzWhUMhUVlaa++67zxx99NHm0ksvzezT1tZmLr/8ctO7d2/j9/vNkCFDzMMPP5y5/9///rc54YQTTE5OjsnPzzdHHnmkWbx4sTHGmFgsZi666CJTXFxsysvLzbRp08wpp5xiJk6cmHn8gAEDzK9//etNYvvpT39qSkpKTF5enjnjjDPMr3/9603O689//rMZNWqU8fv9prS01Hz3u9/d5HmOPPJIs++++3bq9dj4nLf089WV8YNlTOfXrly1apX69u2r1157LdP1X5KuvPJKvfTSS3rzzTc3eczmKrEqKyvV0NDQoYEYAADAljQ2NqqgoIDxQxc89thjmjBhQoc+pk888cQW+5i+9tprOuqoozRt2jSdfPLJmj17tn75y1/q3Xff1YgRIzp1TN4nANhzRSIRLVmyRAMHDlQwuJnFbgClZnsNHTpUF198saZMmdLpx23t56sr44cu9bYuLS2Vx+PRmjVrOty+Zs0a9erVa7OPCQQCCofDHTYAAADsXBv3MR0+fLhmzJihnJwcPfzww5vd/+6779aJJ56on/70p9pnn310880368ADD9R99923iyMHAAC7o9raWt13332qrq7eYt+sna1LSSy/36+DDjpI8+fPz9zmOI7mz5/foTILAAAAPSfdx3TjhrHb6mP6+uuvb9Jgdty4cVvcX0pV3Dc2NnbYAABws3333Vd5eXmb3WbNmtXT4e1U5eXluummm/Sb3/xGRUVFPRJDl1cnnDJliiZOnKiDDz5YhxxyiO666y61tLT0WBYOAAAAHdXV1SmZTKqioqLD7RUVFfrkk082+5jq6urN7l9dvenKvGnTpk3TjTfeuOMBAwCwh3j22WcVj8c3e9/Xr6Nu04VuVDtNl5NYZ5xxhmpra3Xdddepurpao0aN0vPPP+/6NwsAAAAdTZ06tUM/jHTvUwAA3GrAgAE9HUJW63ISS5ImT56syZMnd3csAAAA6Abb08e0V69eXdpfSvU+DQQCOx4wAABAJ3SpJxYAAAB2f9vTx3TMmDEd9pekefPm0fcUALKM4zg9HQJcqLt+rrarEgsAAAC7t231MZ0wYYL69u2radOmSZIuvfRSHX300brzzjv1rW99S3PmzNE777yj3/zmNz15GgCAXcTv98u2ba1atUplZWXy+/2yLKunw8IezhijWCym2tpa2bYtv9+/Q89HEgsAAMCFttXHdNmyZbLtDUX5hx12mGbPnq1rrrlGP/vZzzR06FD95S9/0YgRI3rqFAAAu5Bt2xo4cKBWr16tVatW9XQ4cJmcnBz179+/w9hje1hmF7eXb2xsVEFBgRoaGhQOh3floQEAwB6K8cOegfcJAPZ8xhglEgklk8meDgUu4fF45PV6t1jZ15XxA5VYAAAAAABAkmRZlnw+n3w+X0+HAmyCxu4AAAAAAADY7ZHEAgAAAAAAwG6PJBYAAAAAAAB2e7u8J1a6j3xjY+OuPjQAANhDpccNu3g9GnQR4zwAANBVXRnn7fIkVlNTkySpsrJyVx8aAADs4ZqamlRQUNDTYWALGOcBAIDt1ZlxnmV28UeajuNo1apVys/P3+LyijuisbFRlZWVWr58eVYu7Zzt5y/xGmT7+Uu8Bpx/dp+/5M7XwBijpqYm9enTR7ZNN4TdFeO8nSvbz1/iNcj285d4DbL9/CVeAzeef1fGebu8Esu2bfXr12+nHyccDrvmDd0e2X7+Eq9Btp+/xGvA+Wf3+Uvuew2owNr9Mc7bNbL9/CVeg2w/f4nXINvPX+I1cNv5d3acx0eZAAAAAAAA2O2RxAIAAAAAAMBuz3VJrEAgoOuvv16BQKCnQ+kR2X7+Eq9Btp+/xGvA+Wf3+Uu8BnCvbP/Zzvbzl3gNsv38JV6DbD9/idcg289/lzd2BwAAAAAAALrKdZVYAAAAAAAAcB+SWAAAAAAAANjtkcQCAAAAAADAbo8kFgAAAAAAAHZ7rkpi3X///aqqqlIwGNShhx6qt956q6dD2immTZum0f+/vbuLafJs4wD+LxQQdViF0FpJHS4mbMoIyjSMJTuQ6BYznSxbJJWRzcS41QhqELOF7WBxisu+0AW3HbiD+TUT2QaJWSogjgQQW1ARhyQSVLASPxD8Goxe78FeHikw5X0HfeDu/5eQ2Oe+01zXlbb+c4t9XngBTz31FKKjo/H666+jqanJZ8/Dhw/hcDgQGRmJqVOn4o033sD169d1qnjs7dy5EwaDAdnZ2do11WfQ1taGNWvWIDIyEuHh4YiPj8fp06e1dRHBRx99hJkzZyI8PBypqalobm7WseLR1dfXh7y8PMTGxiI8PBzPPPMMPvnkEwy8V4VKMzh58iRee+01WK1WGAwG/Pzzzz7rI+n11q1bsNvtiIiIgMlkwtq1a3H37l0/dvHvPG4Gvb29yM3NRXx8PKZMmQKr1Yq3334b7e3tPs8xkWfwpNfAQOvXr4fBYMBXX33lc30i90/EnPeI6hlnMOY85jzmPOY85rxHAj3nKXOIdfjwYWzevBkff/wx3G43EhISsGzZMnR0dOhd2qirqKiAw+FAdXU1nE4nent7sXTpUty7d0/bs2nTJhQXF+PIkSOoqKhAe3s70tLSdKx67NTW1uLbb7/F888/73Nd5Rncvn0bKSkpCAkJwbFjx9DY2IjPP/8c06dP1/bs2rULBQUF2Lt3L2pqajBlyhQsW7YMDx8+1LHy0ZOfn4/CwkLs2bMHFy5cQH5+Pnbt2oXdu3dre1Sawb1795CQkIBvvvlm2PWR9Gq323H+/Hk4nU6UlJTg5MmTWLdunb9a+NceN4P79+/D7XYjLy8PbrcbR48eRVNTE1asWOGzbyLP4EmvgX5FRUWorq6G1WodsjaR+6fAxpzHnMecx5zHnMecx5zHnAcAEEUsWrRIHA6H9rivr0+sVqvs2LFDx6r8o6OjQwBIRUWFiIh0dnZKSEiIHDlyRNtz4cIFASBVVVV6lTkmuru7Ze7cueJ0OuXll1+WrKwsEVF/Brm5ufLSSy/947rX6xWLxSKfffaZdq2zs1PCwsLk4MGD/ihxzC1fvlzeffddn2tpaWlit9tFRO0ZAJCioiLt8Uh6bWxsFABSW1ur7Tl27JgYDAZpa2vzW+2jZfAMhnPq1CkBIK2trSKi1gz+qf+rV6/KrFmzpKGhQWbPni1ffvmltqZS/xR4mPOY85jzHlE54/RjzivSHjPnDY85L3BznhK/idXT0wOXy4XU1FTtWlBQEFJTU1FVVaVjZf5x584dAMCMGTMAAC6XC729vT7ziIuLg81mU24eDocDy5cv9+kVUH8Gv/76K5KSkvDmm28iOjoaiYmJ+P7777X1lpYWeDwen/6nTZuGxYsXK9E/ALz44osoLS3FxYsXAQBnzpxBZWUlXn31VQCBMYN+I+m1qqoKJpMJSUlJ2p7U1FQEBQWhpqbG7zX7w507d2AwGGAymQCoPwOv14uMjAzk5ORg3rx5Q9ZV75/UxZzHnMecx5zHnMecNxhzni/V+x/IqHcBo+HGjRvo6+uD2Wz2uW42m/HHH3/oVJV/eL1eZGdnIyUlBfPnzwcAeDwehIaGam/ofmazGR6PR4cqx8ahQ4fgdrtRW1s7ZE31GVy6dAmFhYXYvHkzPvjgA9TW1mLjxo0IDQ1FZmam1uNw7wkV+geAbdu2oaurC3FxcQgODkZfXx+2b98Ou90OAAExg34j6dXj8SA6Otpn3Wg0YsaMGcrNA/j7u1Jyc3ORnp6OiIgIAOrPID8/H0ajERs3bhx2XfX+SV3Mecx5g6k+A+Y85ryBmPOGYs4bSvX+B1LiECuQORwONDQ0oLKyUu9S/OrKlSvIysqC0+nEpEmT9C7H77xeL5KSkvDpp58CABITE9HQ0IC9e/ciMzNT5+r846effsL+/ftx4MABzJs3D/X19cjOzobVag2YGdDwent78dZbb0FEUFhYqHc5fuFyufD111/D7XbDYDDoXQ4RjRLmPOY8gDmPOY8GYs5jzlPivxNGRUUhODh4yB1Jrl+/DovFolNVY2/Dhg0oKSlBeXk5YmJitOsWiwU9PT3o7Oz02a/SPFwuFzo6OrBgwQIYjUYYjUZUVFSgoKAARqMRZrNZ6RnMnDkTzz33nM+1Z599FpcvXwYArUeV3xM5OTnYtm0bVq9ejfj4eGRkZGDTpk3YsWMHgMCYQb+R9GqxWIZ8AfJff/2FW7duKTWP/mDT2toKp9Op/escoPYMfv/9d3R0dMBms2mfia2trdiyZQuefvppAGr3T2pjzmPOY85jzmPOY84DmPOY8/6mxCFWaGgoFi5ciNLSUu2a1+tFaWkpkpOTdaxsbIgINmzYgKKiIpSVlSE2NtZnfeHChQgJCfGZR1NTEy5fvqzMPJYsWYJz586hvr5e+0lKSoLdbtf+rPIMUlJShtxu++LFi5g9ezYAIDY2FhaLxaf/rq4u1NTUKNE/8PddSoKCfD/CgoOD4fV6AQTGDPqNpNfk5GR0dnbC5XJpe8rKyuD1erF48WK/1zwW+oNNc3Mzjh8/jsjISJ91lWeQkZGBs2fP+nwmWq1W5OTk4LfffgOgdv+kNuY85jzmPOY8gDmPOY85jznvv/T9XvnRc+jQIQkLC5MffvhBGhsbZd26dWIymcTj8ehd2qh77733ZNq0aXLixAm5du2a9nP//n1tz/r168Vms0lZWZmcPn1akpOTJTk5Wceqx97Au9aIqD2DU6dOidFolO3bt0tzc7Ps379fJk+eLD/++KO2Z+fOnWIymeSXX36Rs2fPysqVKyU2NlYePHigY+WjJzMzU2bNmiUlJSXS0tIiR48elaioKNm6dau2R6UZdHd3S11dndTV1QkA+eKLL6Surk67I8tIen3llVckMTFRampqpLKyUubOnSvp6el6tfQ/e9wMenp6ZMWKFRITEyP19fU+n41//vmn9hwTeQZPeg0MNviuNSITu38KbMx5zHnMecx5zHnMecx5jwRyzlPmEEtEZPfu3WKz2SQ0NFQWLVok1dXVepc0JgAM+7Nv3z5tz4MHD+T999+X6dOny+TJk2XVqlVy7do1/Yr2g8HhRvUZFBcXy/z58yUsLEzi4uLku+++81n3er2Sl5cnZrNZwsLCZMmSJdLU1KRTtaOvq6tLsrKyxGazyaRJk2TOnDny4Ycf+vxFptIMysvLh33fZ2ZmisjIer1586akp6fL1KlTJSIiQt555x3p7u7WoZv/z+Nm0NLS8o+fjeXl5dpzTOQZPOk1MNhw4WYi90/EnLdP26N6xhkOcx5zHnMecx5z3iOBnPMMIiKj8ztdREREREREREREY0OJ78QiIiIiIiIiIiK18RCLiIiIiIiIiIjGPR5iERERERERERHRuMdDLCIiIiIiIiIiGvd4iEVEREREREREROMeD7GIiIiIiIiIiGjc4yEWERERERERERGNezzEIiIiIiIiIiKicY+HWERERERERERENO7xEIuIiIiIiIiIiMY9HmIREREREREREdG4x0MsIiIiIiIiIiIa9/4D4Ken9KYy1BcAAAAASUVORK5CYII="},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 0 Axes>"},"metadata":{}}]},{"cell_type":"code","source":"@partial(jax.jit, static_argnames=(\"length\"))\ndef generate_text(rng, params, var_params, length):\n    def _scan_generate(carry, _):\n        random_key, context = carry\n        logits = model.apply({'params': params, **var_params}, context, training=False, mutable=['other_variables'])[0]\n        rng, rng_subkey = jax.random.split(random_key)\n        new_token = jax.random.categorical(\n          rng_subkey, logits[:, -n_tokens, :], axis=-1, shape=(1, 1)\n        )\n        context = jnp.concatenate([context[:, 1:], new_token], axis=1)\n        print(context.shape)\n        return (rng, context), new_token\n\n    _, new_tokens = jax.lax.scan(\n    _scan_generate,\n    (rng, jnp.expand_dims(test_data[850:850+block_size], axis=0)),\n    (),\n    length=length,\n    )\n    return new_tokens","metadata":{"execution":{"iopub.status.busy":"2024-06-10T06:55:57.740277Z","iopub.execute_input":"2024-06-10T06:55:57.740966Z","iopub.status.idle":"2024-06-10T06:55:57.750825Z","shell.execute_reply.started":"2024-06-10T06:55:57.740925Z","shell.execute_reply":"2024-06-10T06:55:57.749586Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"test_data[850:850+block_size]","metadata":{"execution":{"iopub.status.busy":"2024-06-10T06:55:57.752175Z","iopub.execute_input":"2024-06-10T06:55:57.752560Z","iopub.status.idle":"2024-06-10T06:55:57.881912Z","shell.execute_reply.started":"2024-06-10T06:55:57.752516Z","shell.execute_reply":"2024-06-10T06:55:57.880886Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"Array([58, 41,  1, 12, 57, 45, 42, 51, 56,  1, 52, 51,  1, 38,  1, 45, 42,\n       38, 53,  7,  0,  0, 31, 20, 24, 26, 25,  9,  0, 34, 38, 55,  5, 56,\n       57,  1, 57, 45, 52, 58,  1,  5, 44, 38, 46, 51, 56, 57,  1, 12, 57,\n       45, 42, 51, 56, 11,  0,  0, 12, 23, 14, 20, 13, 20], dtype=int32)"},"metadata":{}}]},{"cell_type":"code","source":"new_tokenz = 1000\nkey, subkey = jax.random.split(jax.random.PRNGKey(156))\n# key, subkey = jax.random.split(key)\n# token_gen = generate_text(jnp.zeros((1,block_size)).astype(jnp.int32), new_tokenz, {'params': state.params})\ntoken_gen = generate_text(key, params, var_params, new_tokenz)[:, 0, 0].tolist()\nprint(token_gen)\nprint(decode(token_gen))","metadata":{"execution":{"iopub.status.busy":"2024-06-10T06:55:57.883065Z","iopub.execute_input":"2024-06-10T06:55:57.883380Z","iopub.status.idle":"2024-06-10T06:56:14.555786Z","shell.execute_reply.started":"2024-06-10T06:55:57.883352Z","shell.execute_reply":"2024-06-10T06:56:14.554659Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"(1, 64)\n[12, 15, 9, 0, 31, 45, 46, 51, 42, 49, 62, 6, 1, 57, 45, 38, 57, 1, 46, 57, 1, 60, 42, 51, 52, 57, 5, 56, 57, 1, 39, 52, 62, 2, 0, 0, 22, 12, 31, 16, 25, 30, 20, 26, 9, 0, 20, 1, 39, 42, 44, 42, 41, 1, 20, 6, 0, 0, 17, 20, 29, 30, 31, 1, 27, 16, 9, 0, 20, 1, 57, 45, 42, 1, 51, 58, 40, 45, 1, 57, 45, 42, 55, 42, 6, 1, 16, 55, 42, 1, 20, 1, 45, 46, 41, 42, 8, 0, 0, 34, 20, 23, 23, 20, 12, 9, 0, 31, 45, 58, 56, 1, 20, 1, 53, 55, 52, 59, 42, 1, 57, 45, 42, 1, 45, 52, 51, 52, 58, 55, 8, 1, 0, 12, 25, 31, 26, 25, 14, 16, 23, 23, 26, 9, 0, 13, 58, 57, 6, 1, 56, 52, 6, 1, 56, 53, 42, 38, 48, 1, 41, 42, 38, 57, 45, 0, 31, 45, 38, 57, 1, 51, 42, 59, 42, 55, 1, 39, 42, 1, 45, 52, 60, 1, 52, 58, 55, 1, 56, 38, 46, 41, 9, 0, 26, 49, 41, 1, 51, 52, 51, 42, 1, 52, 43, 1, 52, 58, 55, 0, 0, 24, 26, 34, 13, 16, 23, 20, 25, 16, 29, 9, 0, 20, 1, 40, 38, 51, 56, 46, 42, 56, 6, 0, 12, 51, 41, 1, 45, 42, 1, 53, 49, 42, 38, 56, 42, 10, 1, 50, 62, 1, 52, 49, 41, 1, 57, 45, 62, 1, 62, 52, 58, 1, 52, 43, 1, 53, 38, 55, 46, 57, 62, 1, 45, 46, 56, 1, 57, 45, 42, 42, 1, 38, 51, 41, 1, 40, 52, 58, 55, 57, 0, 12, 43, 57, 42, 55, 56, 6, 1, 57, 45, 52, 58, 1, 39, 42, 44, 46, 51, 42, 56, 56, 1, 57, 45, 38, 51, 1, 57, 45, 42, 62, 1, 40, 38, 51, 51, 52, 57, 1, 57, 52, 1, 56, 38, 62, 6, 0, 0, 15, 32, 22, 16, 1, 33, 20, 25, 14, 16, 23, 23, 26, 9, 0, 31, 45, 46, 56, 1, 38, 1, 39, 52, 49, 41, 8, 0, 13, 58, 57, 1, 55, 42, 50, 38, 46, 51, 6, 1, 38, 42, 1, 42, 38, 55, 56, 8, 1, 31, 45, 42, 62, 1, 60, 46, 49, 49, 0, 0, 18, 12, 32, 16, 30, 9, 0, 20, 1, 45, 42, 38, 55, 56, 42, 6, 1, 51, 42, 43, 57, 1, 53, 38, 56, 56, 42, 41, 56, 1, 43, 55, 52, 50, 1, 24, 58, 55, 41, 42, 55, 1, 50, 38, 51, 62, 1, 50, 38, 57, 51, 42, 56, 56, 1, 57, 60, 42, 51, 57, 62, 11, 0, 18, 52, 1, 38, 51, 41, 1, 56, 42, 51, 41, 1, 57, 45, 38, 57, 11, 0, 0, 27, 32, 14, 19, 42, 59, 42, 56, 1, 45, 42, 1, 45, 46, 56, 1, 44, 55, 52, 58, 49, 1, 55, 42, 56, 57, 1, 55, 38, 57, 46, 42, 51, 57, 9, 0, 20, 1, 38, 50, 1, 38, 55, 49, 46, 42, 56, 57, 1, 57, 45, 52, 58, 10, 0, 12, 44, 42, 1, 20, 1, 62, 52, 58, 1, 50, 38, 41, 52, 60, 5, 56, 1, 20, 1, 39, 42, 57, 45, 55, 52, 58, 44, 45, 1, 38, 55, 57, 1, 62, 52, 58, 6, 1, 38, 1, 48, 46, 51, 41, 56, 1, 43, 5, 42, 49, 49, 8, 0, 0, 13, 32, 14, 22, 20, 25, 18, 19, 12, 24, 9, 0, 31, 45, 52, 58, 1, 51, 52, 39, 49, 42, 1, 51, 52, 60, 6, 1, 43, 49, 62, 1, 62, 52, 58, 55, 56, 42, 41, 1, 45, 46, 50, 1, 52, 43, 1, 5, 42, 50, 50, 52, 55, 41, 43, 58, 49, 1, 52, 51, 42, 0, 34, 46, 57, 45, 1, 45, 46, 56, 1, 40, 45, 38, 55, 44, 42, 6, 1, 50, 62, 1, 45, 52, 58, 55, 1, 40, 55, 52, 60, 51, 42, 56, 56, 6, 1, 51, 52, 11, 0, 0, 23, 32, 14, 20, 26, 9, 0, 36, 52, 58, 1, 49, 46, 44, 45, 57, 1, 38, 51, 41, 1, 38, 49, 49, 1, 46, 56, 1, 52, 58, 55, 1, 40, 52, 58, 55, 57, 46, 51, 42, 5, 56, 1, 59, 38, 40, 58, 56, 1, 38, 55, 55, 42, 56, 56, 1, 62, 52, 58, 55, 56, 42, 49, 43, 1, 38, 51, 59, 42, 51, 57, 6, 0, 0, 12, 29, 20, 16, 23, 9, 0, 36, 42, 56, 6, 1, 57, 46, 50, 42, 7, 57, 42, 51, 51, 42, 55, 1, 52, 43, 1, 31, 52, 51, 42, 55, 1, 57, 52, 1, 53, 42, 62, 51, 56, 1, 52, 43, 1, 45, 46, 56, 56, 42, 56, 56, 8, 0, 0, 31, 26, 32, 1, 56, 46, 51, 44, 1, 60, 52, 58, 49, 41, 1, 12, 51, 57, 42, 55, 1, 52, 43, 1, 38, 51, 41, 1, 49, 38, 40, 48, 1, 62, 52, 58, 1, 52, 43, 1, 50, 46, 51, 42, 6, 0, 0, 13, 32, 14, 22, 20, 25, 18, 19, 12, 24, 9, 0, 36, 52, 58, 1, 52, 43, 1, 20, 56, 38, 39, 42, 49, 49, 2, 0, 0, 26, 31, 19, 16, 23, 23, 26, 9, 0, 24, 62, 1, 60, 52, 55, 56, 42, 1, 57, 52, 0, 62, 52, 58, 1, 57, 45, 42, 55, 42, 43, 52, 55, 42, 1, 46, 56, 1, 38, 1, 60, 52, 55, 57, 45, 55, 42, 56, 57, 1, 52, 43, 1, 58, 56, 1, 49, 52, 59, 46, 51, 44, 1, 56, 42, 55, 59, 42, 0, 0, 14, 12, 27, 32, 29, 26, 23, 13, 9, 0, 1, 14, 38, 55, 41, 46, 49, 49, 0, 31, 45, 38, 57, 1, 20, 1, 53, 55, 38, 62]\nAD:\nThinely, that it wenot'st boy!\n\nKATENSIO:\nI beged I,\n\nFIRST PE:\nI the nuch there, Ere I hide.\n\nWILLIA:\nThus I prove the honour. \nANTONCELLO:\nBut, so, speak death\nThat never be how our said:\nOld none of our\n\nMOWBELINER:\nI cansies,\nAnd he please; my old thy you of parity his thee and court\nAfters, thou beginess than they cannot to say,\n\nDUKE VINCELLO:\nThis a bold.\nBut remain, ae ears. They will\n\nGAUES:\nI hearse, neft passeds from Murder many matness twenty?\nGo and send that?\n\nPUCHeves he his groul rest ratient:\nI am arliest thou;\nAge I you madow's I bethrough art you, a kinds f'ell.\n\nBUCKINGHAM:\nThou noble now, fly yoursed him of 'emmordful one\nWith his charge, my hour crowness, no?\n\nLUCIO:\nYou light and all is our courtine's vacus arress yourself anvent,\n\nARIEL:\nYes, time-tenner of Toner to peyns of hissess.\n\nTOU sing would Anter of and lack you of mine,\n\nBUCKINGHAM:\nYou of Isabell!\n\nOTHELLO:\nMy worse to\nyou therefore is a worthrest of us loving serve\n\nCAPUROLB:\n Cardill\nThat I pray\n","output_type":"stream"}]},{"cell_type":"code","source":"dsfsdhfgjdg hfdgjdgjgfjhs'####################","metadata":{"execution":{"iopub.status.busy":"2024-06-10T06:56:14.556883Z","iopub.execute_input":"2024-06-10T06:56:14.557174Z","iopub.status.idle":"2024-06-10T06:56:14.563356Z","shell.execute_reply.started":"2024-06-10T06:56:14.557148Z","shell.execute_reply":"2024-06-10T06:56:14.562070Z"},"trusted":true},"execution_count":42,"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[42], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    dsfsdhfgjdg hfdgjdgjgfjhs'####################\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 1)\n"],"ename":"SyntaxError","evalue":"unterminated string literal (detected at line 1) (2630675753.py, line 1)","output_type":"error"}]},{"cell_type":"code","source":"len(token_gen)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T06:56:14.564208Z","iopub.status.idle":"2024-06-10T06:56:14.564542Z","shell.execute_reply.started":"2024-06-10T06:56:14.564377Z","shell.execute_reply":"2024-06-10T06:56:14.564391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx = 882\ntokenizer.decode(test_data[idx:idx+32])","metadata":{"execution":{"iopub.status.busy":"2024-06-10T06:56:14.566680Z","iopub.status.idle":"2024-06-10T06:56:14.567118Z","shell.execute_reply.started":"2024-06-10T06:56:14.566917Z","shell.execute_reply":"2024-06-10T06:56:14.566938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer('bestopleled', return_tensors='np')","metadata":{"execution":{"iopub.status.busy":"2024-06-10T06:56:14.568906Z","iopub.status.idle":"2024-06-10T06:56:14.569386Z","shell.execute_reply.started":"2024-06-10T06:56:14.569185Z","shell.execute_reply":"2024-06-10T06:56:14.569217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.decode([1991])","metadata":{"execution":{"iopub.status.busy":"2024-06-10T06:56:14.570614Z","iopub.status.idle":"2024-06-10T06:56:14.570920Z","shell.execute_reply.started":"2024-06-10T06:56:14.570768Z","shell.execute_reply":"2024-06-10T06:56:14.570781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params['Dense_12']['kernel'].shape","metadata":{"execution":{"iopub.status.busy":"2024-06-10T06:56:14.572762Z","iopub.status.idle":"2024-06-10T06:56:14.573102Z","shell.execute_reply.started":"2024-06-10T06:56:14.572924Z","shell.execute_reply":"2024-06-10T06:56:14.572938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rngk = jax.random.PRNGKey(389)\nxs, ys = get_batch(rngk, train_data)\nprint(xs[0])\nprint(ys[0])","metadata":{"execution":{"iopub.status.busy":"2024-06-10T06:56:14.574173Z","iopub.status.idle":"2024-06-10T06:56:14.574529Z","shell.execute_reply.started":"2024-06-10T06:56:14.574373Z","shell.execute_reply":"2024-06-10T06:56:14.574387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logits = model.apply({'params': params, **var_params}, xs[0].reshape((1,64)), training=False, mutable=['other_variables'])[0]\nrng, rng_subkey = jax.random.split(rngk)\nfor pso in range(n_tokens):\n    new_token = jax.random.categorical(\n      rng_subkey, logits[:, -1*(n_tokens-pso), :], axis=-1, shape=(1, 1)\n    )\n    print(new_token)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T06:56:14.576085Z","iopub.status.idle":"2024-06-10T06:56:14.576436Z","shell.execute_reply.started":"2024-06-10T06:56:14.576269Z","shell.execute_reply":"2024-06-10T06:56:14.576283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_tok = [51,49,46,46,46,52]\nprint(decode(ys[0].tolist()))","metadata":{"execution":{"iopub.status.busy":"2024-06-10T06:56:14.577522Z","iopub.status.idle":"2024-06-10T06:56:14.577869Z","shell.execute_reply.started":"2024-06-10T06:56:14.577693Z","shell.execute_reply":"2024-06-10T06:56:14.577708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"act_tk = [60, 43, 50, 57,  1, 47]\nprint(decode(act_tk))","metadata":{"execution":{"iopub.status.busy":"2024-06-10T06:56:14.579129Z","iopub.status.idle":"2024-06-10T06:56:14.579470Z","shell.execute_reply.started":"2024-06-10T06:56:14.579311Z","shell.execute_reply":"2024-06-10T06:56:14.579325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jax.nn.standardize(jnp.array([2.0,3.0,4.0]))","metadata":{"id":"Oe_GIDP2HFyt","outputId":"5d3dce16-fcc2-40b9-c49a-00a8c4013ca2","execution":{"iopub.status.busy":"2024-06-10T06:56:14.580895Z","iopub.status.idle":"2024-06-10T06:56:14.581260Z","shell.execute_reply.started":"2024-06-10T06:56:14.581062Z","shell.execute_reply":"2024-06-10T06:56:14.581077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@struct.dataclass\nclass Metrics(metrics.Collection):\n    accuracy: metrics.Accuracy\n    loss: metrics.Average.from_output('loss')","metadata":{"id":"s3nN1jOiHFyu","execution":{"iopub.status.busy":"2024-06-10T06:56:14.582471Z","iopub.status.idle":"2024-06-10T06:56:14.582799Z","shell.execute_reply.started":"2024-06-10T06:56:14.582625Z","shell.execute_reply":"2024-06-10T06:56:14.582638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainState(train_state.TrainState):\n    metrics: Metrics\n\ndef create_train_state(module, rng, learning_rate, train_shape):\n    \"\"\"Creates an initial `TrainState`.\"\"\"\n    params = module.init(rng, jnp.ones(train_shape).astype(jnp.int32), \n                         training=False)['params'] # initialize parameters by passing a template image\n    tx = optax.adamw(learning_rate)\n    return TrainState.create(\n      apply_fn=module.apply, params=params, tx=tx,\n      metrics=Metrics.empty(),\n    )","metadata":{"id":"7LLDTSFQHFyu","execution":{"iopub.status.busy":"2024-06-10T06:56:14.584292Z","iopub.status.idle":"2024-06-10T06:56:14.584737Z","shell.execute_reply.started":"2024-06-10T06:56:14.584502Z","shell.execute_reply":"2024-06-10T06:56:14.584521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TrainState.create(","metadata":{"execution":{"iopub.status.busy":"2024-06-10T06:56:14.586530Z","iopub.status.idle":"2024-06-10T06:56:14.586979Z","shell.execute_reply.started":"2024-06-10T06:56:14.586753Z","shell.execute_reply":"2024-06-10T06:56:14.586773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@jax.jit\ndef train_step(state, inputs, targets):\n    \"\"\"Train for a single step.\"\"\"\n    def loss_fn(params):\n        logits = state.apply_fn({'params': params}, inputs, training=True, \n                                rngs={\"dropout\": key})[0]\n        loss = optax.softmax_cross_entropy_with_integer_labels(\n            logits=logits, labels=targets).mean()\n        return loss\n    grad_fn = jax.grad(loss_fn)\n    grads = grad_fn(state.params)\n    state = state.apply_gradients(grads=grads)\n    return state","metadata":{"id":"zApWXUDaHFyu","execution":{"iopub.status.busy":"2024-06-10T06:56:14.588271Z","iopub.status.idle":"2024-06-10T06:56:14.588714Z","shell.execute_reply.started":"2024-06-10T06:56:14.588480Z","shell.execute_reply":"2024-06-10T06:56:14.588499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@jax.jit\ndef compute_metrics(*, state, inputs, targets):\n    logits = state.apply_fn({'params': state.params}, inputs, training=False)[0]\n    loss = optax.softmax_cross_entropy_with_integer_labels(\n        logits=logits, labels=targets).mean()\n    metric_updates = state.metrics.single_from_model_output(\n    logits=logits, labels=targets, loss=loss)\n    metrics = state.metrics.merge(metric_updates)\n    state = state.replace(metrics=metrics)\n    return state","metadata":{"id":"VzukZ4iEHFyv","execution":{"iopub.status.busy":"2024-06-10T06:56:14.590211Z","iopub.status.idle":"2024-06-10T06:56:14.590663Z","shell.execute_reply.started":"2024-06-10T06:56:14.590423Z","shell.execute_reply":"2024-06-10T06:56:14.590442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 10\nlearning_rate = 0.005\ninit_rng = jax.random.key(0)","metadata":{"id":"ehYvMeuNHFyv","execution":{"iopub.status.busy":"2024-06-10T06:56:14.592788Z","iopub.status.idle":"2024-06-10T06:56:14.593312Z","shell.execute_reply.started":"2024-06-10T06:56:14.593032Z","shell.execute_reply":"2024-06-10T06:56:14.593054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"state = create_train_state(fin_model, init_rng, learning_rate, train_shape)\ndel init_rng  # Must not be used anymore.","metadata":{"id":"D60UHLFHHFyv","execution":{"iopub.status.busy":"2024-06-10T06:56:14.596333Z","iopub.status.idle":"2024-06-10T06:56:14.597001Z","shell.execute_reply.started":"2024-06-10T06:56:14.596756Z","shell.execute_reply":"2024-06-10T06:56:14.596776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics_history = {'train_loss': [],\n                   'train_accuracy': [],\n                   'test_loss': [],\n                   'test_accuracy': []}","metadata":{"id":"Jl-9TlHEHFyv","execution":{"iopub.status.busy":"2024-06-10T06:56:14.598455Z","iopub.status.idle":"2024-06-10T06:56:14.598906Z","shell.execute_reply.started":"2024-06-10T06:56:14.598676Z","shell.execute_reply":"2024-06-10T06:56:14.598696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 442\nkey = jax.random.PRNGKey(SEED)\nloss = 10\ncounter = 0\n# for step in tqdm(range(max_iters)): # increase number of steps for good results...\nwhile counter==max_iters or loss > 1.0:\n\n      # sample a batch of data\n    xb, yb = get_batch(key, train_data)\n    state = train_step(state, xb, yb)\n    state = compute_metrics(state=state, inputs=xb, targets=yb)\n\n    key = (jax.random.split(key)[0])\n\n    if step == 0 or (step+1) % 100 == 0: # one training epoch has passed\n        for metric,value in state.metrics.compute().items(): # compute metrics\n            metrics_history[f'train_{metric}'].append(value) # record metrics\n        state = state.replace(metrics=state.metrics.empty()) # reset train_metrics for next training epoch\n\n        # Compute metrics on the test set after each training epoch\n        test_state = state\n        x_test, y_test = get_batch(key, test_data)\n    #     for test_batch in test_ds.as_numpy_iterator():\n        test_state = compute_metrics(state=test_state, inputs=x_test, targets=y_test)\n\n        for metric,value in test_state.metrics.compute().items():\n            metrics_history[f'test_{metric}'].append(value)\n\n        print(f\"train epoch: {(step+1)}, \"\n              f\"loss: {metrics_history['train_loss'][-1]}, \"\n              f\"accuracy: {metrics_history['train_accuracy'][-1] * 100}\")\n        print(f\"test epoch: {(step+1) }, \"\n          f\"loss: {metrics_history['test_loss'][-1]}, \"\n          f\"accuracy: {metrics_history['test_accuracy'][-1] * 100}\")","metadata":{"id":"CaNt9JazHFyw","outputId":"ba447ddf-9940-44a6-f4b2-d27ed78a88c2","execution":{"iopub.status.busy":"2024-06-10T06:56:14.600735Z","iopub.status.idle":"2024-06-10T06:56:14.601232Z","shell.execute_reply.started":"2024-06-10T06:56:14.600951Z","shell.execute_reply":"2024-06-10T06:56:14.600971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt  # Visualization\n\n# Plot loss and accuracy in subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\nax1.set_title('Loss')\nax2.set_title('Accuracy')\nfor dataset in ('train','test'):\n    ax1.plot(metrics_history[f'{dataset}_loss'], label=f'{dataset}_loss')\n    ax2.plot(metrics_history[f'{dataset}_accuracy'], label=f'{dataset}_accuracy')\nax1.legend()\nax2.legend()\nplt.show()\nplt.clf()","metadata":{"id":"Y40JGx1YHFyw","execution":{"iopub.status.busy":"2024-06-10T06:56:14.602659Z","iopub.status.idle":"2024-06-10T06:56:14.603154Z","shell.execute_reply.started":"2024-06-10T06:56:14.602898Z","shell.execute_reply":"2024-06-10T06:56:14.602920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nlogits = fin_model.apply(fin_params, xb, training=False)[0]\nloss = optax.softmax_cross_entropy_with_integer_labels(\n            logits=logits, labels=yb).mean()\n\nprint(loss)","metadata":{"id":"7pJlFXpVHFyw","execution":{"iopub.status.busy":"2024-06-10T06:56:14.604304Z","iopub.status.idle":"2024-06-10T06:56:14.604752Z","shell.execute_reply.started":"2024-06-10T06:56:14.604516Z","shell.execute_reply":"2024-06-10T06:56:14.604535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def generate_text(idx, max_new_tokens, params):\n# # idx is (B, T) array of indices in the current context\n#     for i in range(max_new_tokens):\n#         # crop idx to the last block_size tokens\n#         idx_cond = idx[:, -block_size:]\n#         # get the predictions\n#         logits = fin_model.apply(params, idx_cond)\n#         # focus only on the last time step\n#         logits = logits[:, -1, :] # becomes (B, C)\n\n#         if i == 0:\n#             rng, rng_subkey = jax.random.split(jax.random.PRNGKey(12))\n#         else:\n#             rng, rng_subkey = jax.random.split(rng)\n\n#         idx_next = jax.random.categorical(rng_subkey, logits, axis=-1, shape=(1, 1)) # (B, 1)\n\n\n#         # append sampled index to the running sequence\n#         idx = jnp.concatenate([idx, idx_next], axis=-1) # (B, T+1)\n\n#     return idx","metadata":{"id":"9d28o-dTHFyx","execution":{"iopub.status.busy":"2024-06-10T06:56:14.605844Z","iopub.status.idle":"2024-06-10T06:56:14.606299Z","shell.execute_reply.started":"2024-06-10T06:56:14.606055Z","shell.execute_reply":"2024-06-10T06:56:14.606074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@partial(jax.jit, static_argnames=(\"self\", \"length\"))\ndef generate_text(rng, params, length):\n    def _scan_generate(carry, _):\n        random_key, context = carry\n        logits = fin_model.apply(params, context, training=False)[0]\n        rng, rng_subkey = jax.random.split(random_key)\n        new_token = jax.random.categorical(\n          rng_subkey, logits[:, -1, :], axis=-1, shape=(1, 1)\n        )\n        context = jnp.concatenate([context[:, 1:], new_token], axis=1)\n        return (rng, context), new_token\n\n    _, new_tokens = jax.lax.scan(\n    _scan_generate,\n    (rng, jnp.zeros((1, block_size), dtype=jnp.int32)),\n    (),\n    length=length,\n    )\n    return new_tokens","metadata":{"id":"WB0og7pAHFyx","execution":{"iopub.status.busy":"2024-06-10T06:56:14.607427Z","iopub.status.idle":"2024-06-10T06:56:14.607878Z","shell.execute_reply.started":"2024-06-10T06:56:14.607650Z","shell.execute_reply":"2024-06-10T06:56:14.607670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_tokenz = 1000\nkey, subkey = jax.random.split(jax.random.PRNGKey(156))\n# key, subkey = jax.random.split(key)\n# token_gen = generate_text(jnp.zeros((1,block_size)).astype(jnp.int32), new_tokenz, {'params': state.params})\ntoken_gen = generate_text(key, {'params': state.params}, new_tokenz)[:, 0, 0].tolist()\nprint(token_gen)\nprint(decode(token_gen))","metadata":{"id":"50Vpg2lEHFyx","execution":{"iopub.status.busy":"2024-06-10T06:56:14.609298Z","iopub.status.idle":"2024-06-10T06:56:14.609754Z","shell.execute_reply.started":"2024-06-10T06:56:14.609505Z","shell.execute_reply":"2024-06-10T06:56:14.609524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sdgh  fs","metadata":{"execution":{"iopub.status.busy":"2024-06-10T06:56:14.610815Z","iopub.status.idle":"2024-06-10T06:56:14.611266Z","shell.execute_reply.started":"2024-06-10T06:56:14.611024Z","shell.execute_reply":"2024-06-10T06:56:14.611042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"state.params","metadata":{"execution":{"iopub.status.busy":"2024-06-10T06:56:14.612449Z","iopub.status.idle":"2024-06-10T06:56:14.613136Z","shell.execute_reply.started":"2024-06-10T06:56:14.612666Z","shell.execute_reply":"2024-06-10T06:56:14.612685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install mamba-ssm","metadata":{"id":"MOw_xjbrHFy0","execution":{"iopub.status.busy":"2024-06-10T06:56:14.614670Z","iopub.status.idle":"2024-06-10T06:56:14.614973Z","shell.execute_reply.started":"2024-06-10T06:56:14.614821Z","shell.execute_reply":"2024-06-10T06:56:14.614834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ones = lambda *size: torch.ones(*size).float().cuda()\nzeros = lambda *size: torch.zeros(*size).float().cuda()\narange = lambda n: torch.arange(n).float().cuda()\nrand = lambda size: torch.rand(*size).abs().float().cuda()\n\ndef create_torch(S = 128, Ba = 2, D = 4, N = 4):\n    x = rand((Ba, 1, D, S))\n    a = -ones((Ba, N, D, 1))\n    b = ones((Ba, N, 1, S)) * 0.1\n    c = rand((Ba, N, 1, S)) * 0.1\n    delta = rand((Ba, 1, D, S)) * 0.1\n    return x, a, b, c, delta","metadata":{"id":"W_PAnYcEOR22","execution":{"iopub.status.busy":"2024-06-10T06:56:14.616033Z","iopub.status.idle":"2024-06-10T06:56:14.616379Z","shell.execute_reply.started":"2024-06-10T06:56:14.616191Z","shell.execute_reply":"2024-06-10T06:56:14.616227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import selective_scan_cuda\n\nxx, aa, bb, cc, ddelta = create_torch()\ny_from_repo = selective_scan_cuda.fwd(xx.squeeze(1), ddelta.squeeze(1), aa[0].squeeze(-1).T, bb.squeeze(-2)[:, None, :, :], cc.squeeze(-2)[:, None, :, :], None, None, None, False)\ny_from_repo","metadata":{"id":"ykh4GTvtOrak","execution":{"iopub.status.busy":"2024-06-10T06:56:14.617652Z","iopub.status.idle":"2024-06-10T06:56:14.617953Z","shell.execute_reply.started":"2024-06-10T06:56:14.617802Z","shell.execute_reply":"2024-06-10T06:56:14.617815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def discretize(a, b, delta):\n    da = delta * a\n    a_ = jnp.exp(da)\n    b_ = b * delta\n    return a_, b_\n\ndef ssm(x, a, b, c, delta):\n    \"Jax Implementation\"\n    y = []\n    h = 0\n    a_, b_ = discretize(a, b, delta)\n    for k in range(x.shape[-1]):\n        h = a_[..., k] * h + b_[..., k] * x[..., k]\n        y.append((c[..., k] * h).sum(1, keepdims=True))\n    return h, jnp.stack(y, -1)\n","metadata":{"id":"NEdG1yPNOtxU","execution":{"iopub.status.busy":"2024-06-10T06:56:14.619530Z","iopub.status.idle":"2024-06-10T06:56:14.619851Z","shell.execute_reply.started":"2024-06-10T06:56:14.619697Z","shell.execute_reply":"2024-06-10T06:56:14.619710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_, y_ = ssm(xx.cpu().numpy(), aa.cpu().numpy(), bb.cpu().numpy(), cc.cpu().numpy(), ddelta.cpu().numpy())","metadata":{"id":"GEjNcZSZPIp_","execution":{"iopub.status.busy":"2024-06-10T06:56:14.621088Z","iopub.status.idle":"2024-06-10T06:56:14.621435Z","shell.execute_reply.started":"2024-06-10T06:56:14.621270Z","shell.execute_reply":"2024-06-10T06:56:14.621284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"tWlqZZOmPnYk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from mamba_ssm import Mamba as Mamba_T\ntorch_mamba = Mamba_T(\n      # This module uses roughly 3 * expand * d_model^2 parameters\n      d_model=n_embd, # Model dimension d_model\n      d_state=16,  # SSM state expansion factor\n      d_conv=4,    # Local convolution width\n      expand=2,    # Block expansion factor\n)","metadata":{"id":"5RHAE_I1Pql9","execution":{"iopub.status.busy":"2024-06-10T06:56:14.622406Z","iopub.status.idle":"2024-06-10T06:56:14.622746Z","shell.execute_reply.started":"2024-06-10T06:56:14.622583Z","shell.execute_reply":"2024-06-10T06:56:14.622597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xm = x = rand((1, 1, n_embd, 32))\nxm.shape","metadata":{"id":"l9zw_M-USrDt","execution":{"iopub.status.busy":"2024-06-10T06:56:14.623881Z","iopub.status.idle":"2024-06-10T06:56:14.624234Z","shell.execute_reply.started":"2024-06-10T06:56:14.624046Z","shell.execute_reply":"2024-06-10T06:56:14.624060Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch_mamba(xm.squeeze(1))","metadata":{"id":"gGmA2EWlTCo0","execution":{"iopub.status.busy":"2024-06-10T06:56:14.625294Z","iopub.status.idle":"2024-06-10T06:56:14.625637Z","shell.execute_reply.started":"2024-06-10T06:56:14.625467Z","shell.execute_reply":"2024-06-10T06:56:14.625481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch_mamba.in_proj","metadata":{"id":"73ek9mx9UBBl","execution":{"iopub.status.busy":"2024-06-10T06:56:14.626803Z","iopub.status.idle":"2024-06-10T06:56:14.627131Z","shell.execute_reply.started":"2024-06-10T06:56:14.626969Z","shell.execute_reply":"2024-06-10T06:56:14.626984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import CLIPTokenizer\ntokenizer_1 = CLIPTokenizer.from_pretrained('openai/clip-vit-base-patch32')","metadata":{"id":"P3l_ssIYbiYT","execution":{"iopub.status.busy":"2024-06-10T06:56:14.628344Z","iopub.status.idle":"2024-06-10T06:56:14.628689Z","shell.execute_reply.started":"2024-06-10T06:56:14.628518Z","shell.execute_reply":"2024-06-10T06:56:14.628532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenise_prompts(prompt):\n    inputs = []\n    for tokenizer in [tokenizer_1, tokenizer_2]:\n        text_inputs = tokenizer(\n            positive_prompt,\n            padding=\"max_length\",\n            max_length=tokenizer.model_max_length,\n            truncation=True,\n            return_tensors=\"np\",\n        )\n        inputs.append(text_inputs.input_ids)\n    return jnp.stack(inputs, axis=1)","metadata":{"id":"-X7hXQRMZhl3","execution":{"iopub.status.busy":"2024-06-10T06:56:14.629877Z","iopub.status.idle":"2024-06-10T06:56:14.630184Z","shell.execute_reply.started":"2024-06-10T06:56:14.630032Z","shell.execute_reply":"2024-06-10T06:56:14.630046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xm.squeeze(1).shape","metadata":{"id":"rJhKQ_Oua9Gy","execution":{"iopub.status.busy":"2024-06-10T06:56:14.631636Z","iopub.status.idle":"2024-06-10T06:56:14.631970Z","shell.execute_reply.started":"2024-06-10T06:56:14.631807Z","shell.execute_reply":"2024-06-10T06:56:14.631821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"mzkoYrSVkoJj"},"execution_count":null,"outputs":[]}]}