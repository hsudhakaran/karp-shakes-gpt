{"metadata":{"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8773959,"sourceType":"datasetVersion","datasetId":5155031}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install -q clu","metadata":{"id":"gS6euWNvHFye","outputId":"45b149a7-9450-439c-da67-ab8678a3b0d0","execution":{"iopub.status.busy":"2024-07-15T09:26:02.263948Z","iopub.execute_input":"2024-07-15T09:26:02.264286Z","iopub.status.idle":"2024-07-15T09:26:02.269888Z","shell.execute_reply.started":"2024-07-15T09:26:02.264257Z","shell.execute_reply":"2024-07-15T09:26:02.268892Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# # We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt","metadata":{"id":"7jjCLfuUHFyg","outputId":"dfe048f0-dd44-40ef-edf3-2fa56558672f","execution":{"iopub.status.busy":"2024-07-15T09:26:02.271805Z","iopub.execute_input":"2024-07-15T09:26:02.272231Z","iopub.status.idle":"2024-07-15T09:26:02.283284Z","shell.execute_reply.started":"2024-07-15T09:26:02.272206Z","shell.execute_reply":"2024-07-15T09:26:02.282464Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from functools import partial\nimport jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\nfrom jax.nn.initializers import lecun_normal, normal\nfrom jax.numpy.linalg import eigh, inv, matrix_power\nfrom jax.scipy.signal import convolve\n\nimport tensorflow_datasets as tfds\n\nimport torch\n\nfrom dataclasses import dataclass\n\nfrom typing import Union\n\nimport matplotlib.pyplot as plt\nimport seaborn\n\n# from clu import metrics\nfrom flax.training import train_state  # Useful dataclass to keep train state\nfrom flax import struct                # Flax dataclasses\nimport optax                           # Common loss functions and optimizers\nfrom tqdm import tqdm\n\nimport pdb","metadata":{"id":"YXSCJzupHFyh","execution":{"iopub.status.busy":"2024-07-15T09:26:02.284285Z","iopub.execute_input":"2024-07-15T09:26:02.284691Z","iopub.status.idle":"2024-07-15T09:26:09.913701Z","shell.execute_reply.started":"2024-07-15T09:26:02.284609Z","shell.execute_reply":"2024-07-15T09:26:09.912946Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# read it in to inspect it\n# with open('/kaggle/input/shak-new-input/input.txt', 'r', encoding='utf-8') as f:\n#     text = f.read()","metadata":{"id":"KpJoV3KQHFyh","execution":{"iopub.status.busy":"2024-07-15T09:26:09.915496Z","iopub.execute_input":"2024-07-15T09:26:09.916010Z","iopub.status.idle":"2024-07-15T09:26:09.919808Z","shell.execute_reply.started":"2024-07-15T09:26:09.915983Z","shell.execute_reply":"2024-07-15T09:26:09.918955Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# ds = tfds.load(\"tiny_shakespeare\")\n\n# # combine train and test examples into a single string\n# text_train = \"\"\n# for example in ds[\"train\"].concatenate(ds[\"test\"]).as_numpy_iterator():\n#     text_train += example[\"text\"].decode(\"utf-8\")\n\n# # similarly, create a single string for validation\n# text_validation = \"\"\n# for example in ds[\"validation\"].as_numpy_iterator():\n#     text_validation += example[\"text\"].decode(\"utf-8\")","metadata":{"execution":{"iopub.status.busy":"2024-07-15T09:26:09.920992Z","iopub.execute_input":"2024-07-15T09:26:09.921306Z","iopub.status.idle":"2024-07-15T09:26:09.930887Z","shell.execute_reply.started":"2024-07-15T09:26:09.921282Z","shell.execute_reply":"2024-07-15T09:26:09.929948Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# del ds\n# with open('train.txt', 'w') as file:\n#     file.write(text_train)\n    \n# with open('test.txt', 'w') as file:\n#     file.write(text_validation)    ","metadata":{"execution":{"iopub.status.busy":"2024-07-15T09:26:09.932217Z","iopub.execute_input":"2024-07-15T09:26:09.932502Z","iopub.status.idle":"2024-07-15T09:26:09.940214Z","shell.execute_reply.started":"2024-07-15T09:26:09.932480Z","shell.execute_reply":"2024-07-15T09:26:09.939458Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/input/shak-new-input/train.txt', 'r', encoding='utf-8') as f:\n    text_train = f.read()\n    \nwith open('/kaggle/input/shak-new-input/test.txt', 'r', encoding='utf-8') as f:\n    text_validation = f.read()    ","metadata":{"execution":{"iopub.status.busy":"2024-07-15T09:26:09.941311Z","iopub.execute_input":"2024-07-15T09:26:09.941727Z","iopub.status.idle":"2024-07-15T09:26:09.968772Z","shell.execute_reply.started":"2024-07-15T09:26:09.941698Z","shell.execute_reply":"2024-07-15T09:26:09.967936Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# here are all the unique characters that occur in this text\nchars = sorted(list(set(text_train+text_validation)))\n# chars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)","metadata":{"id":"PsWxZqyRHFyi","outputId":"b1730724-647e-45cd-edfa-97af24995830","execution":{"iopub.status.busy":"2024-07-15T09:26:09.969702Z","iopub.execute_input":"2024-07-15T09:26:09.969947Z","iopub.status.idle":"2024-07-15T09:26:09.991304Z","shell.execute_reply.started":"2024-07-15T09:26:09.969904Z","shell.execute_reply":"2024-07-15T09:26:09.990435Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n65\n","output_type":"stream"}]},{"cell_type":"code","source":"# from transformers import AutoTokenizer\n\n# tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Phi-3-mini-4k-instruct\", padding_side=\"left\")","metadata":{"execution":{"iopub.status.busy":"2024-07-15T09:26:09.994293Z","iopub.execute_input":"2024-07-15T09:26:09.994561Z","iopub.status.idle":"2024-07-15T09:26:10.002351Z","shell.execute_reply.started":"2024-07-15T09:26:09.994539Z","shell.execute_reply":"2024-07-15T09:26:10.001527Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# text_inputs = tokenizer(text, return_tensors=\"np\")\n# data = jnp.array(text_inputs['input_ids'][0])","metadata":{"execution":{"iopub.status.busy":"2024-07-15T09:26:10.003535Z","iopub.execute_input":"2024-07-15T09:26:10.003971Z","iopub.status.idle":"2024-07-15T09:26:10.010621Z","shell.execute_reply.started":"2024-07-15T09:26:10.003942Z","shell.execute_reply":"2024-07-15T09:26:10.009766Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# vocab_size = tokenizer.vocab_size\n# print(vocab_size)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T09:26:10.011634Z","iopub.execute_input":"2024-07-15T09:26:10.011959Z","iopub.status.idle":"2024-07-15T09:26:10.019186Z","shell.execute_reply.started":"2024-07-15T09:26:10.011910Z","shell.execute_reply":"2024-07-15T09:26:10.018427Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# print(tokenizer.decode((text_inputs['input_ids'][0][0:100]).tolist()))","metadata":{"execution":{"iopub.status.busy":"2024-07-15T09:26:10.020077Z","iopub.execute_input":"2024-07-15T09:26:10.020334Z","iopub.status.idle":"2024-07-15T09:26:10.028004Z","shell.execute_reply.started":"2024-07-15T09:26:10.020313Z","shell.execute_reply":"2024-07-15T09:26:10.027144Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# create a mapping from characters to integers\nstoi = { ch: i for i,ch in enumerate(chars) }\nitos = { i: ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))","metadata":{"id":"S-mzLOk1HFyi","outputId":"f56e2f85-5a1c-4099-87df-436ba39f4363","execution":{"iopub.status.busy":"2024-07-15T09:26:10.029091Z","iopub.execute_input":"2024-07-15T09:26:10.029325Z","iopub.status.idle":"2024-07-15T09:26:10.038043Z","shell.execute_reply.started":"2024-07-15T09:26:10.029304Z","shell.execute_reply":"2024-07-15T09:26:10.037071Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"[46, 47, 47, 1, 58, 46, 43, 56, 43]\nhii there\n","output_type":"stream"}]},{"cell_type":"code","source":"# data = jnp.array(encode(text), dtype=jnp.int32)\n# print(data.shape, data.dtype)\n# print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this","metadata":{"id":"HImuqDd8HFyj","outputId":"91dcd15f-f068-4551-ad29-e6e41e52fd91","execution":{"iopub.status.busy":"2024-07-15T09:26:10.039056Z","iopub.execute_input":"2024-07-15T09:26:10.039355Z","iopub.status.idle":"2024-07-15T09:26:10.045994Z","shell.execute_reply.started":"2024-07-15T09:26:10.039323Z","shell.execute_reply":"2024-07-15T09:26:10.045236Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# train_test_split = 0.9\n# n = int(train_test_split*len(data))\n# train_data = data[:n]\n# test_data = data[n:]\n\ntrain_data = jnp.array(encode(text_train), dtype=jnp.int32)\ntest_data = jnp.array(encode(text_validation), dtype=jnp.int32)","metadata":{"id":"pXrAqMxRHFyj","execution":{"iopub.status.busy":"2024-07-15T09:26:10.047034Z","iopub.execute_input":"2024-07-15T09:26:10.047287Z","iopub.status.idle":"2024-07-15T09:26:12.908454Z","shell.execute_reply.started":"2024-07-15T09:26:10.047266Z","shell.execute_reply":"2024-07-15T09:26:12.907674Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"block_size = 8\ntrain_data[:block_size+1]","metadata":{"id":"ahhKyiAzHFyj","outputId":"98306c96-5082-4dfa-ba66-915051831fc8","execution":{"iopub.status.busy":"2024-07-15T09:26:12.909762Z","iopub.execute_input":"2024-07-15T09:26:12.910112Z","iopub.status.idle":"2024-07-15T09:26:13.090843Z","shell.execute_reply.started":"2024-07-15T09:26:12.910079Z","shell.execute_reply":"2024-07-15T09:26:13.089957Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"Array([18, 47, 56, 57, 58,  1, 15, 47, 58], dtype=int32)"},"metadata":{}}]},{"cell_type":"code","source":"x = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")","metadata":{"id":"HIpsznQmHFyk","outputId":"be9d197b-0b79-43ed-f3a9-e74295d51c79","execution":{"iopub.status.busy":"2024-07-15T09:26:13.092026Z","iopub.execute_input":"2024-07-15T09:26:13.092308Z","iopub.status.idle":"2024-07-15T09:26:13.696570Z","shell.execute_reply.started":"2024-07-15T09:26:13.092284Z","shell.execute_reply":"2024-07-15T09:26:13.695683Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"when input is [18] the target: 47\nwhen input is [18 47] the target: 56\nwhen input is [18 47 56] the target: 57\nwhen input is [18 47 56 57] the target: 58\nwhen input is [18 47 56 57 58] the target: 1\nwhen input is [18 47 56 57 58  1] the target: 15\nwhen input is [18 47 56 57 58  1 15] the target: 47\nwhen input is [18 47 56 57 58  1 15 47] the target: 58\n","output_type":"stream"}]},{"cell_type":"code","source":"batch_size = 128 # how many independent sequences will we process in parallel?\nblock_size = 64 # what is the maximum context length for predictions?\nmax_iters = 50000\nlearning_rate = 5e-4\n# device = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 100\nn_embd = 256\nexpans = 2\nn_heads = 1\nchannel_size = n_embd // n_heads\nn_layers = 6\ndropout = 0.2\nconv_k_size = 3\nn_latent_dim = 16\nn_tokens = 1\n\nrng_key = jax.random.PRNGKey(1564)\n\ndynamic_slice_vmap = jax.vmap(jax.lax.dynamic_slice, in_axes=(None, 0, None))\n\n@jax.jit\ndef get_batch(random_key, data):\n    \"\"\"Prepares a random batch of training data.\n\n    Args:\n      random_key: A random seed for sampling a batch.\n      data: The complete training dataset.\n\n    Returns:\n      x: Input sequences.\n      y: Target sequences (shifted inputs).\n    \"\"\"\n    ix = jax.random.randint(\n      random_key, shape=(batch_size, 1), minval=0, maxval=len(data) - block_size\n    )\n    x = dynamic_slice_vmap(data, ix, (block_size,))\n    y = dynamic_slice_vmap(data, ix + n_tokens, (block_size,))\n    return x, y\n\nxb, yb = get_batch(rng_key, train_data)\ntrain_shape = xb.shape\nprint('inputs:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\n# print('----')\n\n# for b in range(batch_size): # batch dimension\n#     for t in range(block_size): # time dimension\n#         context = xb[b, :t+1]\n#         target = yb[b,t]\n#         print(f\"when input is {context} the target: {target}\")","metadata":{"id":"UuAjtqPeHFyk","outputId":"6a88fb2b-b798-4ee9-9f4f-f38ce898d576","execution":{"iopub.status.busy":"2024-07-15T09:26:13.697589Z","iopub.execute_input":"2024-07-15T09:26:13.697836Z","iopub.status.idle":"2024-07-15T09:26:14.058388Z","shell.execute_reply.started":"2024-07-15T09:26:13.697814Z","shell.execute_reply":"2024-07-15T09:26:14.057408Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"inputs:\n(128, 64)\n[[12  0  0 ... 53 42  1]\n [ 1 44 39 ... 56  1 45]\n [56 57  8 ... 10  0 17]\n ...\n [54 53 53 ... 17 17 26]\n [59 58 58 ... 16 21 33]\n [50  1 57 ... 47 58  1]]\ntargets:\n(128, 64)\n[[ 0  0 15 ... 42  1 40]\n [44 39 56 ...  1 45 56]\n [57  8  0 ...  0 17 47]\n ...\n [53 53 56 ... 17 26  1]\n [58 58 43 ... 21 33 31]\n [ 1 57 53 ... 58  1 40]]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(xb[0])\nprint(yb[0])","metadata":{"execution":{"iopub.status.busy":"2024-07-15T09:26:14.059642Z","iopub.execute_input":"2024-07-15T09:26:14.059945Z","iopub.status.idle":"2024-07-15T09:26:14.158614Z","shell.execute_reply.started":"2024-07-15T09:26:14.059906Z","shell.execute_reply":"2024-07-15T09:26:14.157684Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"[12  0  0 15 27 25 21 26 21 33 31 10  0  5 32 61 39 57  1 44 56 53 51  1\n 58 46 43  1 41 39 52 53 52  8  0  0 15 27 30 21 27 24 13 26 33 31 10  0\n  5 31 46 39 50 50  5  2  0 27  1 45 53 53 42  1]\n[ 0  0 15 27 25 21 26 21 33 31 10  0  5 32 61 39 57  1 44 56 53 51  1 58\n 46 43  1 41 39 52 53 52  8  0  0 15 27 30 21 27 24 13 26 33 31 10  0  5\n 31 46 39 50 50  5  2  0 27  1 45 53 53 42  1 40]\n","output_type":"stream"}]},{"cell_type":"code","source":"# hidden_state = [jnp.zeros((1,n_latent_dim, n_embd * expans)) for _ in range(n_layers)]\n# hidden_state[0].shape","metadata":{"execution":{"iopub.status.busy":"2024-07-15T09:26:14.159758Z","iopub.execute_input":"2024-07-15T09:26:14.160062Z","iopub.status.idle":"2024-07-15T09:26:14.163948Z","shell.execute_reply.started":"2024-07-15T09:26:14.160036Z","shell.execute_reply":"2024-07-15T09:26:14.162942Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# Mamba Block\nDense --> Conv1D --> Silu --> SSM --> Silu -->","metadata":{"id":"yOccqzJlHFym"}},{"cell_type":"code","source":"class Mamba(nn.Module):\n\n    def setup(self):\n        emb_features = n_embd * expans\n        self.in_proj1 = nn.Conv(features=n_embd, kernel_size=conv_k_size,padding=1) #nn.Dense(features=emb_features)\n        self.in_proj2 = nn.Dense(features=emb_features)\n\n        # Adjusted for Flax. Flax does not have nn.Conv1d, so you might need to reshape or use a different approach\n        self.conv1d = nn.Conv(features=emb_features,\n                              kernel_size=conv_k_size,\n                              padding='CAUSAL',\n                              )\n\n        self.A = -1*self.param('A', nn.initializers.ones, (1, n_latent_dim, emb_features, 1))\n        self.B = 0.1*self.param('B', nn.initializers.ones, (1, n_latent_dim, 1, block_size))\n        self.C = 0.09*self.param('C', jax.random.normal, (1, n_latent_dim, 1, block_size))\n        self.D = 0.1*self.param('D', jax.random.normal, (1, 1,emb_features, block_size))\n        self.delta = 0.05*self.param('delta', jax.random.normal, (1, 1,emb_features, block_size))\n\n        self.out_proj = nn.Dense(n_embd // n_heads)\n        \n        self.hidden_state = self.variable('other_variables','hidden_state', \n                                          jnp.zeros, \n                                          (1,n_latent_dim, emb_features))\n#         self.rms_norm = nn.RMSNorm()\n\n    def __call__(self, embeds):\n        x = self.in_proj1(embeds)\n        x = jax.nn.silu(x) #new\n        x = self.conv1d(x)\n        x = jax.nn.silu(x)\n#         pdb.set_trace()\n\n        x = jnp.expand_dims(jnp.transpose(x,(0,2,1)), axis=1)\n#         x = x.reshape((x.shape[0],1,x.shape[2],x.shape[1]))\n        x = self.ssm(x)\n#         x = x.reshape((x.shape[0],x.shape[3],x.shape[2]))\n        x = jnp.transpose(x[:,0,:,:],(0,2,1))\n        x = x*jnp.concatenate([jax.nn.silu(self.in_proj2(embeds))[:,1:,:],jnp.ones((x.shape[0],1,x.shape[-1]))], axis=1)\n\n        x = self.out_proj(x)\n\n#         x = self.rms_norm(x)\n\n        return x\n    def discretize(self):\n        da = self.delta * self.A\n        a_ = jnp.exp(da)\n        b_ = self.B * self.delta\n        return a_, b_\n\n    def ssm(self, x):\n        a_, b_ = self.discretize()\n        h = 0\n        for k in range(x.shape[-1]):\n            h = a_[..., k] * h + b_[..., k] * x[..., k]\n#         _, N, D, S = a_.shape\n#         indices = jnp.tril(jnp.ones((S-1,S-1))) \n#         indices = jnp.expand_dims(a_[...,1:],axis=4)*jnp.expand_dims(indices, axis=(0,1,2)) + jnp.expand_dims(jnp.triu(jnp.ones((S-1,S-1)),1), axis=(0,1,2))\n#         indices = (jnp.concatenate((indices, jnp.ones((1,N,D,S-1,1))), axis=-1)).prod(axis=-2)\n#         h = (indices*(b_*x)).sum(axis=-1)\n\n        y = ((self.C * jax.lax.expand_dims(h,[3])).sum(1, keepdims=True) + self.D*x)\n        \n#         self.hidden_state.value = jax.nn.standardize(h.mean(0, keepdims=True))\n        return y","metadata":{"id":"4qOdblU5HFyo","execution":{"iopub.status.busy":"2024-07-15T09:26:14.165318Z","iopub.execute_input":"2024-07-15T09:26:14.165626Z","iopub.status.idle":"2024-07-15T09:26:14.182997Z","shell.execute_reply.started":"2024-07-15T09:26:14.165602Z","shell.execute_reply":"2024-07-15T09:26:14.182130Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# class MultiHeadMamba(nn.Module):\n#     def setup(self):\n#         self.heads = [Mamba() for _ in range(n_heads)]\n#         self.rms_norm = nn.RMSNorm()\n\n#     def __call__(self, x):\n#         out = jnp.concatenate([h(x) for h in self.heads], axis=-1)\n#         x = self.rms_norm(out)\n#         return x","metadata":{"id":"0bH9vlLZHFyq","execution":{"iopub.status.busy":"2024-07-15T09:26:14.184156Z","iopub.execute_input":"2024-07-15T09:26:14.184458Z","iopub.status.idle":"2024-07-15T09:26:14.194983Z","shell.execute_reply.started":"2024-07-15T09:26:14.184435Z","shell.execute_reply":"2024-07-15T09:26:14.194097Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# class FeedForward(nn.Module):\n#     def setup(self):\n#         self.ffn = nn.Sequential([\n#             nn.Dense(4 * n_embd),\n#             nn.relu,\n#             nn.Dense(n_embd)]\n#         )\n#     def __call__(self, x):\n#         return self.ffn(x)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T09:26:14.195992Z","iopub.execute_input":"2024-07-15T09:26:14.196261Z","iopub.status.idle":"2024-07-15T09:26:14.204398Z","shell.execute_reply.started":"2024-07-15T09:26:14.196231Z","shell.execute_reply":"2024-07-15T09:26:14.203588Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# class MambaBlock(nn.Module):\n#     def setup(self):\n#         self.mamba_block = Mamba()\n#         self.ln1 = nn.RMSNorm()\n#         self.ffn = FeedForward()\n#         self.ln2 = nn.LayerNorm()\n\n#     def __call__(self, x):\n#         x = x + self.mamba_block(self.ln2(x))\n#         x = x + self.ffn(self.ln1(x))\n#         return x\n","metadata":{"id":"UiCxIjoEp2QA","execution":{"iopub.status.busy":"2024-07-15T09:26:14.211535Z","iopub.execute_input":"2024-07-15T09:26:14.211772Z","iopub.status.idle":"2024-07-15T09:26:14.215739Z","shell.execute_reply.started":"2024-07-15T09:26:14.211753Z","shell.execute_reply":"2024-07-15T09:26:14.214813Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# class MambaModel(nn.Module):\n\n#     def setup(self):\n#         self.tok_embeddings = nn.Embed(vocab_size, n_embd)\n#         self.pos_embeddings = nn.Embed(block_size, n_embd)\n#         self.ln = nn.LayerNorm()\n#         self.mamba_layers = [MambaBlock() for _ in range(n_layers)]\n#         self.preds_out = nn.Dense(vocab_size)\n\n#     def __call__(self, x, training: bool):\n#         x = self.tok_embeddings(x) + self.pos_embeddings(jnp.arange(block_size))\n# #         x = self.ln(x)\n#         for layer in self.mamba_layers:\n#             x = layer(x)\n            \n#         return self.preds_out(x)\n\n#     @jax.jit\n#     def generate(self, idx, max_new_tokens, params):\n#     # idx is (B, T) array of indices in the current context\n#         for _ in range(max_new_tokens):\n#             # crop idx to the last block_size tokens\n#             idx_cond = idx[:, -block_size:]\n#             # get the predictions\n#             logits = self.apply(params, idx_cond)\n#             # focus only on the last time step\n#             logits = logits[:, -1, :] # becomes (B, C)\n#             # apply softmax to get probabilities\n#             ##probs = tf.keras.activations.softmax(logits, dim=-1) # (B, C)\n#             # sample from the distribution\n#             idx_next = jax.random.categorical(jax.random.PRNGKey(52), logits) # (B, 1)\n#             # append sampled index to the running sequence\n#             idx = jax.numpy.expand_dims(jnp.concatenate([idx[0], idx_next], axis=0), 0) # (B, T+1)\n#     #         print(idx_next)\n#     #         print(idx)\n\n#         return idx","metadata":{"id":"y4C7OWL8HFyq","execution":{"iopub.status.busy":"2024-07-15T09:26:14.216759Z","iopub.execute_input":"2024-07-15T09:26:14.217025Z","iopub.status.idle":"2024-07-15T09:26:14.226730Z","shell.execute_reply.started":"2024-07-15T09:26:14.217003Z","shell.execute_reply":"2024-07-15T09:26:14.225967Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# model = Mamba()\n# params = model.init(jax.random.key(42), jnp.ones((1,64,256)))\n# # print(params['other_variables']['hidden_state'].shape, params['other_variables']['hidden_state'].min(), params['other_variables']['hidden_state'].max())\n# # print(model.tabulate(jax.random.key(0), jnp.ones((1,64,256)),\n# #                    compute_flops=True, compute_vjp_flops=True))\n# xs = model.apply(params, jnp.ones((1,64,256)), mutable=['other_variables'])\n# # # print(params['other_variables']['hidden_state'].shape, params['other_variables']['hidden_state'].min(), params['other_variables']['hidden_state'].max())\n# xb.shape, xs[0].shape, xs[1].keys()","metadata":{"id":"wTd3jSQWHFyp","execution":{"iopub.status.busy":"2024-07-15T09:26:14.227748Z","iopub.execute_input":"2024-07-15T09:26:14.228434Z","iopub.status.idle":"2024-07-15T09:26:14.240246Z","shell.execute_reply.started":"2024-07-15T09:26:14.228403Z","shell.execute_reply":"2024-07-15T09:26:14.239440Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# print(xs[1]['other_variables']['hidden_state'].shape, xs[1]['other_variables']['hidden_state'].min(), xs[1]['other_variables']['hidden_state'].max())","metadata":{"execution":{"iopub.status.busy":"2024-07-15T09:26:14.241401Z","iopub.execute_input":"2024-07-15T09:26:14.241691Z","iopub.status.idle":"2024-07-15T09:26:14.253683Z","shell.execute_reply.started":"2024-07-15T09:26:14.241658Z","shell.execute_reply":"2024-07-15T09:26:14.252849Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# xfs = model.apply(params, 2*jnp.ones((1,64,256)), mutable=['other_variables'])\n# print(params['other_variables']['hidden_state'].shape, params['other_variables']['hidden_state'].min(), params['other_variables']['hidden_state'].max())\n# print(xfs[1]['other_variables']['hidden_state'].shape, xfs[1]['other_variables']['hidden_state'].min(), xfs[1]['other_variables']['hidden_state'].max())","metadata":{"execution":{"iopub.status.busy":"2024-07-15T09:26:14.254806Z","iopub.execute_input":"2024-07-15T09:26:14.255620Z","iopub.status.idle":"2024-07-15T09:26:14.263275Z","shell.execute_reply.started":"2024-07-15T09:26:14.255590Z","shell.execute_reply":"2024-07-15T09:26:14.262463Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# test_model = Mamba()\n# test_params = test_model.init(jax.random.key(42), xb)\n# n_params = sum(p.size for p in jax.tree_util.tree_leaves(test_params))\n# print(f\"Total number of parameters: {n_params:_}\")\n# # print(fin_model.tabulate(jax.random.key(42), xb,\n# #                    compute_flops=True, compute_vjp_flops=True))\n# xf = test_model.apply(test_params, xb)\n# xb.shape, xf.shape","metadata":{"id":"cm2a0nepHFyq","execution":{"iopub.status.busy":"2024-07-15T09:26:14.264224Z","iopub.execute_input":"2024-07-15T09:26:14.264471Z","iopub.status.idle":"2024-07-15T09:26:14.273540Z","shell.execute_reply.started":"2024-07-15T09:26:14.264450Z","shell.execute_reply":"2024-07-15T09:26:14.272768Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"class NanoLM(nn.Module):\n    \"\"\"NanoLM model.\"\"\"\n    vocab_size: int = 65\n    num_layers: int = 6\n    num_heads: int = 8\n    head_size: int = 32\n    dropout_rate: float = 0.2\n    embed_size: int = 256\n    block_size: int = 64\n\n    @nn.compact\n    def __call__(self, x, training: bool):\n        breakpoint()\n        x = nn.Embed(self.vocab_size, self.embed_size)(x) + nn.Embed(\n            self.block_size, self.embed_size\n        )(jnp.arange(self.block_size))\n        \n        for i in range(self.num_layers):\n#             x = x + nn.MultiHeadDotProductAttention(\n#               num_heads=self.num_heads,\n#               qkv_features=self.head_size,\n#               out_features=self.head_size * self.num_heads,\n#               dropout_rate=self.dropout_rate,\n#             )(\n#               x_norm,\n#               x_norm,\n#               mask=jnp.tril(jnp.ones((x.shape[-2], x.shape[-2]))),\n#               deterministic=not training,\n#             )\n    \n            x = Mamba()(nn.RMSNorm()(x)) * jnp.concatenate([x[:,1:,:],jnp.ones((x.shape[0],1,x.shape[-1]))], axis=1)\n#             x = x + nn.Sequential([\n#               nn.Dense(4 * self.embed_size),\n#               nn.relu,\n#               nn.Dropout(self.dropout_rate, deterministic=not training),\n#               nn.Dense(self.embed_size),\n#             ])(nn.RMSNorm()(x))\n\n        x = nn.Dense(self.vocab_size)(nn.RMSNorm()(x))\n        return x","metadata":{"id":"zuiaFP6WHFyr","execution":{"iopub.status.busy":"2024-07-15T09:26:14.274439Z","iopub.execute_input":"2024-07-15T09:26:14.274672Z","iopub.status.idle":"2024-07-15T09:26:14.285080Z","shell.execute_reply.started":"2024-07-15T09:26:14.274652Z","shell.execute_reply":"2024-07-15T09:26:14.284211Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# key = jax.random.key(42)\n\n# # fin_model = MambaModel()\n# # fin_params = fin_model.init(key, xb, training=False)\n\n\n# fin_model = NanoLM(\n#     vocab_size=vocab_size,\n#     num_layers=n_layers,\n#     num_heads=8,\n#     head_size=32,\n#     dropout_rate=0.2,\n#     embed_size=n_embd,\n#     block_size=block_size,\n# )\n\n# fin_params = fin_model.init(\n#     {'params': key},\n#     jnp.ones((batch_size, block_size), dtype=jnp.int32),\n#     training=False\n# )\n\n# n_params = sum(p.size for p in jax.tree_util.tree_leaves(fin_params))\n# print(f\"Total number of parameters: {n_params:_}\")\n# # print(fin_model.tabulate(jax.random.key(42), xb,\n# #                    compute_flops=True, compute_vjp_flops=True))\n# xf = fin_model.apply(fin_params, xb, training=False)[0]\n# xb.shape, xf.shape","metadata":{"id":"fnUQPyuvHFys","outputId":"f04ebf31-d67f-4488-dd5d-7fd5b20dd1ea","execution":{"iopub.status.busy":"2024-07-15T09:26:14.287515Z","iopub.execute_input":"2024-07-15T09:26:14.288208Z","iopub.status.idle":"2024-07-15T09:26:14.298450Z","shell.execute_reply.started":"2024-07-15T09:26:14.288179Z","shell.execute_reply":"2024-07-15T09:26:14.297610Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"def loss_fun(params, x, y, var_params,dropout_key):\n    logits, updated_variables = model.apply({'params': params, **var_params}, x, training=True, rngs={\"dropout\": dropout_key}, mutable=['other_variables'])\n    accuracy = jnp.mean(jnp.argmax(logits, axis=-1) == y)\n    return optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=y).mean(), (updated_variables, accuracy)\n\n@jax.jit\ndef eval_step(params, x, y, var_params):\n    logits, _ = model.apply({'params': params, **var_params}, x, training=False, mutable=['other_variables'])\n    accuracy = jnp.mean(jnp.argmax(logits, axis=-1) == y)\n    return optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=y).mean(), accuracy","metadata":{"execution":{"iopub.status.busy":"2024-07-15T09:26:14.299436Z","iopub.execute_input":"2024-07-15T09:26:14.299702Z","iopub.status.idle":"2024-07-15T09:26:14.311938Z","shell.execute_reply.started":"2024-07-15T09:26:14.299679Z","shell.execute_reply":"2024-07-15T09:26:14.311226Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"key = jax.random.PRNGKey(42)\nkey, subkey = jax.random.split(key)\n\nmodel = NanoLM(\n    vocab_size=vocab_size,\n    num_layers=n_layers,\n    num_heads=8,\n    head_size=32,\n    dropout_rate=0.2,\n    embed_size=n_embd,\n    block_size=block_size,\n)\n\nvar_params = model.init(\n    key,\n    jnp.ones((batch_size, block_size), dtype=jnp.int32),\n    training=False,\n)\nprint(var_params.keys())\nn_params = sum(p.size for p in jax.tree_util.tree_leaves(var_params))\n\nprint(f\"Total number of parameters: {n_params:_}\")","metadata":{"id":"PKpb3864HFyt","execution":{"iopub.status.busy":"2024-07-15T09:26:14.313085Z","iopub.execute_input":"2024-07-15T09:26:14.313444Z","iopub.status.idle":"2024-07-15T09:26:24.954461Z","shell.execute_reply.started":"2024-07-15T09:26:14.313416Z","shell.execute_reply":"2024-07-15T09:26:24.953464Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"dict_keys(['params', 'other_variables'])\nTotal number of parameters: 5_676_353\n","output_type":"stream"}]},{"cell_type":"code","source":"var_params['params']['Embed_0']['embedding'].shape","metadata":{"execution":{"iopub.status.busy":"2024-07-15T09:26:24.955656Z","iopub.execute_input":"2024-07-15T09:26:24.956030Z","iopub.status.idle":"2024-07-15T09:26:24.962157Z","shell.execute_reply.started":"2024-07-15T09:26:24.955998Z","shell.execute_reply":"2024-07-15T09:26:24.961224Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"(65, 256)"},"metadata":{}}]},{"cell_type":"code","source":"params = var_params.pop('params')","metadata":{"execution":{"iopub.status.busy":"2024-07-15T09:26:24.963207Z","iopub.execute_input":"2024-07-15T09:26:24.963462Z","iopub.status.idle":"2024-07-15T09:26:24.974455Z","shell.execute_reply.started":"2024-07-15T09:26:24.963440Z","shell.execute_reply":"2024-07-15T09:26:24.973603Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"var_params = jax.tree_map(lambda x: jnp.zeros_like(x), var_params)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T09:26:24.975411Z","iopub.execute_input":"2024-07-15T09:26:24.975638Z","iopub.status.idle":"2024-07-15T09:26:24.986748Z","shell.execute_reply.started":"2024-07-15T09:26:24.975617Z","shell.execute_reply":"2024-07-15T09:26:24.985877Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# decay_rate = 0.96\n# learning_rate_schedule = optax.exponential_decay(learning_rate, decay_rate, max_iters//1000)\nopt = optax.adamw(learning_rate=learning_rate)\n\nopt_state = opt.init(params)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T09:26:24.987907Z","iopub.execute_input":"2024-07-15T09:26:24.988181Z","iopub.status.idle":"2024-07-15T09:26:25.382902Z","shell.execute_reply.started":"2024-07-15T09:26:24.988160Z","shell.execute_reply":"2024-07-15T09:26:25.381948Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"%%time\n\nall_train_losses = []\nall_eval_losses = []\n\nall_train_accuracy =  []\nall_test_accuracy = []\n\n# we define one iteration of the optimizer and JIT this function\n@jax.jit\ndef step(key, params, var_params, opt_state):\n    key, subkey = jax.random.split(key)\n    xb, yb = get_batch(key, train_data)\n    (loss, aux_data), grad = jax.value_and_grad(loss_fun, has_aux=True)(params, xb, yb, var_params, subkey)\n    var_params, train_accuracy = aux_data\n    updates, opt_state = opt.update(grad, opt_state, params)\n    params = optax.apply_updates(params, updates)\n    return params, key, opt_state, loss, var_params, train_accuracy\n\n# for i in tqdm(range(max_iters)):\ncounter = 0\nloss = 10\nwhile counter<max_iters: # and loss > 1.0:\n\n    params, key, opt_state, loss, var_params, train_accuracy = step(key, params, var_params, opt_state)\n    \n\n    # once every N_FREQ_EVAL we compute loss on the validation set\n    if counter % eval_iters == 0:\n        key, subkey = jax.random.split(key)\n        eval_loss, eval_accuracy = eval_step(params, *get_batch(subkey, test_data), var_params)\n        all_train_losses.append(loss)\n        all_eval_losses.append(eval_loss)\n        all_train_accuracy.append(train_accuracy)\n        all_test_accuracy.append(eval_accuracy)\n        print('##########################################################')\n        print(\"Step: \", counter,\"\\t Train Loss: \", loss,\"\\t Train Accuracy: \", format(train_accuracy, \".2%\"))\n        print(\"Step: \", counter,\"\\t Eval Loss: \", eval_loss,\"\\t Eval Accuracy: \", format(eval_accuracy, \".2%\"))\n        \n    counter += 1\n        ","metadata":{"execution":{"iopub.status.busy":"2024-07-15T09:26:25.384332Z","iopub.execute_input":"2024-07-15T09:26:25.384693Z","iopub.status.idle":"2024-07-15T12:10:48.540630Z","shell.execute_reply.started":"2024-07-15T09:26:25.384661Z","shell.execute_reply":"2024-07-15T12:10:48.539662Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"##########################################################\nStep:  0 \t Train Loss:  4.179881 \t Train Accuracy:  3.32%\nStep:  0 \t Eval Loss:  4.1672344 \t Eval Accuracy:  15.00%\n##########################################################\nStep:  100 \t Train Loss:  3.9900093 \t Train Accuracy:  17.50%\nStep:  100 \t Eval Loss:  3.9881847 \t Eval Accuracy:  17.36%\n##########################################################\nStep:  200 \t Train Loss:  3.9196405 \t Train Accuracy:  18.15%\nStep:  200 \t Eval Loss:  3.920411 \t Eval Accuracy:  18.40%\n##########################################################\nStep:  300 \t Train Loss:  3.8727484 \t Train Accuracy:  18.20%\nStep:  300 \t Eval Loss:  3.877905 \t Eval Accuracy:  18.19%\n##########################################################\nStep:  400 \t Train Loss:  3.8241873 \t Train Accuracy:  18.44%\nStep:  400 \t Eval Loss:  3.821784 \t Eval Accuracy:  18.14%\n##########################################################\nStep:  500 \t Train Loss:  3.7248394 \t Train Accuracy:  19.36%\nStep:  500 \t Eval Loss:  3.7367897 \t Eval Accuracy:  19.35%\n##########################################################\nStep:  600 \t Train Loss:  3.6859705 \t Train Accuracy:  19.90%\nStep:  600 \t Eval Loss:  3.693698 \t Eval Accuracy:  19.67%\n##########################################################\nStep:  700 \t Train Loss:  3.6631756 \t Train Accuracy:  19.31%\nStep:  700 \t Eval Loss:  3.661425 \t Eval Accuracy:  19.70%\n##########################################################\nStep:  800 \t Train Loss:  3.6291108 \t Train Accuracy:  19.17%\nStep:  800 \t Eval Loss:  3.6323214 \t Eval Accuracy:  19.41%\n##########################################################\nStep:  900 \t Train Loss:  3.6080854 \t Train Accuracy:  19.18%\nStep:  900 \t Eval Loss:  3.5916166 \t Eval Accuracy:  19.65%\n##########################################################\nStep:  1000 \t Train Loss:  3.5455751 \t Train Accuracy:  19.93%\nStep:  1000 \t Eval Loss:  3.5734966 \t Eval Accuracy:  19.17%\n##########################################################\nStep:  1100 \t Train Loss:  3.53469 \t Train Accuracy:  19.78%\nStep:  1100 \t Eval Loss:  3.5503037 \t Eval Accuracy:  19.59%\n##########################################################\nStep:  1200 \t Train Loss:  3.5008788 \t Train Accuracy:  19.41%\nStep:  1200 \t Eval Loss:  3.5153484 \t Eval Accuracy:  19.52%\n##########################################################\nStep:  1300 \t Train Loss:  3.4869711 \t Train Accuracy:  19.57%\nStep:  1300 \t Eval Loss:  3.4768486 \t Eval Accuracy:  19.90%\n##########################################################\nStep:  1400 \t Train Loss:  3.446604 \t Train Accuracy:  19.70%\nStep:  1400 \t Eval Loss:  3.466568 \t Eval Accuracy:  19.76%\n##########################################################\nStep:  1500 \t Train Loss:  3.4324355 \t Train Accuracy:  19.93%\nStep:  1500 \t Eval Loss:  3.4546957 \t Eval Accuracy:  19.45%\n##########################################################\nStep:  1600 \t Train Loss:  3.4015284 \t Train Accuracy:  19.91%\nStep:  1600 \t Eval Loss:  3.4194732 \t Eval Accuracy:  20.24%\n##########################################################\nStep:  1700 \t Train Loss:  3.3839552 \t Train Accuracy:  19.69%\nStep:  1700 \t Eval Loss:  3.4084287 \t Eval Accuracy:  19.62%\n##########################################################\nStep:  1800 \t Train Loss:  3.363515 \t Train Accuracy:  20.01%\nStep:  1800 \t Eval Loss:  3.4018574 \t Eval Accuracy:  19.48%\n##########################################################\nStep:  1900 \t Train Loss:  3.3491666 \t Train Accuracy:  20.47%\nStep:  1900 \t Eval Loss:  3.344025 \t Eval Accuracy:  20.42%\n##########################################################\nStep:  2000 \t Train Loss:  3.31597 \t Train Accuracy:  20.67%\nStep:  2000 \t Eval Loss:  3.3064818 \t Eval Accuracy:  20.89%\n##########################################################\nStep:  2100 \t Train Loss:  3.3100634 \t Train Accuracy:  20.48%\nStep:  2100 \t Eval Loss:  3.294226 \t Eval Accuracy:  21.17%\n##########################################################\nStep:  2200 \t Train Loss:  3.2705526 \t Train Accuracy:  21.20%\nStep:  2200 \t Eval Loss:  3.2628675 \t Eval Accuracy:  21.39%\n##########################################################\nStep:  2300 \t Train Loss:  3.2732008 \t Train Accuracy:  20.70%\nStep:  2300 \t Eval Loss:  3.2733645 \t Eval Accuracy:  20.94%\n##########################################################\nStep:  2400 \t Train Loss:  3.2340233 \t Train Accuracy:  21.36%\nStep:  2400 \t Eval Loss:  3.2817857 \t Eval Accuracy:  20.36%\n##########################################################\nStep:  2500 \t Train Loss:  3.241179 \t Train Accuracy:  20.89%\nStep:  2500 \t Eval Loss:  3.2700367 \t Eval Accuracy:  20.63%\n##########################################################\nStep:  2600 \t Train Loss:  3.2395797 \t Train Accuracy:  21.00%\nStep:  2600 \t Eval Loss:  3.2514048 \t Eval Accuracy:  20.67%\n##########################################################\nStep:  2700 \t Train Loss:  3.2177062 \t Train Accuracy:  21.34%\nStep:  2700 \t Eval Loss:  3.2242436 \t Eval Accuracy:  21.08%\n##########################################################\nStep:  2800 \t Train Loss:  3.2233975 \t Train Accuracy:  20.85%\nStep:  2800 \t Eval Loss:  3.2263465 \t Eval Accuracy:  20.70%\n##########################################################\nStep:  2900 \t Train Loss:  3.199746 \t Train Accuracy:  21.09%\nStep:  2900 \t Eval Loss:  3.2030268 \t Eval Accuracy:  21.39%\n##########################################################\nStep:  3000 \t Train Loss:  3.1892276 \t Train Accuracy:  20.78%\nStep:  3000 \t Eval Loss:  3.1923134 \t Eval Accuracy:  21.17%\n##########################################################\nStep:  3100 \t Train Loss:  3.1580868 \t Train Accuracy:  20.96%\nStep:  3100 \t Eval Loss:  3.1674838 \t Eval Accuracy:  21.45%\n##########################################################\nStep:  3200 \t Train Loss:  3.1594758 \t Train Accuracy:  21.51%\nStep:  3200 \t Eval Loss:  3.196029 \t Eval Accuracy:  21.17%\n##########################################################\nStep:  3300 \t Train Loss:  3.1622686 \t Train Accuracy:  21.22%\nStep:  3300 \t Eval Loss:  3.174526 \t Eval Accuracy:  20.97%\n##########################################################\nStep:  3400 \t Train Loss:  3.1300664 \t Train Accuracy:  21.17%\nStep:  3400 \t Eval Loss:  3.159705 \t Eval Accuracy:  21.13%\n##########################################################\nStep:  3500 \t Train Loss:  3.174961 \t Train Accuracy:  21.12%\nStep:  3500 \t Eval Loss:  3.1818335 \t Eval Accuracy:  20.95%\n##########################################################\nStep:  3600 \t Train Loss:  3.1255312 \t Train Accuracy:  21.25%\nStep:  3600 \t Eval Loss:  3.1607432 \t Eval Accuracy:  21.33%\n##########################################################\nStep:  3700 \t Train Loss:  3.169606 \t Train Accuracy:  20.92%\nStep:  3700 \t Eval Loss:  3.1539683 \t Eval Accuracy:  20.92%\n##########################################################\nStep:  3800 \t Train Loss:  3.1235642 \t Train Accuracy:  21.13%\nStep:  3800 \t Eval Loss:  3.1488342 \t Eval Accuracy:  21.11%\n##########################################################\nStep:  3900 \t Train Loss:  3.125152 \t Train Accuracy:  21.06%\nStep:  3900 \t Eval Loss:  3.1511273 \t Eval Accuracy:  20.94%\n##########################################################\nStep:  4000 \t Train Loss:  3.1217966 \t Train Accuracy:  21.12%\nStep:  4000 \t Eval Loss:  3.1006937 \t Eval Accuracy:  21.46%\n##########################################################\nStep:  4100 \t Train Loss:  3.0586648 \t Train Accuracy:  22.46%\nStep:  4100 \t Eval Loss:  3.0613236 \t Eval Accuracy:  22.47%\n##########################################################\nStep:  4200 \t Train Loss:  2.997504 \t Train Accuracy:  23.91%\nStep:  4200 \t Eval Loss:  3.0042768 \t Eval Accuracy:  24.08%\n##########################################################\nStep:  4300 \t Train Loss:  2.8973255 \t Train Accuracy:  25.88%\nStep:  4300 \t Eval Loss:  2.9323468 \t Eval Accuracy:  25.56%\n##########################################################\nStep:  4400 \t Train Loss:  2.8150942 \t Train Accuracy:  27.04%\nStep:  4400 \t Eval Loss:  2.8425517 \t Eval Accuracy:  27.53%\n##########################################################\nStep:  4500 \t Train Loss:  2.7858005 \t Train Accuracy:  28.49%\nStep:  4500 \t Eval Loss:  2.7946737 \t Eval Accuracy:  28.71%\n##########################################################\nStep:  4600 \t Train Loss:  2.5983574 \t Train Accuracy:  32.20%\nStep:  4600 \t Eval Loss:  2.6768882 \t Eval Accuracy:  31.09%\n##########################################################\nStep:  4700 \t Train Loss:  2.3644903 \t Train Accuracy:  37.24%\nStep:  4700 \t Eval Loss:  2.365068 \t Eval Accuracy:  37.28%\n##########################################################\nStep:  4800 \t Train Loss:  1.8481991 \t Train Accuracy:  47.75%\nStep:  4800 \t Eval Loss:  1.8644965 \t Eval Accuracy:  48.00%\n##########################################################\nStep:  4900 \t Train Loss:  1.260772 \t Train Accuracy:  61.99%\nStep:  4900 \t Eval Loss:  1.2860644 \t Eval Accuracy:  61.90%\n##########################################################\nStep:  5000 \t Train Loss:  0.7911726 \t Train Accuracy:  75.89%\nStep:  5000 \t Eval Loss:  0.71065056 \t Eval Accuracy:  77.80%\n##########################################################\nStep:  5100 \t Train Loss:  0.40995994 \t Train Accuracy:  87.99%\nStep:  5100 \t Eval Loss:  0.43718833 \t Eval Accuracy:  87.08%\n##########################################################\nStep:  5200 \t Train Loss:  0.26134667 \t Train Accuracy:  93.08%\nStep:  5200 \t Eval Loss:  0.2898219 \t Eval Accuracy:  92.03%\n##########################################################\nStep:  5300 \t Train Loss:  0.17743862 \t Train Accuracy:  95.46%\nStep:  5300 \t Eval Loss:  0.1786869 \t Eval Accuracy:  95.36%\n##########################################################\nStep:  5400 \t Train Loss:  0.13283819 \t Train Accuracy:  96.45%\nStep:  5400 \t Eval Loss:  0.13538292 \t Eval Accuracy:  96.44%\n##########################################################\nStep:  5500 \t Train Loss:  0.09307568 \t Train Accuracy:  97.61%\nStep:  5500 \t Eval Loss:  0.10458794 \t Eval Accuracy:  97.35%\n##########################################################\nStep:  5600 \t Train Loss:  0.08829057 \t Train Accuracy:  97.69%\nStep:  5600 \t Eval Loss:  0.08451925 \t Eval Accuracy:  97.99%\n##########################################################\nStep:  5700 \t Train Loss:  0.0715593 \t Train Accuracy:  98.22%\nStep:  5700 \t Eval Loss:  0.087588176 \t Eval Accuracy:  97.68%\n##########################################################\nStep:  5800 \t Train Loss:  0.058861457 \t Train Accuracy:  98.68%\nStep:  5800 \t Eval Loss:  0.059900254 \t Eval Accuracy:  98.60%\n##########################################################\nStep:  5900 \t Train Loss:  0.055379815 \t Train Accuracy:  98.56%\nStep:  5900 \t Eval Loss:  0.053999074 \t Eval Accuracy:  98.61%\n##########################################################\nStep:  6000 \t Train Loss:  0.05054442 \t Train Accuracy:  98.75%\nStep:  6000 \t Eval Loss:  0.05358671 \t Eval Accuracy:  98.65%\n##########################################################\nStep:  6100 \t Train Loss:  0.04898846 \t Train Accuracy:  98.69%\nStep:  6100 \t Eval Loss:  0.047106672 \t Eval Accuracy:  98.84%\n##########################################################\nStep:  6200 \t Train Loss:  0.04815656 \t Train Accuracy:  98.72%\nStep:  6200 \t Eval Loss:  0.050165474 \t Eval Accuracy:  98.71%\n##########################################################\nStep:  6300 \t Train Loss:  0.036856696 \t Train Accuracy:  99.00%\nStep:  6300 \t Eval Loss:  0.041935526 \t Eval Accuracy:  98.90%\n##########################################################\nStep:  6400 \t Train Loss:  0.056710772 \t Train Accuracy:  98.50%\nStep:  6400 \t Eval Loss:  0.05361297 \t Eval Accuracy:  98.56%\n##########################################################\nStep:  6500 \t Train Loss:  0.04104153 \t Train Accuracy:  98.89%\nStep:  6500 \t Eval Loss:  0.042344425 \t Eval Accuracy:  98.83%\n##########################################################\nStep:  6600 \t Train Loss:  0.03752908 \t Train Accuracy:  99.08%\nStep:  6600 \t Eval Loss:  0.04058604 \t Eval Accuracy:  98.94%\n##########################################################\nStep:  6700 \t Train Loss:  0.03563408 \t Train Accuracy:  98.95%\nStep:  6700 \t Eval Loss:  0.034130484 \t Eval Accuracy:  99.12%\n##########################################################\nStep:  6800 \t Train Loss:  0.034912094 \t Train Accuracy:  99.02%\nStep:  6800 \t Eval Loss:  0.036568508 \t Eval Accuracy:  98.96%\n##########################################################\nStep:  6900 \t Train Loss:  0.030058729 \t Train Accuracy:  99.17%\nStep:  6900 \t Eval Loss:  0.034664184 \t Eval Accuracy:  99.06%\n##########################################################\nStep:  7000 \t Train Loss:  0.030215478 \t Train Accuracy:  99.11%\nStep:  7000 \t Eval Loss:  0.03527738 \t Eval Accuracy:  99.02%\n##########################################################\nStep:  7100 \t Train Loss:  0.030195324 \t Train Accuracy:  99.16%\nStep:  7100 \t Eval Loss:  0.031822816 \t Eval Accuracy:  99.05%\n##########################################################\nStep:  7200 \t Train Loss:  0.04051804 \t Train Accuracy:  98.88%\nStep:  7200 \t Eval Loss:  0.034378693 \t Eval Accuracy:  99.01%\n##########################################################\nStep:  7300 \t Train Loss:  0.031010155 \t Train Accuracy:  99.08%\nStep:  7300 \t Eval Loss:  0.03876377 \t Eval Accuracy:  98.91%\n##########################################################\nStep:  7400 \t Train Loss:  0.04050418 \t Train Accuracy:  98.93%\nStep:  7400 \t Eval Loss:  0.040548276 \t Eval Accuracy:  98.84%\n##########################################################\nStep:  7500 \t Train Loss:  0.028168635 \t Train Accuracy:  99.19%\nStep:  7500 \t Eval Loss:  0.038359113 \t Eval Accuracy:  98.94%\n##########################################################\nStep:  7600 \t Train Loss:  0.050179 \t Train Accuracy:  98.43%\nStep:  7600 \t Eval Loss:  0.044574652 \t Eval Accuracy:  98.72%\n##########################################################\nStep:  7700 \t Train Loss:  0.053017545 \t Train Accuracy:  98.47%\nStep:  7700 \t Eval Loss:  0.05518089 \t Eval Accuracy:  98.55%\n##########################################################\nStep:  7800 \t Train Loss:  0.037386272 \t Train Accuracy:  99.04%\nStep:  7800 \t Eval Loss:  0.04000605 \t Eval Accuracy:  98.91%\n##########################################################\nStep:  7900 \t Train Loss:  0.03152375 \t Train Accuracy:  99.15%\nStep:  7900 \t Eval Loss:  0.03460556 \t Eval Accuracy:  98.99%\n##########################################################\nStep:  8000 \t Train Loss:  0.034016214 \t Train Accuracy:  99.08%\nStep:  8000 \t Eval Loss:  0.03815353 \t Eval Accuracy:  98.85%\n##########################################################\nStep:  8100 \t Train Loss:  0.029624613 \t Train Accuracy:  99.19%\nStep:  8100 \t Eval Loss:  0.032500166 \t Eval Accuracy:  98.99%\n##########################################################\nStep:  8200 \t Train Loss:  0.030124616 \t Train Accuracy:  99.18%\nStep:  8200 \t Eval Loss:  0.035019897 \t Eval Accuracy:  98.94%\n##########################################################\nStep:  8300 \t Train Loss:  0.03559515 \t Train Accuracy:  99.00%\nStep:  8300 \t Eval Loss:  0.035776198 \t Eval Accuracy:  99.00%\n##########################################################\nStep:  8400 \t Train Loss:  0.028284475 \t Train Accuracy:  99.22%\nStep:  8400 \t Eval Loss:  0.0336183 \t Eval Accuracy:  99.11%\n##########################################################\nStep:  8500 \t Train Loss:  0.028917417 \t Train Accuracy:  99.08%\nStep:  8500 \t Eval Loss:  0.03882476 \t Eval Accuracy:  98.94%\n##########################################################\nStep:  8600 \t Train Loss:  0.028457176 \t Train Accuracy:  99.08%\nStep:  8600 \t Eval Loss:  0.03112243 \t Eval Accuracy:  99.13%\n##########################################################\nStep:  8700 \t Train Loss:  0.025940655 \t Train Accuracy:  99.23%\nStep:  8700 \t Eval Loss:  0.028500423 \t Eval Accuracy:  99.05%\n##########################################################\nStep:  8800 \t Train Loss:  0.02504237 \t Train Accuracy:  99.26%\nStep:  8800 \t Eval Loss:  0.031770635 \t Eval Accuracy:  99.05%\n##########################################################\nStep:  8900 \t Train Loss:  0.029638616 \t Train Accuracy:  99.17%\nStep:  8900 \t Eval Loss:  0.026245084 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  9000 \t Train Loss:  0.027024888 \t Train Accuracy:  99.18%\nStep:  9000 \t Eval Loss:  0.029935252 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  9100 \t Train Loss:  0.023855377 \t Train Accuracy:  99.38%\nStep:  9100 \t Eval Loss:  0.034914088 \t Eval Accuracy:  99.11%\n##########################################################\nStep:  9200 \t Train Loss:  0.07667361 \t Train Accuracy:  98.05%\nStep:  9200 \t Eval Loss:  0.06278421 \t Eval Accuracy:  98.22%\n##########################################################\nStep:  9300 \t Train Loss:  0.027050838 \t Train Accuracy:  99.22%\nStep:  9300 \t Eval Loss:  0.036604695 \t Eval Accuracy:  98.95%\n##########################################################\nStep:  9400 \t Train Loss:  0.030050984 \t Train Accuracy:  99.10%\nStep:  9400 \t Eval Loss:  0.028190352 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  9500 \t Train Loss:  0.027392931 \t Train Accuracy:  99.22%\nStep:  9500 \t Eval Loss:  0.02999444 \t Eval Accuracy:  99.10%\n##########################################################\nStep:  9600 \t Train Loss:  0.024243128 \t Train Accuracy:  99.28%\nStep:  9600 \t Eval Loss:  0.03137427 \t Eval Accuracy:  99.08%\n##########################################################\nStep:  9700 \t Train Loss:  0.028629092 \t Train Accuracy:  99.16%\nStep:  9700 \t Eval Loss:  0.028286291 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  9800 \t Train Loss:  0.02761832 \t Train Accuracy:  99.23%\nStep:  9800 \t Eval Loss:  0.028863912 \t Eval Accuracy:  99.16%\n##########################################################\nStep:  9900 \t Train Loss:  0.02910256 \t Train Accuracy:  99.15%\nStep:  9900 \t Eval Loss:  0.03253266 \t Eval Accuracy:  99.04%\n##########################################################\nStep:  10000 \t Train Loss:  0.030403221 \t Train Accuracy:  98.95%\nStep:  10000 \t Eval Loss:  0.034534104 \t Eval Accuracy:  99.04%\n##########################################################\nStep:  10100 \t Train Loss:  0.03276351 \t Train Accuracy:  99.02%\nStep:  10100 \t Eval Loss:  0.031766184 \t Eval Accuracy:  99.12%\n##########################################################\nStep:  10200 \t Train Loss:  0.027641099 \t Train Accuracy:  99.19%\nStep:  10200 \t Eval Loss:  0.030198349 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  10300 \t Train Loss:  0.026179004 \t Train Accuracy:  99.32%\nStep:  10300 \t Eval Loss:  0.030743746 \t Eval Accuracy:  99.08%\n##########################################################\nStep:  10400 \t Train Loss:  0.021413585 \t Train Accuracy:  99.41%\nStep:  10400 \t Eval Loss:  0.028295364 \t Eval Accuracy:  99.05%\n##########################################################\nStep:  10500 \t Train Loss:  0.025868773 \t Train Accuracy:  99.29%\nStep:  10500 \t Eval Loss:  0.030867549 \t Eval Accuracy:  99.11%\n##########################################################\nStep:  10600 \t Train Loss:  0.025372263 \t Train Accuracy:  99.21%\nStep:  10600 \t Eval Loss:  0.028133245 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  10700 \t Train Loss:  0.028382542 \t Train Accuracy:  99.11%\nStep:  10700 \t Eval Loss:  0.028261079 \t Eval Accuracy:  99.17%\n##########################################################\nStep:  10800 \t Train Loss:  0.032836948 \t Train Accuracy:  99.07%\nStep:  10800 \t Eval Loss:  0.0330329 \t Eval Accuracy:  98.95%\n##########################################################\nStep:  10900 \t Train Loss:  0.024517514 \t Train Accuracy:  99.22%\nStep:  10900 \t Eval Loss:  0.029487133 \t Eval Accuracy:  99.06%\n##########################################################\nStep:  11000 \t Train Loss:  0.025594905 \t Train Accuracy:  99.21%\nStep:  11000 \t Eval Loss:  0.028107483 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  11100 \t Train Loss:  0.027291188 \t Train Accuracy:  99.26%\nStep:  11100 \t Eval Loss:  0.029707355 \t Eval Accuracy:  99.15%\n##########################################################\nStep:  11200 \t Train Loss:  0.029560799 \t Train Accuracy:  99.24%\nStep:  11200 \t Eval Loss:  0.029781643 \t Eval Accuracy:  99.15%\n##########################################################\nStep:  11300 \t Train Loss:  0.026637373 \t Train Accuracy:  99.24%\nStep:  11300 \t Eval Loss:  0.0265592 \t Eval Accuracy:  99.17%\n##########################################################\nStep:  11400 \t Train Loss:  0.03022781 \t Train Accuracy:  99.06%\nStep:  11400 \t Eval Loss:  0.029056001 \t Eval Accuracy:  99.17%\n##########################################################\nStep:  11500 \t Train Loss:  0.029924605 \t Train Accuracy:  99.01%\nStep:  11500 \t Eval Loss:  0.03178016 \t Eval Accuracy:  99.06%\n##########################################################\nStep:  11600 \t Train Loss:  0.025417602 \t Train Accuracy:  99.26%\nStep:  11600 \t Eval Loss:  0.027707338 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  11700 \t Train Loss:  0.024903739 \t Train Accuracy:  99.24%\nStep:  11700 \t Eval Loss:  0.028603403 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  11800 \t Train Loss:  0.027934214 \t Train Accuracy:  99.17%\nStep:  11800 \t Eval Loss:  0.033957005 \t Eval Accuracy:  99.08%\n##########################################################\nStep:  11900 \t Train Loss:  0.034053948 \t Train Accuracy:  99.00%\nStep:  11900 \t Eval Loss:  0.037727065 \t Eval Accuracy:  98.95%\n##########################################################\nStep:  12000 \t Train Loss:  0.02807106 \t Train Accuracy:  99.13%\nStep:  12000 \t Eval Loss:  0.027679227 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  12100 \t Train Loss:  0.026049038 \t Train Accuracy:  99.24%\nStep:  12100 \t Eval Loss:  0.028259639 \t Eval Accuracy:  99.17%\n##########################################################\nStep:  12200 \t Train Loss:  0.021607062 \t Train Accuracy:  99.44%\nStep:  12200 \t Eval Loss:  0.026775502 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  12300 \t Train Loss:  0.024505273 \t Train Accuracy:  99.21%\nStep:  12300 \t Eval Loss:  0.027575234 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  12400 \t Train Loss:  0.02124229 \t Train Accuracy:  99.29%\nStep:  12400 \t Eval Loss:  0.024922589 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  12500 \t Train Loss:  0.026761254 \t Train Accuracy:  99.32%\nStep:  12500 \t Eval Loss:  0.026814155 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  12600 \t Train Loss:  0.021349348 \t Train Accuracy:  99.33%\nStep:  12600 \t Eval Loss:  0.024650272 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  12700 \t Train Loss:  0.027762461 \t Train Accuracy:  99.18%\nStep:  12700 \t Eval Loss:  0.023413986 \t Eval Accuracy:  99.34%\n##########################################################\nStep:  12800 \t Train Loss:  0.03466029 \t Train Accuracy:  99.02%\nStep:  12800 \t Eval Loss:  0.03515944 \t Eval Accuracy:  99.02%\n##########################################################\nStep:  12900 \t Train Loss:  0.024533965 \t Train Accuracy:  99.24%\nStep:  12900 \t Eval Loss:  0.029198844 \t Eval Accuracy:  99.12%\n##########################################################\nStep:  13000 \t Train Loss:  0.02756507 \t Train Accuracy:  99.11%\nStep:  13000 \t Eval Loss:  0.030383745 \t Eval Accuracy:  98.99%\n##########################################################\nStep:  13100 \t Train Loss:  0.026490968 \t Train Accuracy:  99.26%\nStep:  13100 \t Eval Loss:  0.025194667 \t Eval Accuracy:  99.32%\n##########################################################\nStep:  13200 \t Train Loss:  0.027612505 \t Train Accuracy:  99.18%\nStep:  13200 \t Eval Loss:  0.027685784 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  13300 \t Train Loss:  0.02752059 \t Train Accuracy:  99.15%\nStep:  13300 \t Eval Loss:  0.03213805 \t Eval Accuracy:  99.10%\n##########################################################\nStep:  13400 \t Train Loss:  0.027413884 \t Train Accuracy:  99.18%\nStep:  13400 \t Eval Loss:  0.028471755 \t Eval Accuracy:  99.15%\n##########################################################\nStep:  13500 \t Train Loss:  0.024424572 \t Train Accuracy:  99.28%\nStep:  13500 \t Eval Loss:  0.030006334 \t Eval Accuracy:  99.15%\n##########################################################\nStep:  13600 \t Train Loss:  0.034268957 \t Train Accuracy:  99.18%\nStep:  13600 \t Eval Loss:  0.030648924 \t Eval Accuracy:  99.11%\n##########################################################\nStep:  13700 \t Train Loss:  0.030924052 \t Train Accuracy:  99.12%\nStep:  13700 \t Eval Loss:  0.026542068 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  13800 \t Train Loss:  0.023551334 \t Train Accuracy:  99.37%\nStep:  13800 \t Eval Loss:  0.028115239 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  13900 \t Train Loss:  0.023420585 \t Train Accuracy:  99.34%\nStep:  13900 \t Eval Loss:  0.028000232 \t Eval Accuracy:  99.16%\n##########################################################\nStep:  14000 \t Train Loss:  0.026377957 \t Train Accuracy:  99.26%\nStep:  14000 \t Eval Loss:  0.02635552 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  14100 \t Train Loss:  0.022832941 \t Train Accuracy:  99.23%\nStep:  14100 \t Eval Loss:  0.023474965 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  14200 \t Train Loss:  0.020430453 \t Train Accuracy:  99.44%\nStep:  14200 \t Eval Loss:  0.0297247 \t Eval Accuracy:  99.16%\n##########################################################\nStep:  14300 \t Train Loss:  0.024688669 \t Train Accuracy:  99.28%\nStep:  14300 \t Eval Loss:  0.027212948 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  14400 \t Train Loss:  0.023302183 \t Train Accuracy:  99.28%\nStep:  14400 \t Eval Loss:  0.028893713 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  14500 \t Train Loss:  0.023438502 \t Train Accuracy:  99.26%\nStep:  14500 \t Eval Loss:  0.028919281 \t Eval Accuracy:  99.10%\n##########################################################\nStep:  14600 \t Train Loss:  0.022094108 \t Train Accuracy:  99.35%\nStep:  14600 \t Eval Loss:  0.026901223 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  14700 \t Train Loss:  0.02124472 \t Train Accuracy:  99.38%\nStep:  14700 \t Eval Loss:  0.027713124 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  14800 \t Train Loss:  0.025331004 \t Train Accuracy:  99.26%\nStep:  14800 \t Eval Loss:  0.025380645 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  14900 \t Train Loss:  0.029256228 \t Train Accuracy:  99.22%\nStep:  14900 \t Eval Loss:  0.03425417 \t Eval Accuracy:  99.07%\n##########################################################\nStep:  15000 \t Train Loss:  0.028616909 \t Train Accuracy:  99.18%\nStep:  15000 \t Eval Loss:  0.026469868 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  15100 \t Train Loss:  0.024588343 \t Train Accuracy:  99.34%\nStep:  15100 \t Eval Loss:  0.028541053 \t Eval Accuracy:  99.12%\n##########################################################\nStep:  15200 \t Train Loss:  0.026166257 \t Train Accuracy:  99.30%\nStep:  15200 \t Eval Loss:  0.03040728 \t Eval Accuracy:  99.15%\n##########################################################\nStep:  15300 \t Train Loss:  0.024273396 \t Train Accuracy:  99.29%\nStep:  15300 \t Eval Loss:  0.028708458 \t Eval Accuracy:  99.13%\n##########################################################\nStep:  15400 \t Train Loss:  0.023730695 \t Train Accuracy:  99.32%\nStep:  15400 \t Eval Loss:  0.023982156 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  15500 \t Train Loss:  0.03836355 \t Train Accuracy:  98.90%\nStep:  15500 \t Eval Loss:  0.032903876 \t Eval Accuracy:  99.05%\n##########################################################\nStep:  15600 \t Train Loss:  0.021563753 \t Train Accuracy:  99.35%\nStep:  15600 \t Eval Loss:  0.02483989 \t Eval Accuracy:  99.13%\n##########################################################\nStep:  15700 \t Train Loss:  0.023757175 \t Train Accuracy:  99.26%\nStep:  15700 \t Eval Loss:  0.03068723 \t Eval Accuracy:  99.12%\n##########################################################\nStep:  15800 \t Train Loss:  0.024025574 \t Train Accuracy:  99.23%\nStep:  15800 \t Eval Loss:  0.0244375 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  15900 \t Train Loss:  0.021619808 \t Train Accuracy:  99.24%\nStep:  15900 \t Eval Loss:  0.025045894 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  16000 \t Train Loss:  0.024853632 \t Train Accuracy:  99.23%\nStep:  16000 \t Eval Loss:  0.027813192 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  16100 \t Train Loss:  0.018880345 \t Train Accuracy:  99.45%\nStep:  16100 \t Eval Loss:  0.026771385 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  16200 \t Train Loss:  0.025839744 \t Train Accuracy:  99.15%\nStep:  16200 \t Eval Loss:  0.026718687 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  16300 \t Train Loss:  0.034852803 \t Train Accuracy:  98.90%\nStep:  16300 \t Eval Loss:  0.031205582 \t Eval Accuracy:  99.08%\n##########################################################\nStep:  16400 \t Train Loss:  0.026661236 \t Train Accuracy:  99.23%\nStep:  16400 \t Eval Loss:  0.029573377 \t Eval Accuracy:  99.13%\n##########################################################\nStep:  16500 \t Train Loss:  0.022327907 \t Train Accuracy:  99.23%\nStep:  16500 \t Eval Loss:  0.02711419 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  16600 \t Train Loss:  0.024587635 \t Train Accuracy:  99.27%\nStep:  16600 \t Eval Loss:  0.024350798 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  16700 \t Train Loss:  0.021954447 \t Train Accuracy:  99.35%\nStep:  16700 \t Eval Loss:  0.02551394 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  16800 \t Train Loss:  0.025406074 \t Train Accuracy:  99.24%\nStep:  16800 \t Eval Loss:  0.02077856 \t Eval Accuracy:  99.40%\n##########################################################\nStep:  16900 \t Train Loss:  0.025397997 \t Train Accuracy:  99.22%\nStep:  16900 \t Eval Loss:  0.027304577 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  17000 \t Train Loss:  0.03785001 \t Train Accuracy:  98.95%\nStep:  17000 \t Eval Loss:  0.041296676 \t Eval Accuracy:  98.69%\n##########################################################\nStep:  17100 \t Train Loss:  0.033793636 \t Train Accuracy:  98.95%\nStep:  17100 \t Eval Loss:  0.03451781 \t Eval Accuracy:  98.99%\n##########################################################\nStep:  17200 \t Train Loss:  0.025328282 \t Train Accuracy:  99.21%\nStep:  17200 \t Eval Loss:  0.026194062 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  17300 \t Train Loss:  0.025137607 \t Train Accuracy:  99.23%\nStep:  17300 \t Eval Loss:  0.02962332 \t Eval Accuracy:  99.16%\n##########################################################\nStep:  17400 \t Train Loss:  0.02509426 \t Train Accuracy:  99.23%\nStep:  17400 \t Eval Loss:  0.021498363 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  17500 \t Train Loss:  0.02106107 \t Train Accuracy:  99.32%\nStep:  17500 \t Eval Loss:  0.02869236 \t Eval Accuracy:  99.17%\n##########################################################\nStep:  17600 \t Train Loss:  0.023204412 \t Train Accuracy:  99.28%\nStep:  17600 \t Eval Loss:  0.023737064 \t Eval Accuracy:  99.30%\n##########################################################\nStep:  17700 \t Train Loss:  0.02364598 \t Train Accuracy:  99.21%\nStep:  17700 \t Eval Loss:  0.026704777 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  17800 \t Train Loss:  0.019749932 \t Train Accuracy:  99.43%\nStep:  17800 \t Eval Loss:  0.025478642 \t Eval Accuracy:  99.37%\n##########################################################\nStep:  17900 \t Train Loss:  0.023243655 \t Train Accuracy:  99.28%\nStep:  17900 \t Eval Loss:  0.0246055 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  18000 \t Train Loss:  0.023949703 \t Train Accuracy:  99.24%\nStep:  18000 \t Eval Loss:  0.022075487 \t Eval Accuracy:  99.38%\n##########################################################\nStep:  18100 \t Train Loss:  0.024356497 \t Train Accuracy:  99.27%\nStep:  18100 \t Eval Loss:  0.021926694 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  18200 \t Train Loss:  0.020900723 \t Train Accuracy:  99.28%\nStep:  18200 \t Eval Loss:  0.023811597 \t Eval Accuracy:  99.33%\n##########################################################\nStep:  18300 \t Train Loss:  0.025312517 \t Train Accuracy:  99.32%\nStep:  18300 \t Eval Loss:  0.024739489 \t Eval Accuracy:  99.41%\n##########################################################\nStep:  18400 \t Train Loss:  0.022854395 \t Train Accuracy:  99.35%\nStep:  18400 \t Eval Loss:  0.02615225 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  18500 \t Train Loss:  0.025903214 \t Train Accuracy:  99.26%\nStep:  18500 \t Eval Loss:  0.028892778 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  18600 \t Train Loss:  0.023312485 \t Train Accuracy:  99.32%\nStep:  18600 \t Eval Loss:  0.02947725 \t Eval Accuracy:  99.17%\n##########################################################\nStep:  18700 \t Train Loss:  0.030163575 \t Train Accuracy:  99.10%\nStep:  18700 \t Eval Loss:  0.030081453 \t Eval Accuracy:  99.12%\n##########################################################\nStep:  18800 \t Train Loss:  0.025998529 \t Train Accuracy:  99.17%\nStep:  18800 \t Eval Loss:  0.029038634 \t Eval Accuracy:  99.13%\n##########################################################\nStep:  18900 \t Train Loss:  0.025212292 \t Train Accuracy:  99.33%\nStep:  18900 \t Eval Loss:  0.027092831 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  19000 \t Train Loss:  0.023689384 \t Train Accuracy:  99.27%\nStep:  19000 \t Eval Loss:  0.02716576 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  19100 \t Train Loss:  0.0233394 \t Train Accuracy:  99.32%\nStep:  19100 \t Eval Loss:  0.021989863 \t Eval Accuracy:  99.34%\n##########################################################\nStep:  19200 \t Train Loss:  0.022922091 \t Train Accuracy:  99.26%\nStep:  19200 \t Eval Loss:  0.027912207 \t Eval Accuracy:  99.13%\n##########################################################\nStep:  19300 \t Train Loss:  0.026301458 \t Train Accuracy:  99.23%\nStep:  19300 \t Eval Loss:  0.030721206 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  19400 \t Train Loss:  0.023237787 \t Train Accuracy:  99.30%\nStep:  19400 \t Eval Loss:  0.026655551 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  19500 \t Train Loss:  0.022351623 \t Train Accuracy:  99.27%\nStep:  19500 \t Eval Loss:  0.025718115 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  19600 \t Train Loss:  0.022358043 \t Train Accuracy:  99.27%\nStep:  19600 \t Eval Loss:  0.029006481 \t Eval Accuracy:  99.11%\n##########################################################\nStep:  19700 \t Train Loss:  0.026992964 \t Train Accuracy:  99.27%\nStep:  19700 \t Eval Loss:  0.021210877 \t Eval Accuracy:  99.38%\n##########################################################\nStep:  19800 \t Train Loss:  0.020276561 \t Train Accuracy:  99.35%\nStep:  19800 \t Eval Loss:  0.025313895 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  19900 \t Train Loss:  0.027791195 \t Train Accuracy:  99.18%\nStep:  19900 \t Eval Loss:  0.023915194 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  20000 \t Train Loss:  0.021231698 \t Train Accuracy:  99.34%\nStep:  20000 \t Eval Loss:  0.026342493 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  20100 \t Train Loss:  0.024026737 \t Train Accuracy:  99.22%\nStep:  20100 \t Eval Loss:  0.026938068 \t Eval Accuracy:  99.16%\n##########################################################\nStep:  20200 \t Train Loss:  0.021891724 \t Train Accuracy:  99.34%\nStep:  20200 \t Eval Loss:  0.023517631 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  20300 \t Train Loss:  0.026395943 \t Train Accuracy:  99.23%\nStep:  20300 \t Eval Loss:  0.025203155 \t Eval Accuracy:  99.35%\n##########################################################\nStep:  20400 \t Train Loss:  0.022035457 \t Train Accuracy:  99.35%\nStep:  20400 \t Eval Loss:  0.02304435 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  20500 \t Train Loss:  0.028463861 \t Train Accuracy:  99.13%\nStep:  20500 \t Eval Loss:  0.027992597 \t Eval Accuracy:  99.17%\n##########################################################\nStep:  20600 \t Train Loss:  0.024214992 \t Train Accuracy:  99.30%\nStep:  20600 \t Eval Loss:  0.02626041 \t Eval Accuracy:  99.06%\n##########################################################\nStep:  20700 \t Train Loss:  0.025402782 \t Train Accuracy:  99.13%\nStep:  20700 \t Eval Loss:  0.03192766 \t Eval Accuracy:  99.10%\n##########################################################\nStep:  20800 \t Train Loss:  0.026137292 \t Train Accuracy:  99.19%\nStep:  20800 \t Eval Loss:  0.02663153 \t Eval Accuracy:  99.34%\n##########################################################\nStep:  20900 \t Train Loss:  0.027641904 \t Train Accuracy:  99.21%\nStep:  20900 \t Eval Loss:  0.025789466 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  21000 \t Train Loss:  0.02438699 \t Train Accuracy:  99.22%\nStep:  21000 \t Eval Loss:  0.02615371 \t Eval Accuracy:  99.16%\n##########################################################\nStep:  21100 \t Train Loss:  0.028031059 \t Train Accuracy:  99.17%\nStep:  21100 \t Eval Loss:  0.026333608 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  21200 \t Train Loss:  0.02215987 \t Train Accuracy:  99.32%\nStep:  21200 \t Eval Loss:  0.03062443 \t Eval Accuracy:  99.17%\n##########################################################\nStep:  21300 \t Train Loss:  0.024204057 \t Train Accuracy:  99.29%\nStep:  21300 \t Eval Loss:  0.027132344 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  21400 \t Train Loss:  0.022922728 \t Train Accuracy:  99.30%\nStep:  21400 \t Eval Loss:  0.026804196 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  21500 \t Train Loss:  0.02860656 \t Train Accuracy:  99.11%\nStep:  21500 \t Eval Loss:  0.02230293 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  21600 \t Train Loss:  0.021908952 \t Train Accuracy:  99.24%\nStep:  21600 \t Eval Loss:  0.025383819 \t Eval Accuracy:  99.30%\n##########################################################\nStep:  21700 \t Train Loss:  0.024799803 \t Train Accuracy:  99.21%\nStep:  21700 \t Eval Loss:  0.0238923 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  21800 \t Train Loss:  0.022718033 \t Train Accuracy:  99.27%\nStep:  21800 \t Eval Loss:  0.027161509 \t Eval Accuracy:  99.17%\n##########################################################\nStep:  21900 \t Train Loss:  0.027677806 \t Train Accuracy:  99.23%\nStep:  21900 \t Eval Loss:  0.027066503 \t Eval Accuracy:  99.11%\n##########################################################\nStep:  22000 \t Train Loss:  0.020852162 \t Train Accuracy:  99.41%\nStep:  22000 \t Eval Loss:  0.026122466 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  22100 \t Train Loss:  0.023284812 \t Train Accuracy:  99.21%\nStep:  22100 \t Eval Loss:  0.024722539 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  22200 \t Train Loss:  0.022382202 \t Train Accuracy:  99.34%\nStep:  22200 \t Eval Loss:  0.025285807 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  22300 \t Train Loss:  0.027659979 \t Train Accuracy:  99.17%\nStep:  22300 \t Eval Loss:  0.030650515 \t Eval Accuracy:  99.04%\n##########################################################\nStep:  22400 \t Train Loss:  0.022353971 \t Train Accuracy:  99.28%\nStep:  22400 \t Eval Loss:  0.028911248 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  22500 \t Train Loss:  0.02415225 \t Train Accuracy:  99.33%\nStep:  22500 \t Eval Loss:  0.02515987 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  22600 \t Train Loss:  0.024872359 \t Train Accuracy:  99.23%\nStep:  22600 \t Eval Loss:  0.023749575 \t Eval Accuracy:  99.37%\n##########################################################\nStep:  22700 \t Train Loss:  0.019504916 \t Train Accuracy:  99.32%\nStep:  22700 \t Eval Loss:  0.02462649 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  22800 \t Train Loss:  0.020254074 \t Train Accuracy:  99.26%\nStep:  22800 \t Eval Loss:  0.026279435 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  22900 \t Train Loss:  0.019307116 \t Train Accuracy:  99.38%\nStep:  22900 \t Eval Loss:  0.023411553 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  23000 \t Train Loss:  0.023368869 \t Train Accuracy:  99.22%\nStep:  23000 \t Eval Loss:  0.02495086 \t Eval Accuracy:  99.35%\n##########################################################\nStep:  23100 \t Train Loss:  0.022559868 \t Train Accuracy:  99.27%\nStep:  23100 \t Eval Loss:  0.024474783 \t Eval Accuracy:  99.34%\n##########################################################\nStep:  23200 \t Train Loss:  0.026919179 \t Train Accuracy:  99.17%\nStep:  23200 \t Eval Loss:  0.028021898 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  23300 \t Train Loss:  0.027948894 \t Train Accuracy:  99.10%\nStep:  23300 \t Eval Loss:  0.024447337 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  23400 \t Train Loss:  0.02602151 \t Train Accuracy:  99.21%\nStep:  23400 \t Eval Loss:  0.027472362 \t Eval Accuracy:  99.17%\n##########################################################\nStep:  23500 \t Train Loss:  0.021445159 \t Train Accuracy:  99.32%\nStep:  23500 \t Eval Loss:  0.02559254 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  23600 \t Train Loss:  0.024936654 \t Train Accuracy:  99.23%\nStep:  23600 \t Eval Loss:  0.02801148 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  23700 \t Train Loss:  0.023797637 \t Train Accuracy:  99.22%\nStep:  23700 \t Eval Loss:  0.027765494 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  23800 \t Train Loss:  0.021399545 \t Train Accuracy:  99.22%\nStep:  23800 \t Eval Loss:  0.02563817 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  23900 \t Train Loss:  0.023416668 \t Train Accuracy:  99.32%\nStep:  23900 \t Eval Loss:  0.022638632 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  24000 \t Train Loss:  0.023017498 \t Train Accuracy:  99.32%\nStep:  24000 \t Eval Loss:  0.025434043 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  24100 \t Train Loss:  0.025454488 \t Train Accuracy:  99.26%\nStep:  24100 \t Eval Loss:  0.025576416 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  24200 \t Train Loss:  0.024404101 \t Train Accuracy:  99.26%\nStep:  24200 \t Eval Loss:  0.027359845 \t Eval Accuracy:  99.13%\n##########################################################\nStep:  24300 \t Train Loss:  0.020688139 \t Train Accuracy:  99.37%\nStep:  24300 \t Eval Loss:  0.028770085 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  24400 \t Train Loss:  0.021916687 \t Train Accuracy:  99.38%\nStep:  24400 \t Eval Loss:  0.025704194 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  24500 \t Train Loss:  0.02754616 \t Train Accuracy:  99.15%\nStep:  24500 \t Eval Loss:  0.02868596 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  24600 \t Train Loss:  0.025500055 \t Train Accuracy:  99.23%\nStep:  24600 \t Eval Loss:  0.02339494 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  24700 \t Train Loss:  0.02104162 \t Train Accuracy:  99.38%\nStep:  24700 \t Eval Loss:  0.026982056 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  24800 \t Train Loss:  0.02280835 \t Train Accuracy:  99.30%\nStep:  24800 \t Eval Loss:  0.026983466 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  24900 \t Train Loss:  0.023341235 \t Train Accuracy:  99.30%\nStep:  24900 \t Eval Loss:  0.024394602 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  25000 \t Train Loss:  0.023200354 \t Train Accuracy:  99.28%\nStep:  25000 \t Eval Loss:  0.022819826 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  25100 \t Train Loss:  0.062056597 \t Train Accuracy:  98.47%\nStep:  25100 \t Eval Loss:  0.036798194 \t Eval Accuracy:  98.89%\n##########################################################\nStep:  25200 \t Train Loss:  0.024053847 \t Train Accuracy:  99.22%\nStep:  25200 \t Eval Loss:  0.025340337 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  25300 \t Train Loss:  0.020743944 \t Train Accuracy:  99.38%\nStep:  25300 \t Eval Loss:  0.024932945 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  25400 \t Train Loss:  0.024945494 \t Train Accuracy:  99.22%\nStep:  25400 \t Eval Loss:  0.026805675 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  25500 \t Train Loss:  0.026035868 \t Train Accuracy:  99.22%\nStep:  25500 \t Eval Loss:  0.02536564 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  25600 \t Train Loss:  0.019064888 \t Train Accuracy:  99.45%\nStep:  25600 \t Eval Loss:  0.024137236 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  25700 \t Train Loss:  0.026517866 \t Train Accuracy:  99.27%\nStep:  25700 \t Eval Loss:  0.021973707 \t Eval Accuracy:  99.35%\n##########################################################\nStep:  25800 \t Train Loss:  0.02058423 \t Train Accuracy:  99.41%\nStep:  25800 \t Eval Loss:  0.02594023 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  25900 \t Train Loss:  0.022161443 \t Train Accuracy:  99.32%\nStep:  25900 \t Eval Loss:  0.023664381 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  26000 \t Train Loss:  0.022251038 \t Train Accuracy:  99.29%\nStep:  26000 \t Eval Loss:  0.025422834 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  26100 \t Train Loss:  0.02352038 \t Train Accuracy:  99.33%\nStep:  26100 \t Eval Loss:  0.022919152 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  26200 \t Train Loss:  0.021043597 \t Train Accuracy:  99.30%\nStep:  26200 \t Eval Loss:  0.024415698 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  26300 \t Train Loss:  0.02168683 \t Train Accuracy:  99.33%\nStep:  26300 \t Eval Loss:  0.026017742 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  26400 \t Train Loss:  0.018081523 \t Train Accuracy:  99.41%\nStep:  26400 \t Eval Loss:  0.02622232 \t Eval Accuracy:  99.17%\n##########################################################\nStep:  26500 \t Train Loss:  0.025156185 \t Train Accuracy:  99.29%\nStep:  26500 \t Eval Loss:  0.028797379 \t Eval Accuracy:  99.05%\n##########################################################\nStep:  26600 \t Train Loss:  0.02192435 \t Train Accuracy:  99.32%\nStep:  26600 \t Eval Loss:  0.024908956 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  26700 \t Train Loss:  0.023153076 \t Train Accuracy:  99.30%\nStep:  26700 \t Eval Loss:  0.02740255 \t Eval Accuracy:  99.07%\n##########################################################\nStep:  26800 \t Train Loss:  0.022976942 \t Train Accuracy:  99.38%\nStep:  26800 \t Eval Loss:  0.026389072 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  26900 \t Train Loss:  0.025064927 \t Train Accuracy:  99.32%\nStep:  26900 \t Eval Loss:  0.026038738 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  27000 \t Train Loss:  0.02122803 \t Train Accuracy:  99.32%\nStep:  27000 \t Eval Loss:  0.026761107 \t Eval Accuracy:  99.30%\n##########################################################\nStep:  27100 \t Train Loss:  0.02724907 \t Train Accuracy:  99.15%\nStep:  27100 \t Eval Loss:  0.045400135 \t Eval Accuracy:  99.10%\n##########################################################\nStep:  27200 \t Train Loss:  0.023032088 \t Train Accuracy:  99.27%\nStep:  27200 \t Eval Loss:  0.029842423 \t Eval Accuracy:  99.15%\n##########################################################\nStep:  27300 \t Train Loss:  0.023610342 \t Train Accuracy:  99.32%\nStep:  27300 \t Eval Loss:  0.023789424 \t Eval Accuracy:  99.32%\n##########################################################\nStep:  27400 \t Train Loss:  0.021980498 \t Train Accuracy:  99.26%\nStep:  27400 \t Eval Loss:  0.022724885 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  27500 \t Train Loss:  0.02448719 \t Train Accuracy:  99.29%\nStep:  27500 \t Eval Loss:  0.027019002 \t Eval Accuracy:  99.12%\n##########################################################\nStep:  27600 \t Train Loss:  0.024801992 \t Train Accuracy:  99.28%\nStep:  27600 \t Eval Loss:  0.02637746 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  27700 \t Train Loss:  0.025343096 \t Train Accuracy:  99.18%\nStep:  27700 \t Eval Loss:  0.025948592 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  27800 \t Train Loss:  0.021303177 \t Train Accuracy:  99.37%\nStep:  27800 \t Eval Loss:  0.027127702 \t Eval Accuracy:  99.10%\n##########################################################\nStep:  27900 \t Train Loss:  0.021329608 \t Train Accuracy:  99.34%\nStep:  27900 \t Eval Loss:  0.022155281 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  28000 \t Train Loss:  0.025903683 \t Train Accuracy:  99.17%\nStep:  28000 \t Eval Loss:  0.028256439 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  28100 \t Train Loss:  0.01984202 \t Train Accuracy:  99.38%\nStep:  28100 \t Eval Loss:  0.023034792 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  28200 \t Train Loss:  0.021745097 \t Train Accuracy:  99.35%\nStep:  28200 \t Eval Loss:  0.025319355 \t Eval Accuracy:  99.32%\n##########################################################\nStep:  28300 \t Train Loss:  0.026308795 \t Train Accuracy:  99.19%\nStep:  28300 \t Eval Loss:  0.021901794 \t Eval Accuracy:  99.38%\n##########################################################\nStep:  28400 \t Train Loss:  0.020445956 \t Train Accuracy:  99.33%\nStep:  28400 \t Eval Loss:  0.027253097 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  28500 \t Train Loss:  0.019518467 \t Train Accuracy:  99.33%\nStep:  28500 \t Eval Loss:  0.021529645 \t Eval Accuracy:  99.39%\n##########################################################\nStep:  28600 \t Train Loss:  0.021650434 \t Train Accuracy:  99.30%\nStep:  28600 \t Eval Loss:  0.022792729 \t Eval Accuracy:  99.38%\n##########################################################\nStep:  28700 \t Train Loss:  0.020510416 \t Train Accuracy:  99.41%\nStep:  28700 \t Eval Loss:  0.024245085 \t Eval Accuracy:  99.32%\n##########################################################\nStep:  28800 \t Train Loss:  0.022515094 \t Train Accuracy:  99.28%\nStep:  28800 \t Eval Loss:  0.024749404 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  28900 \t Train Loss:  0.025520772 \t Train Accuracy:  99.27%\nStep:  28900 \t Eval Loss:  0.021107316 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  29000 \t Train Loss:  0.0219553 \t Train Accuracy:  99.40%\nStep:  29000 \t Eval Loss:  0.024282817 \t Eval Accuracy:  99.37%\n##########################################################\nStep:  29100 \t Train Loss:  0.021155396 \t Train Accuracy:  99.37%\nStep:  29100 \t Eval Loss:  0.023936624 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  29200 \t Train Loss:  0.02665738 \t Train Accuracy:  99.08%\nStep:  29200 \t Eval Loss:  0.02367799 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  29300 \t Train Loss:  0.026062004 \t Train Accuracy:  99.21%\nStep:  29300 \t Eval Loss:  0.024291428 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  29400 \t Train Loss:  0.022550028 \t Train Accuracy:  99.34%\nStep:  29400 \t Eval Loss:  0.02320756 \t Eval Accuracy:  99.33%\n##########################################################\nStep:  29500 \t Train Loss:  0.024684519 \t Train Accuracy:  99.21%\nStep:  29500 \t Eval Loss:  0.026001181 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  29600 \t Train Loss:  0.020799153 \t Train Accuracy:  99.40%\nStep:  29600 \t Eval Loss:  0.0206275 \t Eval Accuracy:  99.32%\n##########################################################\nStep:  29700 \t Train Loss:  0.02551078 \t Train Accuracy:  99.28%\nStep:  29700 \t Eval Loss:  0.023602493 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  29800 \t Train Loss:  0.021952704 \t Train Accuracy:  99.34%\nStep:  29800 \t Eval Loss:  0.03162237 \t Eval Accuracy:  99.10%\n##########################################################\nStep:  29900 \t Train Loss:  0.018290088 \t Train Accuracy:  99.37%\nStep:  29900 \t Eval Loss:  0.024714507 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  30000 \t Train Loss:  0.018834485 \t Train Accuracy:  99.32%\nStep:  30000 \t Eval Loss:  0.022954166 \t Eval Accuracy:  99.33%\n##########################################################\nStep:  30100 \t Train Loss:  0.02637256 \t Train Accuracy:  99.23%\nStep:  30100 \t Eval Loss:  0.025155764 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  30200 \t Train Loss:  0.017812707 \t Train Accuracy:  99.46%\nStep:  30200 \t Eval Loss:  0.028262343 \t Eval Accuracy:  99.16%\n##########################################################\nStep:  30300 \t Train Loss:  0.019871136 \t Train Accuracy:  99.34%\nStep:  30300 \t Eval Loss:  0.027665662 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  30400 \t Train Loss:  0.020572394 \t Train Accuracy:  99.29%\nStep:  30400 \t Eval Loss:  0.023461284 \t Eval Accuracy:  99.33%\n##########################################################\nStep:  30500 \t Train Loss:  0.023518246 \t Train Accuracy:  99.30%\nStep:  30500 \t Eval Loss:  0.025273385 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  30600 \t Train Loss:  0.021390256 \t Train Accuracy:  99.33%\nStep:  30600 \t Eval Loss:  0.02000397 \t Eval Accuracy:  99.43%\n##########################################################\nStep:  30700 \t Train Loss:  0.021228805 \t Train Accuracy:  99.30%\nStep:  30700 \t Eval Loss:  0.025163883 \t Eval Accuracy:  99.32%\n##########################################################\nStep:  30800 \t Train Loss:  0.019428106 \t Train Accuracy:  99.41%\nStep:  30800 \t Eval Loss:  0.026900336 \t Eval Accuracy:  99.16%\n##########################################################\nStep:  30900 \t Train Loss:  0.022760294 \t Train Accuracy:  99.32%\nStep:  30900 \t Eval Loss:  0.024438556 \t Eval Accuracy:  99.34%\n##########################################################\nStep:  31000 \t Train Loss:  0.022805292 \t Train Accuracy:  99.33%\nStep:  31000 \t Eval Loss:  0.022280544 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  31100 \t Train Loss:  0.021305934 \t Train Accuracy:  99.29%\nStep:  31100 \t Eval Loss:  0.022903087 \t Eval Accuracy:  99.33%\n##########################################################\nStep:  31200 \t Train Loss:  0.02424533 \t Train Accuracy:  99.17%\nStep:  31200 \t Eval Loss:  0.022568408 \t Eval Accuracy:  99.39%\n##########################################################\nStep:  31300 \t Train Loss:  0.024893995 \t Train Accuracy:  99.32%\nStep:  31300 \t Eval Loss:  0.023857348 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  31400 \t Train Loss:  0.022594742 \t Train Accuracy:  99.29%\nStep:  31400 \t Eval Loss:  0.024520285 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  31500 \t Train Loss:  0.024585526 \t Train Accuracy:  99.26%\nStep:  31500 \t Eval Loss:  0.024674214 \t Eval Accuracy:  99.17%\n##########################################################\nStep:  31600 \t Train Loss:  0.023919925 \t Train Accuracy:  99.18%\nStep:  31600 \t Eval Loss:  0.023810707 \t Eval Accuracy:  99.37%\n##########################################################\nStep:  31700 \t Train Loss:  0.02567679 \t Train Accuracy:  99.22%\nStep:  31700 \t Eval Loss:  0.026214555 \t Eval Accuracy:  99.33%\n##########################################################\nStep:  31800 \t Train Loss:  0.021927882 \t Train Accuracy:  99.35%\nStep:  31800 \t Eval Loss:  0.021837585 \t Eval Accuracy:  99.41%\n##########################################################\nStep:  31900 \t Train Loss:  0.018962737 \t Train Accuracy:  99.45%\nStep:  31900 \t Eval Loss:  0.024907695 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  32000 \t Train Loss:  0.024761584 \t Train Accuracy:  99.17%\nStep:  32000 \t Eval Loss:  0.02533232 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  32100 \t Train Loss:  0.020924296 \t Train Accuracy:  99.37%\nStep:  32100 \t Eval Loss:  0.022572558 \t Eval Accuracy:  99.34%\n##########################################################\nStep:  32200 \t Train Loss:  0.020655077 \t Train Accuracy:  99.41%\nStep:  32200 \t Eval Loss:  0.02746144 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  32300 \t Train Loss:  0.019129755 \t Train Accuracy:  99.44%\nStep:  32300 \t Eval Loss:  0.022397395 \t Eval Accuracy:  99.38%\n##########################################################\nStep:  32400 \t Train Loss:  0.021014497 \t Train Accuracy:  99.33%\nStep:  32400 \t Eval Loss:  0.024329627 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  32500 \t Train Loss:  0.03209355 \t Train Accuracy:  99.07%\nStep:  32500 \t Eval Loss:  0.025899062 \t Eval Accuracy:  99.16%\n##########################################################\nStep:  32600 \t Train Loss:  0.020517504 \t Train Accuracy:  99.35%\nStep:  32600 \t Eval Loss:  0.02356906 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  32700 \t Train Loss:  0.023343647 \t Train Accuracy:  99.22%\nStep:  32700 \t Eval Loss:  0.024058096 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  32800 \t Train Loss:  0.021407632 \t Train Accuracy:  99.30%\nStep:  32800 \t Eval Loss:  0.02444917 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  32900 \t Train Loss:  0.02364406 \t Train Accuracy:  99.16%\nStep:  32900 \t Eval Loss:  0.023331571 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  33000 \t Train Loss:  0.021333976 \t Train Accuracy:  99.30%\nStep:  33000 \t Eval Loss:  0.02659735 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  33100 \t Train Loss:  0.022979116 \t Train Accuracy:  99.32%\nStep:  33100 \t Eval Loss:  0.02542799 \t Eval Accuracy:  99.38%\n##########################################################\nStep:  33200 \t Train Loss:  0.023308128 \t Train Accuracy:  99.24%\nStep:  33200 \t Eval Loss:  0.022849131 \t Eval Accuracy:  99.40%\n##########################################################\nStep:  33300 \t Train Loss:  0.021628931 \t Train Accuracy:  99.29%\nStep:  33300 \t Eval Loss:  0.024353243 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  33400 \t Train Loss:  0.023010237 \t Train Accuracy:  99.35%\nStep:  33400 \t Eval Loss:  0.02091512 \t Eval Accuracy:  99.35%\n##########################################################\nStep:  33500 \t Train Loss:  0.020421527 \t Train Accuracy:  99.37%\nStep:  33500 \t Eval Loss:  0.026498605 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  33600 \t Train Loss:  0.020317003 \t Train Accuracy:  99.39%\nStep:  33600 \t Eval Loss:  0.02256984 \t Eval Accuracy:  99.33%\n##########################################################\nStep:  33700 \t Train Loss:  0.01959273 \t Train Accuracy:  99.45%\nStep:  33700 \t Eval Loss:  0.023034338 \t Eval Accuracy:  99.33%\n##########################################################\nStep:  33800 \t Train Loss:  0.021808296 \t Train Accuracy:  99.30%\nStep:  33800 \t Eval Loss:  0.02411294 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  33900 \t Train Loss:  0.0223278 \t Train Accuracy:  99.32%\nStep:  33900 \t Eval Loss:  0.026825545 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  34000 \t Train Loss:  0.025372736 \t Train Accuracy:  99.24%\nStep:  34000 \t Eval Loss:  0.021770515 \t Eval Accuracy:  99.34%\n##########################################################\nStep:  34100 \t Train Loss:  0.021065254 \t Train Accuracy:  99.37%\nStep:  34100 \t Eval Loss:  0.02891738 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  34200 \t Train Loss:  0.02541416 \t Train Accuracy:  99.33%\nStep:  34200 \t Eval Loss:  0.02681632 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  34300 \t Train Loss:  0.018978827 \t Train Accuracy:  99.44%\nStep:  34300 \t Eval Loss:  0.028194025 \t Eval Accuracy:  99.11%\n##########################################################\nStep:  34400 \t Train Loss:  0.020157404 \t Train Accuracy:  99.37%\nStep:  34400 \t Eval Loss:  0.021026345 \t Eval Accuracy:  99.37%\n##########################################################\nStep:  34500 \t Train Loss:  0.019343495 \t Train Accuracy:  99.37%\nStep:  34500 \t Eval Loss:  0.027266677 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  34600 \t Train Loss:  0.019203546 \t Train Accuracy:  99.41%\nStep:  34600 \t Eval Loss:  0.019840341 \t Eval Accuracy:  99.39%\n##########################################################\nStep:  34700 \t Train Loss:  0.020828648 \t Train Accuracy:  99.34%\nStep:  34700 \t Eval Loss:  0.02450864 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  34800 \t Train Loss:  0.018406194 \t Train Accuracy:  99.49%\nStep:  34800 \t Eval Loss:  0.02749769 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  34900 \t Train Loss:  0.02335369 \t Train Accuracy:  99.29%\nStep:  34900 \t Eval Loss:  0.023922812 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  35000 \t Train Loss:  0.020800203 \t Train Accuracy:  99.34%\nStep:  35000 \t Eval Loss:  0.023709739 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  35100 \t Train Loss:  0.02118192 \t Train Accuracy:  99.27%\nStep:  35100 \t Eval Loss:  0.02200493 \t Eval Accuracy:  99.40%\n##########################################################\nStep:  35200 \t Train Loss:  0.021164851 \t Train Accuracy:  99.38%\nStep:  35200 \t Eval Loss:  0.031669304 \t Eval Accuracy:  99.08%\n##########################################################\nStep:  35300 \t Train Loss:  0.021056866 \t Train Accuracy:  99.32%\nStep:  35300 \t Eval Loss:  0.025662875 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  35400 \t Train Loss:  0.02437393 \t Train Accuracy:  99.22%\nStep:  35400 \t Eval Loss:  0.023542233 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  35500 \t Train Loss:  0.023088207 \t Train Accuracy:  99.34%\nStep:  35500 \t Eval Loss:  0.022376407 \t Eval Accuracy:  99.37%\n##########################################################\nStep:  35600 \t Train Loss:  0.024510406 \t Train Accuracy:  99.24%\nStep:  35600 \t Eval Loss:  0.02376076 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  35700 \t Train Loss:  0.021941433 \t Train Accuracy:  99.29%\nStep:  35700 \t Eval Loss:  0.024806403 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  35800 \t Train Loss:  0.020793326 \t Train Accuracy:  99.35%\nStep:  35800 \t Eval Loss:  0.025567824 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  35900 \t Train Loss:  0.021244425 \t Train Accuracy:  99.39%\nStep:  35900 \t Eval Loss:  0.023232205 \t Eval Accuracy:  99.32%\n##########################################################\nStep:  36000 \t Train Loss:  0.022588402 \t Train Accuracy:  99.27%\nStep:  36000 \t Eval Loss:  0.024143482 \t Eval Accuracy:  99.32%\n##########################################################\nStep:  36100 \t Train Loss:  0.021256428 \t Train Accuracy:  99.37%\nStep:  36100 \t Eval Loss:  0.024550818 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  36200 \t Train Loss:  0.022511804 \t Train Accuracy:  99.39%\nStep:  36200 \t Eval Loss:  0.026195684 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  36300 \t Train Loss:  0.024241202 \t Train Accuracy:  99.27%\nStep:  36300 \t Eval Loss:  0.02402667 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  36400 \t Train Loss:  0.023357637 \t Train Accuracy:  99.24%\nStep:  36400 \t Eval Loss:  0.021893483 \t Eval Accuracy:  99.30%\n##########################################################\nStep:  36500 \t Train Loss:  0.022031922 \t Train Accuracy:  99.37%\nStep:  36500 \t Eval Loss:  0.02719099 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  36600 \t Train Loss:  0.023910033 \t Train Accuracy:  99.26%\nStep:  36600 \t Eval Loss:  0.023121659 \t Eval Accuracy:  99.32%\n##########################################################\nStep:  36700 \t Train Loss:  0.022549137 \t Train Accuracy:  99.33%\nStep:  36700 \t Eval Loss:  0.023563523 \t Eval Accuracy:  99.35%\n##########################################################\nStep:  36800 \t Train Loss:  0.022073828 \t Train Accuracy:  99.28%\nStep:  36800 \t Eval Loss:  0.027202642 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  36900 \t Train Loss:  0.021300796 \t Train Accuracy:  99.29%\nStep:  36900 \t Eval Loss:  0.02301843 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  37000 \t Train Loss:  0.021292351 \t Train Accuracy:  99.30%\nStep:  37000 \t Eval Loss:  0.025594238 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  37100 \t Train Loss:  0.021536186 \t Train Accuracy:  99.32%\nStep:  37100 \t Eval Loss:  0.027049629 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  37200 \t Train Loss:  0.02024142 \t Train Accuracy:  99.37%\nStep:  37200 \t Eval Loss:  0.024693854 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  37300 \t Train Loss:  0.019456912 \t Train Accuracy:  99.32%\nStep:  37300 \t Eval Loss:  0.023509456 \t Eval Accuracy:  99.33%\n##########################################################\nStep:  37400 \t Train Loss:  0.017621685 \t Train Accuracy:  99.41%\nStep:  37400 \t Eval Loss:  0.023189234 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  37500 \t Train Loss:  0.023897124 \t Train Accuracy:  99.28%\nStep:  37500 \t Eval Loss:  0.02598602 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  37600 \t Train Loss:  0.024519973 \t Train Accuracy:  99.22%\nStep:  37600 \t Eval Loss:  0.024076216 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  37700 \t Train Loss:  0.027068565 \t Train Accuracy:  99.16%\nStep:  37700 \t Eval Loss:  0.02964756 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  37800 \t Train Loss:  0.022877544 \t Train Accuracy:  99.29%\nStep:  37800 \t Eval Loss:  0.020033121 \t Eval Accuracy:  99.39%\n##########################################################\nStep:  37900 \t Train Loss:  0.022101365 \t Train Accuracy:  99.35%\nStep:  37900 \t Eval Loss:  0.027589487 \t Eval Accuracy:  99.13%\n##########################################################\nStep:  38000 \t Train Loss:  0.020204168 \t Train Accuracy:  99.40%\nStep:  38000 \t Eval Loss:  0.024138067 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  38100 \t Train Loss:  0.019712131 \t Train Accuracy:  99.38%\nStep:  38100 \t Eval Loss:  0.025731148 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  38200 \t Train Loss:  0.02223931 \t Train Accuracy:  99.30%\nStep:  38200 \t Eval Loss:  0.025936332 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  38300 \t Train Loss:  0.021022372 \t Train Accuracy:  99.30%\nStep:  38300 \t Eval Loss:  0.022664344 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  38400 \t Train Loss:  0.021564947 \t Train Accuracy:  99.35%\nStep:  38400 \t Eval Loss:  0.020624172 \t Eval Accuracy:  99.38%\n##########################################################\nStep:  38500 \t Train Loss:  0.020168908 \t Train Accuracy:  99.44%\nStep:  38500 \t Eval Loss:  0.022349307 \t Eval Accuracy:  99.32%\n##########################################################\nStep:  38600 \t Train Loss:  0.0206734 \t Train Accuracy:  99.32%\nStep:  38600 \t Eval Loss:  0.022498203 \t Eval Accuracy:  99.34%\n##########################################################\nStep:  38700 \t Train Loss:  0.018809645 \t Train Accuracy:  99.38%\nStep:  38700 \t Eval Loss:  0.024322806 \t Eval Accuracy:  99.33%\n##########################################################\nStep:  38800 \t Train Loss:  0.021122716 \t Train Accuracy:  99.39%\nStep:  38800 \t Eval Loss:  0.030107422 \t Eval Accuracy:  99.11%\n##########################################################\nStep:  38900 \t Train Loss:  0.02154123 \t Train Accuracy:  99.30%\nStep:  38900 \t Eval Loss:  0.021992382 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  39000 \t Train Loss:  0.022822186 \t Train Accuracy:  99.35%\nStep:  39000 \t Eval Loss:  0.031031035 \t Eval Accuracy:  99.12%\n##########################################################\nStep:  39100 \t Train Loss:  0.018094232 \t Train Accuracy:  99.48%\nStep:  39100 \t Eval Loss:  0.022794468 \t Eval Accuracy:  99.32%\n##########################################################\nStep:  39200 \t Train Loss:  0.024116643 \t Train Accuracy:  99.30%\nStep:  39200 \t Eval Loss:  0.027980087 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  39300 \t Train Loss:  0.023191724 \t Train Accuracy:  99.26%\nStep:  39300 \t Eval Loss:  0.025768872 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  39400 \t Train Loss:  0.024171416 \t Train Accuracy:  99.18%\nStep:  39400 \t Eval Loss:  0.025001531 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  39500 \t Train Loss:  0.020923246 \t Train Accuracy:  99.30%\nStep:  39500 \t Eval Loss:  0.022013405 \t Eval Accuracy:  99.32%\n##########################################################\nStep:  39600 \t Train Loss:  0.021676306 \t Train Accuracy:  99.40%\nStep:  39600 \t Eval Loss:  0.027585112 \t Eval Accuracy:  99.17%\n##########################################################\nStep:  39700 \t Train Loss:  0.021795694 \t Train Accuracy:  99.29%\nStep:  39700 \t Eval Loss:  0.022465501 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  39800 \t Train Loss:  0.022979517 \t Train Accuracy:  99.23%\nStep:  39800 \t Eval Loss:  0.023318322 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  39900 \t Train Loss:  0.017965697 \t Train Accuracy:  99.41%\nStep:  39900 \t Eval Loss:  0.024446616 \t Eval Accuracy:  99.30%\n##########################################################\nStep:  40000 \t Train Loss:  0.024072 \t Train Accuracy:  99.22%\nStep:  40000 \t Eval Loss:  0.024005396 \t Eval Accuracy:  99.32%\n##########################################################\nStep:  40100 \t Train Loss:  0.025076568 \t Train Accuracy:  99.30%\nStep:  40100 \t Eval Loss:  0.024908688 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  40200 \t Train Loss:  0.02127393 \t Train Accuracy:  99.34%\nStep:  40200 \t Eval Loss:  0.026527854 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  40300 \t Train Loss:  0.021770291 \t Train Accuracy:  99.29%\nStep:  40300 \t Eval Loss:  0.022414712 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  40400 \t Train Loss:  0.023564922 \t Train Accuracy:  99.29%\nStep:  40400 \t Eval Loss:  0.020218862 \t Eval Accuracy:  99.41%\n##########################################################\nStep:  40500 \t Train Loss:  0.022908479 \t Train Accuracy:  99.27%\nStep:  40500 \t Eval Loss:  0.023945382 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  40600 \t Train Loss:  0.02165625 \t Train Accuracy:  99.34%\nStep:  40600 \t Eval Loss:  0.022331553 \t Eval Accuracy:  99.35%\n##########################################################\nStep:  40700 \t Train Loss:  0.017867636 \t Train Accuracy:  99.43%\nStep:  40700 \t Eval Loss:  0.021741305 \t Eval Accuracy:  99.37%\n##########################################################\nStep:  40800 \t Train Loss:  0.021720197 \t Train Accuracy:  99.26%\nStep:  40800 \t Eval Loss:  0.019223645 \t Eval Accuracy:  99.43%\n##########################################################\nStep:  40900 \t Train Loss:  0.020324249 \t Train Accuracy:  99.37%\nStep:  40900 \t Eval Loss:  0.024045683 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  41000 \t Train Loss:  0.019825596 \t Train Accuracy:  99.48%\nStep:  41000 \t Eval Loss:  0.023700602 \t Eval Accuracy:  99.30%\n##########################################################\nStep:  41100 \t Train Loss:  0.02091227 \t Train Accuracy:  99.37%\nStep:  41100 \t Eval Loss:  0.027401391 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  41200 \t Train Loss:  0.024666544 \t Train Accuracy:  99.16%\nStep:  41200 \t Eval Loss:  0.024523044 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  41300 \t Train Loss:  0.021960879 \t Train Accuracy:  99.32%\nStep:  41300 \t Eval Loss:  0.02738277 \t Eval Accuracy:  99.16%\n##########################################################\nStep:  41400 \t Train Loss:  0.015679881 \t Train Accuracy:  99.51%\nStep:  41400 \t Eval Loss:  0.026019711 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  41500 \t Train Loss:  0.020287441 \t Train Accuracy:  99.41%\nStep:  41500 \t Eval Loss:  0.023476265 \t Eval Accuracy:  99.35%\n##########################################################\nStep:  41600 \t Train Loss:  0.02189273 \t Train Accuracy:  99.34%\nStep:  41600 \t Eval Loss:  0.021647245 \t Eval Accuracy:  99.43%\n##########################################################\nStep:  41700 \t Train Loss:  0.017209183 \t Train Accuracy:  99.48%\nStep:  41700 \t Eval Loss:  0.029747456 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  41800 \t Train Loss:  0.025078446 \t Train Accuracy:  99.27%\nStep:  41800 \t Eval Loss:  0.024680508 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  41900 \t Train Loss:  0.021954989 \t Train Accuracy:  99.38%\nStep:  41900 \t Eval Loss:  0.02258582 \t Eval Accuracy:  99.34%\n##########################################################\nStep:  42000 \t Train Loss:  0.021769438 \t Train Accuracy:  99.27%\nStep:  42000 \t Eval Loss:  0.023329362 \t Eval Accuracy:  99.32%\n##########################################################\nStep:  42100 \t Train Loss:  0.020104088 \t Train Accuracy:  99.32%\nStep:  42100 \t Eval Loss:  0.023417773 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  42200 \t Train Loss:  0.020543132 \t Train Accuracy:  99.27%\nStep:  42200 \t Eval Loss:  0.023993712 \t Eval Accuracy:  99.33%\n##########################################################\nStep:  42300 \t Train Loss:  0.022856375 \t Train Accuracy:  99.30%\nStep:  42300 \t Eval Loss:  0.02122556 \t Eval Accuracy:  99.30%\n##########################################################\nStep:  42400 \t Train Loss:  0.021926243 \t Train Accuracy:  99.26%\nStep:  42400 \t Eval Loss:  0.022676662 \t Eval Accuracy:  99.33%\n##########################################################\nStep:  42500 \t Train Loss:  0.020127129 \t Train Accuracy:  99.39%\nStep:  42500 \t Eval Loss:  0.019478258 \t Eval Accuracy:  99.38%\n##########################################################\nStep:  42600 \t Train Loss:  0.021031473 \t Train Accuracy:  99.35%\nStep:  42600 \t Eval Loss:  0.02622703 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  42700 \t Train Loss:  0.025527805 \t Train Accuracy:  99.19%\nStep:  42700 \t Eval Loss:  0.023640117 \t Eval Accuracy:  99.37%\n##########################################################\nStep:  42800 \t Train Loss:  0.019595757 \t Train Accuracy:  99.43%\nStep:  42800 \t Eval Loss:  0.024309367 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  42900 \t Train Loss:  0.025282249 \t Train Accuracy:  99.17%\nStep:  42900 \t Eval Loss:  0.020857109 \t Eval Accuracy:  99.38%\n##########################################################\nStep:  43000 \t Train Loss:  0.019683756 \t Train Accuracy:  99.34%\nStep:  43000 \t Eval Loss:  0.022877563 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  43100 \t Train Loss:  0.020287469 \t Train Accuracy:  99.41%\nStep:  43100 \t Eval Loss:  0.02256615 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  43200 \t Train Loss:  0.01847521 \t Train Accuracy:  99.39%\nStep:  43200 \t Eval Loss:  0.024629433 \t Eval Accuracy:  99.35%\n##########################################################\nStep:  43300 \t Train Loss:  0.02022527 \t Train Accuracy:  99.32%\nStep:  43300 \t Eval Loss:  0.020512413 \t Eval Accuracy:  99.33%\n##########################################################\nStep:  43400 \t Train Loss:  0.02025326 \t Train Accuracy:  99.37%\nStep:  43400 \t Eval Loss:  0.026743373 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  43500 \t Train Loss:  0.01976315 \t Train Accuracy:  99.38%\nStep:  43500 \t Eval Loss:  0.027487956 \t Eval Accuracy:  99.34%\n##########################################################\nStep:  43600 \t Train Loss:  0.01852039 \t Train Accuracy:  99.44%\nStep:  43600 \t Eval Loss:  0.021619618 \t Eval Accuracy:  99.38%\n##########################################################\nStep:  43700 \t Train Loss:  0.018432695 \t Train Accuracy:  99.38%\nStep:  43700 \t Eval Loss:  0.020068035 \t Eval Accuracy:  99.35%\n##########################################################\nStep:  43800 \t Train Loss:  0.018145956 \t Train Accuracy:  99.38%\nStep:  43800 \t Eval Loss:  0.026022308 \t Eval Accuracy:  99.16%\n##########################################################\nStep:  43900 \t Train Loss:  0.023033882 \t Train Accuracy:  99.30%\nStep:  43900 \t Eval Loss:  0.024503008 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  44000 \t Train Loss:  0.020209948 \t Train Accuracy:  99.33%\nStep:  44000 \t Eval Loss:  0.026430754 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  44100 \t Train Loss:  0.02198915 \t Train Accuracy:  99.35%\nStep:  44100 \t Eval Loss:  0.021470167 \t Eval Accuracy:  99.32%\n##########################################################\nStep:  44200 \t Train Loss:  0.02346534 \t Train Accuracy:  99.28%\nStep:  44200 \t Eval Loss:  0.030023549 \t Eval Accuracy:  99.08%\n##########################################################\nStep:  44300 \t Train Loss:  0.022157587 \t Train Accuracy:  99.29%\nStep:  44300 \t Eval Loss:  0.026952593 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  44400 \t Train Loss:  0.019810425 \t Train Accuracy:  99.40%\nStep:  44400 \t Eval Loss:  0.021764409 \t Eval Accuracy:  99.33%\n##########################################################\nStep:  44500 \t Train Loss:  0.017048 \t Train Accuracy:  99.44%\nStep:  44500 \t Eval Loss:  0.0234411 \t Eval Accuracy:  99.37%\n##########################################################\nStep:  44600 \t Train Loss:  0.02075492 \t Train Accuracy:  99.33%\nStep:  44600 \t Eval Loss:  0.022124805 \t Eval Accuracy:  99.37%\n##########################################################\nStep:  44700 \t Train Loss:  0.017971974 \t Train Accuracy:  99.48%\nStep:  44700 \t Eval Loss:  0.024413958 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  44800 \t Train Loss:  0.019922039 \t Train Accuracy:  99.38%\nStep:  44800 \t Eval Loss:  0.025208361 \t Eval Accuracy:  99.30%\n##########################################################\nStep:  44900 \t Train Loss:  0.021426765 \t Train Accuracy:  99.29%\nStep:  44900 \t Eval Loss:  0.020948056 \t Eval Accuracy:  99.35%\n##########################################################\nStep:  45000 \t Train Loss:  0.019519135 \t Train Accuracy:  99.35%\nStep:  45000 \t Eval Loss:  0.027677678 \t Eval Accuracy:  99.16%\n##########################################################\nStep:  45100 \t Train Loss:  0.023317132 \t Train Accuracy:  99.32%\nStep:  45100 \t Eval Loss:  0.025458071 \t Eval Accuracy:  99.33%\n##########################################################\nStep:  45200 \t Train Loss:  0.017873004 \t Train Accuracy:  99.45%\nStep:  45200 \t Eval Loss:  0.023726057 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  45300 \t Train Loss:  0.019481681 \t Train Accuracy:  99.43%\nStep:  45300 \t Eval Loss:  0.025414981 \t Eval Accuracy:  99.16%\n##########################################################\nStep:  45400 \t Train Loss:  0.022168089 \t Train Accuracy:  99.29%\nStep:  45400 \t Eval Loss:  0.020071799 \t Eval Accuracy:  99.37%\n##########################################################\nStep:  45500 \t Train Loss:  0.021613263 \t Train Accuracy:  99.38%\nStep:  45500 \t Eval Loss:  0.022732133 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  45600 \t Train Loss:  0.019650182 \t Train Accuracy:  99.40%\nStep:  45600 \t Eval Loss:  0.027409373 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  45700 \t Train Loss:  0.022049122 \t Train Accuracy:  99.34%\nStep:  45700 \t Eval Loss:  0.02575267 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  45800 \t Train Loss:  0.021322617 \t Train Accuracy:  99.38%\nStep:  45800 \t Eval Loss:  0.025420628 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  45900 \t Train Loss:  0.020660656 \t Train Accuracy:  99.33%\nStep:  45900 \t Eval Loss:  0.021872507 \t Eval Accuracy:  99.38%\n##########################################################\nStep:  46000 \t Train Loss:  0.019975599 \t Train Accuracy:  99.34%\nStep:  46000 \t Eval Loss:  0.024296239 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  46100 \t Train Loss:  0.020949231 \t Train Accuracy:  99.24%\nStep:  46100 \t Eval Loss:  0.023981895 \t Eval Accuracy:  99.32%\n##########################################################\nStep:  46200 \t Train Loss:  0.02363228 \t Train Accuracy:  99.29%\nStep:  46200 \t Eval Loss:  0.02290188 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  46300 \t Train Loss:  0.021899883 \t Train Accuracy:  99.34%\nStep:  46300 \t Eval Loss:  0.019749636 \t Eval Accuracy:  99.34%\n##########################################################\nStep:  46400 \t Train Loss:  0.019829445 \t Train Accuracy:  99.38%\nStep:  46400 \t Eval Loss:  0.020263376 \t Eval Accuracy:  99.39%\n##########################################################\nStep:  46500 \t Train Loss:  0.020544326 \t Train Accuracy:  99.37%\nStep:  46500 \t Eval Loss:  0.023034904 \t Eval Accuracy:  99.33%\n##########################################################\nStep:  46600 \t Train Loss:  0.022535369 \t Train Accuracy:  99.34%\nStep:  46600 \t Eval Loss:  0.02554978 \t Eval Accuracy:  99.30%\n##########################################################\nStep:  46700 \t Train Loss:  0.018826883 \t Train Accuracy:  99.45%\nStep:  46700 \t Eval Loss:  0.02304235 \t Eval Accuracy:  99.37%\n##########################################################\nStep:  46800 \t Train Loss:  0.022357628 \t Train Accuracy:  99.28%\nStep:  46800 \t Eval Loss:  0.019980973 \t Eval Accuracy:  99.44%\n##########################################################\nStep:  46900 \t Train Loss:  0.02050021 \t Train Accuracy:  99.37%\nStep:  46900 \t Eval Loss:  0.022520594 \t Eval Accuracy:  99.30%\n##########################################################\nStep:  47000 \t Train Loss:  0.020299625 \t Train Accuracy:  99.48%\nStep:  47000 \t Eval Loss:  0.023705535 \t Eval Accuracy:  99.34%\n##########################################################\nStep:  47100 \t Train Loss:  0.019224353 \t Train Accuracy:  99.41%\nStep:  47100 \t Eval Loss:  0.024129365 \t Eval Accuracy:  99.37%\n##########################################################\nStep:  47200 \t Train Loss:  0.033274926 \t Train Accuracy:  99.13%\nStep:  47200 \t Eval Loss:  0.03431053 \t Eval Accuracy:  99.01%\n##########################################################\nStep:  47300 \t Train Loss:  0.024960814 \t Train Accuracy:  99.21%\nStep:  47300 \t Eval Loss:  0.024088454 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  47400 \t Train Loss:  0.021603964 \t Train Accuracy:  99.28%\nStep:  47400 \t Eval Loss:  0.02419123 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  47500 \t Train Loss:  0.02142035 \t Train Accuracy:  99.38%\nStep:  47500 \t Eval Loss:  0.027737428 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  47600 \t Train Loss:  0.017905686 \t Train Accuracy:  99.38%\nStep:  47600 \t Eval Loss:  0.023860406 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  47700 \t Train Loss:  0.019683987 \t Train Accuracy:  99.38%\nStep:  47700 \t Eval Loss:  0.024484392 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  47800 \t Train Loss:  0.023046743 \t Train Accuracy:  99.29%\nStep:  47800 \t Eval Loss:  0.027896047 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  47900 \t Train Loss:  0.022045568 \t Train Accuracy:  99.33%\nStep:  47900 \t Eval Loss:  0.01921569 \t Eval Accuracy:  99.37%\n##########################################################\nStep:  48000 \t Train Loss:  0.01828925 \t Train Accuracy:  99.50%\nStep:  48000 \t Eval Loss:  0.025929712 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  48100 \t Train Loss:  0.020710247 \t Train Accuracy:  99.39%\nStep:  48100 \t Eval Loss:  0.029048894 \t Eval Accuracy:  99.10%\n##########################################################\nStep:  48200 \t Train Loss:  0.019934583 \t Train Accuracy:  99.34%\nStep:  48200 \t Eval Loss:  0.024350557 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  48300 \t Train Loss:  0.017288879 \t Train Accuracy:  99.43%\nStep:  48300 \t Eval Loss:  0.026287537 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  48400 \t Train Loss:  0.021280646 \t Train Accuracy:  99.33%\nStep:  48400 \t Eval Loss:  0.022052051 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  48500 \t Train Loss:  0.016930487 \t Train Accuracy:  99.46%\nStep:  48500 \t Eval Loss:  0.028339725 \t Eval Accuracy:  99.16%\n##########################################################\nStep:  48600 \t Train Loss:  0.02022351 \t Train Accuracy:  99.37%\nStep:  48600 \t Eval Loss:  0.021089321 \t Eval Accuracy:  99.34%\n##########################################################\nStep:  48700 \t Train Loss:  0.022301778 \t Train Accuracy:  99.29%\nStep:  48700 \t Eval Loss:  0.026868902 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  48800 \t Train Loss:  0.01692542 \t Train Accuracy:  99.41%\nStep:  48800 \t Eval Loss:  0.02046185 \t Eval Accuracy:  99.44%\n##########################################################\nStep:  48900 \t Train Loss:  0.018546293 \t Train Accuracy:  99.37%\nStep:  48900 \t Eval Loss:  0.025087148 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  49000 \t Train Loss:  0.020489335 \t Train Accuracy:  99.39%\nStep:  49000 \t Eval Loss:  0.022039026 \t Eval Accuracy:  99.34%\n##########################################################\nStep:  49100 \t Train Loss:  0.020768657 \t Train Accuracy:  99.34%\nStep:  49100 \t Eval Loss:  0.021472028 \t Eval Accuracy:  99.38%\n##########################################################\nStep:  49200 \t Train Loss:  0.020354562 \t Train Accuracy:  99.40%\nStep:  49200 \t Eval Loss:  0.027253032 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  49300 \t Train Loss:  0.022986569 \t Train Accuracy:  99.35%\nStep:  49300 \t Eval Loss:  0.022053005 \t Eval Accuracy:  99.33%\n##########################################################\nStep:  49400 \t Train Loss:  0.022824083 \t Train Accuracy:  99.35%\nStep:  49400 \t Eval Loss:  0.022002488 \t Eval Accuracy:  99.39%\n##########################################################\nStep:  49500 \t Train Loss:  0.018318921 \t Train Accuracy:  99.40%\nStep:  49500 \t Eval Loss:  0.023470052 \t Eval Accuracy:  99.32%\n##########################################################\nStep:  49600 \t Train Loss:  0.01950292 \t Train Accuracy:  99.34%\nStep:  49600 \t Eval Loss:  0.02382874 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  49700 \t Train Loss:  0.01900764 \t Train Accuracy:  99.40%\nStep:  49700 \t Eval Loss:  0.02674893 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  49800 \t Train Loss:  0.023617446 \t Train Accuracy:  99.16%\nStep:  49800 \t Eval Loss:  0.027348597 \t Eval Accuracy:  99.17%\n##########################################################\nStep:  49900 \t Train Loss:  0.021598937 \t Train Accuracy:  99.34%\nStep:  49900 \t Eval Loss:  0.023200445 \t Eval Accuracy:  99.30%\nCPU times: user 1h 32min 59s, sys: 1h 11min 36s, total: 2h 44min 36s\nWall time: 2h 44min 23s\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt  # Visualization\n\n# Plot loss and accuracy in subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\nax1.set_title('Loss')\nax2.set_title('Accuracy')\n\n\n\nax1.plot(all_train_losses, label='train_loss')\nax1.plot(all_eval_losses, label='eval_loss')\n\nax2.plot(all_train_accuracy, label='train_accuracy')\nax2.plot(all_test_accuracy, label='eval_accuracy')\n\nax1.legend()\nax2.legend()\nplt.show()\nplt.clf()","metadata":{"execution":{"iopub.status.busy":"2024-07-15T12:10:48.541839Z","iopub.execute_input":"2024-07-15T12:10:48.542149Z","iopub.status.idle":"2024-07-15T12:10:49.079527Z","shell.execute_reply.started":"2024-07-15T12:10:48.542123Z","shell.execute_reply":"2024-07-15T12:10:49.078659Z"},"trusted":true},"execution_count":39,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1500x500 with 2 Axes>","image/png":"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"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 0 Axes>"},"metadata":{}}]},{"cell_type":"code","source":"@partial(jax.jit, static_argnames=(\"length\"))\ndef generate_text(rng, params, var_params, length):\n    def _scan_generate(carry, _):\n        random_key, context = carry\n        logits = model.apply({'params': params, **var_params}, context, training=False, mutable=['other_variables'])[0]\n        rng, rng_subkey = jax.random.split(random_key)\n        new_token = jax.random.categorical(\n          rng_subkey, logits[:, -n_tokens, :], axis=-1, shape=(1, 1)\n        )\n        context = jnp.concatenate([context[:, 1:], new_token], axis=1)\n        print(context.shape)\n        return (rng, context), new_token\n\n    _, new_tokens = jax.lax.scan(\n    _scan_generate,\n    (rng, jnp.expand_dims(test_data[852:852+block_size], axis=0)),\n    (),\n    length=length,\n    )\n    return new_tokens","metadata":{"execution":{"iopub.status.busy":"2024-07-15T12:34:55.661252Z","iopub.execute_input":"2024-07-15T12:34:55.661575Z","iopub.status.idle":"2024-07-15T12:34:55.670260Z","shell.execute_reply.started":"2024-07-15T12:34:55.661548Z","shell.execute_reply":"2024-07-15T12:34:55.669264Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"test_data[852:852+block_size]","metadata":{"execution":{"iopub.status.busy":"2024-07-15T12:10:49.090439Z","iopub.execute_input":"2024-07-15T12:10:49.090747Z","iopub.status.idle":"2024-07-15T12:10:49.381177Z","shell.execute_reply.started":"2024-07-15T12:10:49.090724Z","shell.execute_reply":"2024-07-15T12:10:49.380193Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"Array([51,  6,  1, 53, 56,  1, 43, 50, 57, 43,  1, 63, 53, 59,  1, 42, 53,\n        1, 51, 43,  1, 61, 56, 53, 52, 45, 10,  0, 20, 47, 57,  1, 52, 39,\n       51, 43,  1, 47, 57,  1, 24, 47, 41, 47, 53,  6,  1, 40, 53, 56, 52,\n        1, 47, 52,  1, 25, 39, 52, 58, 59, 39,  8,  0,  0], dtype=int32)"},"metadata":{}}]},{"cell_type":"code","source":"i = 852\ndecode(test_data[i:i+block_size].tolist())","metadata":{"execution":{"iopub.status.busy":"2024-07-15T12:10:49.382310Z","iopub.execute_input":"2024-07-15T12:10:49.382592Z","iopub.status.idle":"2024-07-15T12:10:49.389989Z","shell.execute_reply.started":"2024-07-15T12:10:49.382566Z","shell.execute_reply":"2024-07-15T12:10:49.389066Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"'m, or else you do me wrong:\\nHis name is Licio, born in Mantua.\\n\\n'"},"metadata":{}}]},{"cell_type":"code","source":"new_tokenz = 1000\nkey, subkey = jax.random.split(jax.random.PRNGKey(156))\n# key, subkey = jax.random.split(key)\n# token_gen = generate_text(jnp.zeros((1,block_size)).astype(jnp.int32), new_tokenz, {'params': state.params})\ntoken_gen = generate_text(key, params, var_params, new_tokenz)[:, 0, 0].tolist()\nprint(token_gen)\nprint('\\n')\nprint(decode(token_gen))","metadata":{"execution":{"iopub.status.busy":"2024-07-15T12:34:59.136640Z","iopub.execute_input":"2024-07-15T12:34:59.137472Z","iopub.status.idle":"2024-07-15T12:35:14.297844Z","shell.execute_reply.started":"2024-07-15T12:34:59.137438Z","shell.execute_reply":"2024-07-15T12:35:14.296898Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"(1, 64)\n[24, 33, 15, 17, 26, 32, 21, 27, 10, 0, 32, 53, 1, 52, 53, 52, 43, 1, 51, 53, 56, 43, 1, 50, 47, 60, 43, 57, 1, 41, 53, 52, 42, 43, 51, 52, 5, 42, 1, 21, 56, 43, 50, 39, 52, 42, 1, 39, 57, 1, 58, 46, 43, 1, 58, 59, 44, 58, 0, 25, 53, 56, 43, 10, 1, 40, 59, 58, 1, 51, 63, 1, 44, 53, 50, 50, 53, 61, 57, 1, 53, 44, 58, 43, 52, 1, 52, 47, 45, 46, 58, 10, 1, 40, 39, 58, 58, 50, 43, 1, 39, 57, 1, 50, 47, 49, 43, 1, 58, 46, 53, 59, 1, 42, 53, 57, 58, 1, 58, 46, 53, 59, 1, 52, 53, 1, 46, 53, 54, 43, 1, 43, 55, 59, 43, 43, 52, 1, 47, 52, 58, 43, 56, 54, 53, 57, 43, 42, 8, 0, 31, 54, 41, 49, 47, 52, 45, 1, 39, 52, 42, 1, 60, 39, 52, 58, 39, 45, 43, 1, 54, 47, 58, 63, 1, 46, 39, 42, 57, 58, 1, 52, 53, 58, 43, 1, 47, 52, 1, 40, 43, 39, 56, 1, 58, 46, 43, 1, 61, 53, 56, 50, 42, 1, 42, 56, 39, 61, 1, 51, 63, 1, 39, 44, 44, 47, 41, 43, 56, 8, 0, 0, 16, 33, 23, 17, 1, 34, 21, 26, 15, 17, 26, 32, 21, 27, 10, 0, 21, 1, 54, 56, 39, 63, 6, 1, 58, 46, 53, 59, 1, 41, 53, 51, 51, 53, 52, 1, 59, 52, 42, 53, 61, 1, 30, 53, 51, 43, 1, 46, 43, 56, 1, 58, 46, 43, 1, 57, 39, 56, 56, 47, 53, 56, 57, 6, 1, 39, 58, 1, 21, 1, 45, 53, 58, 1, 47, 52, 1, 58, 46, 63, 1, 50, 53, 56, 42, 8, 0, 0, 37, 27, 30, 23, 10, 0, 35, 53, 59, 50, 42, 1, 58, 46, 43, 1, 49, 47, 52, 45, 1, 51, 39, 52, 63, 1, 50, 53, 53, 49, 1, 58, 53, 1, 44, 39, 50, 50, 5, 52, 1, 39, 52, 42, 1, 40, 43, 50, 42, 0, 32, 46, 39, 58, 1, 47, 57, 1, 58, 46, 43, 1, 46, 53, 50, 63, 1, 40, 43, 41, 39, 58, 43, 1, 53, 59, 56, 1, 56, 53, 63, 39, 50, 1, 58, 46, 56, 43, 43, 6, 0, 32, 46, 39, 58, 1, 58, 46, 43, 51, 1, 58, 46, 43, 51, 1, 44, 53, 56, 1, 56, 47, 45, 46, 58, 6, 1, 57, 53, 51, 43, 1, 51, 53, 52, 58, 46, 8, 1, 24, 43, 58, 1, 51, 43, 1, 49, 47, 52, 42, 0, 21, 58, 1, 44, 53, 56, 1, 52, 39, 58, 59, 56, 43, 6, 1, 57, 46, 53, 59, 50, 42, 0, 52, 43, 60, 43, 56, 1, 44, 56, 53, 51, 1, 47, 57, 1, 39, 1, 50, 59, 40, 8, 0, 16, 39, 57, 46, 1, 51, 43, 6, 1, 57, 50, 47, 50, 43, 11, 1, 46, 43, 56, 43, 1, 57, 50, 53, 54, 57, 6, 1, 61, 43, 5, 50, 50, 1, 58, 43, 50, 50, 1, 51, 43, 6, 1, 58, 46, 39, 52, 1, 50, 43, 58, 58, 43, 56, 57, 6, 1, 39, 52, 42, 1, 58, 46, 43, 1, 53, 58, 46, 43, 56, 6, 1, 27, 1, 63, 53, 59, 58, 46, 6, 1, 58, 46, 43, 1, 57, 53, 59, 52, 42, 1, 52, 53, 61, 6, 1, 47, 58, 1, 57, 46, 39, 50, 50, 1, 21, 1, 56, 59, 52, 1, 39, 50, 56, 43, 39, 42, 63, 1, 57, 47, 56, 6, 0, 32, 46, 43, 1, 39, 40, 57, 43, 52, 58, 1, 57, 59, 52, 1, 21, 1, 61, 47, 58, 46, 1, 58, 46, 43, 1, 58, 56, 39, 47, 52, 57, 10, 1, 46, 53, 61, 1, 63, 53, 59, 1, 57, 46, 53, 59, 50, 42, 43, 57, 58, 1, 51, 53, 56, 43, 1, 57, 46, 53, 59, 50, 42, 1, 54, 50, 43, 39, 57, 47, 52, 45, 0, 40, 43, 52, 58, 1, 58, 53, 50, 42, 1, 51, 43, 43, 58, 1, 61, 46, 47, 41, 46, 1, 45, 53, 50, 42, 1, 63, 53, 59, 56, 1, 54, 53, 61, 43, 56, 8, 1, 20, 43, 56, 43, 1, 41, 53, 51, 51, 53, 52, 6, 0, 52, 53, 58, 1, 46, 39, 60, 43, 1, 28, 53, 50, 47, 62, 43, 52, 43, 42, 6, 1, 51, 39, 44, 58, 43, 56, 1, 47, 57, 1, 57, 54, 39, 49, 43, 6, 1, 17, 62, 43, 58, 43, 56, 8, 0, 0, 21, 31, 13, 14, 17, 24, 24, 13, 10, 0, 14, 59, 58, 6, 1, 51, 53, 56, 43, 1, 50, 47, 41, 49, 1, 58, 46, 47, 52, 45, 1, 58, 53, 1, 58, 46, 47, 52, 49, 57, 1, 46, 43, 8, 0, 0, 18, 24, 27, 30, 21, 38, 17, 24, 10, 0, 25, 59, 57, 58, 1, 47, 58, 1, 54, 53, 53, 56, 1, 32, 47, 58, 59, 57, 6, 1, 58, 46, 53, 59, 1, 39, 50, 50, 1, 58, 46, 43, 1, 50, 39, 61, 1, 58, 46, 39, 58, 0, 35, 46, 59, 56, 41, 46, 6, 1, 21, 1, 42, 53, 1, 58, 46, 43, 1, 43, 52, 41, 53, 59, 56, 57, 43, 1, 40, 43, 58, 58, 43, 56, 1, 40, 43, 58, 58, 43, 56, 1, 61, 43, 50, 50, 8, 0, 0, 15, 24, 13, 33, 16, 21, 27, 10, 0, 25, 39, 57, 58, 43, 56, 1, 32, 63, 40, 39, 50, 58, 6, 1, 51, 53, 57, 58, 1, 58, 43, 39, 41, 46, 1, 47, 57, 1, 51, 53, 56, 43, 2, 1, 32, 46, 53, 59, 1, 52, 53, 58, 1, 40, 43, 1, 44, 50, 39, 58, 58, 43, 56, 5, 57, 1, 42]\n\n\nLUCENTIO:\nTo none more lives condemn'd Ireland as the tuft\nMore: but my follows often night: battle as like thou dost thou no hope equeen interposed.\nSpcking and vantage pity hadst note in bear the world draw my afficer.\n\nDUKE VINCENTIO:\nI pray, thou common undow Rome her the sarriors, at I got in thy lord.\n\nYORK:\nWould the king many look to fall'n and beld\nThat is the holy becate our royal three,\nThat them them for right, some month. Let me kind\nIt for nature, should\nnever from is a lub.\nDash me, slile; here slops, we'll tell me, than letters, and the other, O youth, the sound now, it shall I run already sir,\nThe absent sun I with the trains: how you shouldest more should pleasing\nbent told meet which gold your power. Here common,\nnot have Polixened, mafter is spake, Exeter.\n\nISABELLA:\nBut, more lick thing to thinks he.\n\nFLORIZEL:\nMust it poor Titus, thou all the law that\nWhurch, I do the encourse better better well.\n\nCLAUDIO:\nMaster Tybalt, most teach is more! Thou not be flatter's d\n","output_type":"stream"}]},{"cell_type":"code","source":"dsfsdhfgjdg hfdgjdgjgfjhs'####################","metadata":{"execution":{"iopub.status.busy":"2024-07-15T12:10:52.739552Z","iopub.status.idle":"2024-07-15T12:10:52.739857Z","shell.execute_reply.started":"2024-07-15T12:10:52.739703Z","shell.execute_reply":"2024-07-15T12:10:52.739716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(token_gen)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T12:10:52.741406Z","iopub.status.idle":"2024-07-15T12:10:52.741715Z","shell.execute_reply.started":"2024-07-15T12:10:52.741564Z","shell.execute_reply":"2024-07-15T12:10:52.741577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx = 882\ntokenizer.decode(test_data[idx:idx+32])","metadata":{"execution":{"iopub.status.busy":"2024-07-15T12:10:52.742766Z","iopub.status.idle":"2024-07-15T12:10:52.743102Z","shell.execute_reply.started":"2024-07-15T12:10:52.742932Z","shell.execute_reply":"2024-07-15T12:10:52.742951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer('bestopleled', return_tensors='np')","metadata":{"execution":{"iopub.status.busy":"2024-07-15T12:10:52.744573Z","iopub.status.idle":"2024-07-15T12:10:52.744868Z","shell.execute_reply.started":"2024-07-15T12:10:52.744719Z","shell.execute_reply":"2024-07-15T12:10:52.744731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.decode([1991])","metadata":{"execution":{"iopub.status.busy":"2024-07-15T12:10:52.745916Z","iopub.status.idle":"2024-07-15T12:10:52.746289Z","shell.execute_reply.started":"2024-07-15T12:10:52.746119Z","shell.execute_reply":"2024-07-15T12:10:52.746134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params['Dense_12']['kernel'].shape","metadata":{"execution":{"iopub.status.busy":"2024-07-15T12:10:52.748215Z","iopub.status.idle":"2024-07-15T12:10:52.748652Z","shell.execute_reply.started":"2024-07-15T12:10:52.748428Z","shell.execute_reply":"2024-07-15T12:10:52.748448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rngk = jax.random.PRNGKey(389)\nxs, ys = get_batch(rngk, train_data)\nprint(xs[0])\nprint(ys[0])","metadata":{"execution":{"iopub.status.busy":"2024-07-15T12:10:52.750213Z","iopub.status.idle":"2024-07-15T12:10:52.750665Z","shell.execute_reply.started":"2024-07-15T12:10:52.750424Z","shell.execute_reply":"2024-07-15T12:10:52.750442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logits = model.apply({'params': params, **var_params}, xs[0].reshape((1,64)), training=False, mutable=['other_variables'])[0]\nrng, rng_subkey = jax.random.split(rngk)\nfor pso in range(n_tokens):\n    new_token = jax.random.categorical(\n      rng_subkey, logits[:, -1*(n_tokens-pso), :], axis=-1, shape=(1, 1)\n    )\n    print(new_token)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T12:10:52.751653Z","iopub.status.idle":"2024-07-15T12:10:52.752088Z","shell.execute_reply.started":"2024-07-15T12:10:52.751853Z","shell.execute_reply":"2024-07-15T12:10:52.751871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_tok = [51,49,46,46,46,52]\nprint(decode(ys[0].tolist()))","metadata":{"execution":{"iopub.status.busy":"2024-07-15T12:10:52.753728Z","iopub.status.idle":"2024-07-15T12:10:52.754062Z","shell.execute_reply.started":"2024-07-15T12:10:52.753879Z","shell.execute_reply":"2024-07-15T12:10:52.753892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"act_tk = [60, 43, 50, 57,  1, 47]\nprint(decode(act_tk))","metadata":{"execution":{"iopub.status.busy":"2024-07-15T12:10:52.754948Z","iopub.status.idle":"2024-07-15T12:10:52.755271Z","shell.execute_reply.started":"2024-07-15T12:10:52.755114Z","shell.execute_reply":"2024-07-15T12:10:52.755128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jax.nn.standardize(jnp.array([2.0,3.0,4.0]))","metadata":{"id":"Oe_GIDP2HFyt","outputId":"5d3dce16-fcc2-40b9-c49a-00a8c4013ca2","execution":{"iopub.status.busy":"2024-07-15T12:10:52.756753Z","iopub.status.idle":"2024-07-15T12:10:52.757085Z","shell.execute_reply.started":"2024-07-15T12:10:52.756899Z","shell.execute_reply":"2024-07-15T12:10:52.756912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@struct.dataclass\nclass Metrics(metrics.Collection):\n    accuracy: metrics.Accuracy\n    loss: metrics.Average.from_output('loss')","metadata":{"id":"s3nN1jOiHFyu","execution":{"iopub.status.busy":"2024-07-15T12:10:52.758819Z","iopub.status.idle":"2024-07-15T12:10:52.759152Z","shell.execute_reply.started":"2024-07-15T12:10:52.758994Z","shell.execute_reply":"2024-07-15T12:10:52.759007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainState(train_state.TrainState):\n    metrics: Metrics\n\ndef create_train_state(module, rng, learning_rate, train_shape):\n    \"\"\"Creates an initial `TrainState`.\"\"\"\n    params = module.init(rng, jnp.ones(train_shape).astype(jnp.int32), \n                         training=False)['params'] # initialize parameters by passing a template image\n    tx = optax.adamw(learning_rate)\n    return TrainState.create(\n      apply_fn=module.apply, params=params, tx=tx,\n      metrics=Metrics.empty(),\n    )","metadata":{"id":"7LLDTSFQHFyu","execution":{"iopub.status.busy":"2024-07-15T12:10:52.760054Z","iopub.status.idle":"2024-07-15T12:10:52.760377Z","shell.execute_reply.started":"2024-07-15T12:10:52.760217Z","shell.execute_reply":"2024-07-15T12:10:52.760230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TrainState.create(","metadata":{"execution":{"iopub.status.busy":"2024-07-15T12:10:52.761257Z","iopub.status.idle":"2024-07-15T12:10:52.761560Z","shell.execute_reply.started":"2024-07-15T12:10:52.761404Z","shell.execute_reply":"2024-07-15T12:10:52.761416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@jax.jit\ndef train_step(state, inputs, targets):\n    \"\"\"Train for a single step.\"\"\"\n    def loss_fn(params):\n        logits = state.apply_fn({'params': params}, inputs, training=True, \n                                rngs={\"dropout\": key})[0]\n        loss = optax.softmax_cross_entropy_with_integer_labels(\n            logits=logits, labels=targets).mean()\n        return loss\n    grad_fn = jax.grad(loss_fn)\n    grads = grad_fn(state.params)\n    state = state.apply_gradients(grads=grads)\n    return state","metadata":{"id":"zApWXUDaHFyu","execution":{"iopub.status.busy":"2024-07-15T12:10:52.762679Z","iopub.status.idle":"2024-07-15T12:10:52.763032Z","shell.execute_reply.started":"2024-07-15T12:10:52.762843Z","shell.execute_reply":"2024-07-15T12:10:52.762857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@jax.jit\ndef compute_metrics(*, state, inputs, targets):\n    logits = state.apply_fn({'params': state.params}, inputs, training=False)[0]\n    loss = optax.softmax_cross_entropy_with_integer_labels(\n        logits=logits, labels=targets).mean()\n    metric_updates = state.metrics.single_from_model_output(\n    logits=logits, labels=targets, loss=loss)\n    metrics = state.metrics.merge(metric_updates)\n    state = state.replace(metrics=metrics)\n    return state","metadata":{"id":"VzukZ4iEHFyv","execution":{"iopub.status.busy":"2024-07-15T12:10:52.764490Z","iopub.status.idle":"2024-07-15T12:10:52.764817Z","shell.execute_reply.started":"2024-07-15T12:10:52.764660Z","shell.execute_reply":"2024-07-15T12:10:52.764674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 10\nlearning_rate = 0.005\ninit_rng = jax.random.key(0)","metadata":{"id":"ehYvMeuNHFyv","execution":{"iopub.status.busy":"2024-07-15T12:10:52.765772Z","iopub.status.idle":"2024-07-15T12:10:52.766112Z","shell.execute_reply.started":"2024-07-15T12:10:52.765948Z","shell.execute_reply":"2024-07-15T12:10:52.765965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"state = create_train_state(fin_model, init_rng, learning_rate, train_shape)\ndel init_rng  # Must not be used anymore.","metadata":{"id":"D60UHLFHHFyv","execution":{"iopub.status.busy":"2024-07-15T12:10:52.767188Z","iopub.status.idle":"2024-07-15T12:10:52.767509Z","shell.execute_reply.started":"2024-07-15T12:10:52.767354Z","shell.execute_reply":"2024-07-15T12:10:52.767367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics_history = {'train_loss': [],\n                   'train_accuracy': [],\n                   'test_loss': [],\n                   'test_accuracy': []}","metadata":{"id":"Jl-9TlHEHFyv","execution":{"iopub.status.busy":"2024-07-15T12:10:52.769334Z","iopub.status.idle":"2024-07-15T12:10:52.769665Z","shell.execute_reply.started":"2024-07-15T12:10:52.769501Z","shell.execute_reply":"2024-07-15T12:10:52.769516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 442\nkey = jax.random.PRNGKey(SEED)\nloss = 10\ncounter = 0\n# for step in tqdm(range(max_iters)): # increase number of steps for good results...\nwhile counter==max_iters or loss > 1.0:\n\n      # sample a batch of data\n    xb, yb = get_batch(key, train_data)\n    state = train_step(state, xb, yb)\n    state = compute_metrics(state=state, inputs=xb, targets=yb)\n\n    key = (jax.random.split(key)[0])\n\n    if step == 0 or (step+1) % 100 == 0: # one training epoch has passed\n        for metric,value in state.metrics.compute().items(): # compute metrics\n            metrics_history[f'train_{metric}'].append(value) # record metrics\n        state = state.replace(metrics=state.metrics.empty()) # reset train_metrics for next training epoch\n\n        # Compute metrics on the test set after each training epoch\n        test_state = state\n        x_test, y_test = get_batch(key, test_data)\n    #     for test_batch in test_ds.as_numpy_iterator():\n        test_state = compute_metrics(state=test_state, inputs=x_test, targets=y_test)\n\n        for metric,value in test_state.metrics.compute().items():\n            metrics_history[f'test_{metric}'].append(value)\n\n        print(f\"train epoch: {(step+1)}, \"\n              f\"loss: {metrics_history['train_loss'][-1]}, \"\n              f\"accuracy: {metrics_history['train_accuracy'][-1] * 100}\")\n        print(f\"test epoch: {(step+1) }, \"\n          f\"loss: {metrics_history['test_loss'][-1]}, \"\n          f\"accuracy: {metrics_history['test_accuracy'][-1] * 100}\")","metadata":{"id":"CaNt9JazHFyw","outputId":"ba447ddf-9940-44a6-f4b2-d27ed78a88c2","execution":{"iopub.status.busy":"2024-07-15T12:10:52.771185Z","iopub.status.idle":"2024-07-15T12:10:52.771496Z","shell.execute_reply.started":"2024-07-15T12:10:52.771341Z","shell.execute_reply":"2024-07-15T12:10:52.771354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt  # Visualization\n\n# Plot loss and accuracy in subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\nax1.set_title('Loss')\nax2.set_title('Accuracy')\nfor dataset in ('train','test'):\n    ax1.plot(metrics_history[f'{dataset}_loss'], label=f'{dataset}_loss')\n    ax2.plot(metrics_history[f'{dataset}_accuracy'], label=f'{dataset}_accuracy')\nax1.legend()\nax2.legend()\nplt.show()\nplt.clf()","metadata":{"id":"Y40JGx1YHFyw","execution":{"iopub.status.busy":"2024-07-15T12:10:52.772701Z","iopub.status.idle":"2024-07-15T12:10:52.773036Z","shell.execute_reply.started":"2024-07-15T12:10:52.772851Z","shell.execute_reply":"2024-07-15T12:10:52.772864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nlogits = fin_model.apply(fin_params, xb, training=False)[0]\nloss = optax.softmax_cross_entropy_with_integer_labels(\n            logits=logits, labels=yb).mean()\n\nprint(loss)","metadata":{"id":"7pJlFXpVHFyw","execution":{"iopub.status.busy":"2024-07-15T12:10:52.774514Z","iopub.status.idle":"2024-07-15T12:10:52.774838Z","shell.execute_reply.started":"2024-07-15T12:10:52.774679Z","shell.execute_reply":"2024-07-15T12:10:52.774693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def generate_text(idx, max_new_tokens, params):\n# # idx is (B, T) array of indices in the current context\n#     for i in range(max_new_tokens):\n#         # crop idx to the last block_size tokens\n#         idx_cond = idx[:, -block_size:]\n#         # get the predictions\n#         logits = fin_model.apply(params, idx_cond)\n#         # focus only on the last time step\n#         logits = logits[:, -1, :] # becomes (B, C)\n\n#         if i == 0:\n#             rng, rng_subkey = jax.random.split(jax.random.PRNGKey(12))\n#         else:\n#             rng, rng_subkey = jax.random.split(rng)\n\n#         idx_next = jax.random.categorical(rng_subkey, logits, axis=-1, shape=(1, 1)) # (B, 1)\n\n\n#         # append sampled index to the running sequence\n#         idx = jnp.concatenate([idx, idx_next], axis=-1) # (B, T+1)\n\n#     return idx","metadata":{"id":"9d28o-dTHFyx","execution":{"iopub.status.busy":"2024-07-15T12:10:52.775911Z","iopub.status.idle":"2024-07-15T12:10:52.776250Z","shell.execute_reply.started":"2024-07-15T12:10:52.776095Z","shell.execute_reply":"2024-07-15T12:10:52.776109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@partial(jax.jit, static_argnames=(\"self\", \"length\"))\ndef generate_text(rng, params, length):\n    def _scan_generate(carry, _):\n        random_key, context = carry\n        logits = fin_model.apply(params, context, training=False)[0]\n        rng, rng_subkey = jax.random.split(random_key)\n        new_token = jax.random.categorical(\n          rng_subkey, logits[:, -1, :], axis=-1, shape=(1, 1)\n        )\n        context = jnp.concatenate([context[:, 1:], new_token], axis=1)\n        return (rng, context), new_token\n\n    _, new_tokens = jax.lax.scan(\n    _scan_generate,\n    (rng, jnp.zeros((1, block_size), dtype=jnp.int32)),\n    (),\n    length=length,\n    )\n    return new_tokens","metadata":{"id":"WB0og7pAHFyx","execution":{"iopub.status.busy":"2024-07-15T12:34:43.289356Z","iopub.execute_input":"2024-07-15T12:34:43.290077Z","iopub.status.idle":"2024-07-15T12:34:43.300527Z","shell.execute_reply.started":"2024-07-15T12:34:43.290047Z","shell.execute_reply":"2024-07-15T12:34:43.299592Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/jax/_src/api_util.py:231: SyntaxWarning: Jitted function has invalid argnames {'self'} in static_argnames. Function does not take these args.This warning will be replaced by an error after 2022-08-20 at the earliest.\n  warnings.warn(f\"Jitted function has invalid argnames {invalid_argnames} \"\n","output_type":"stream"}]},{"cell_type":"code","source":"new_tokenz = 1000\nkey, subkey = jax.random.split(jax.random.PRNGKey(156))\n# key, subkey = jax.random.split(key)\n# token_gen = generate_text(jnp.zeros((1,block_size)).astype(jnp.int32), new_tokenz, {'params': state.params})\ntoken_gen = generate_text(key, {'params': state.params}, new_tokenz)[:, 0, 0].tolist()\nprint(token_gen)\nprint(decode(token_gen))","metadata":{"id":"50Vpg2lEHFyx","execution":{"iopub.status.busy":"2024-07-15T12:10:52.779631Z","iopub.status.idle":"2024-07-15T12:10:52.779983Z","shell.execute_reply.started":"2024-07-15T12:10:52.779796Z","shell.execute_reply":"2024-07-15T12:10:52.779810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sdgh  fs","metadata":{"execution":{"iopub.status.busy":"2024-07-15T12:10:52.784265Z","iopub.status.idle":"2024-07-15T12:10:52.784609Z","shell.execute_reply.started":"2024-07-15T12:10:52.784447Z","shell.execute_reply":"2024-07-15T12:10:52.784461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"state.params","metadata":{"execution":{"iopub.status.busy":"2024-07-15T12:10:52.785715Z","iopub.status.idle":"2024-07-15T12:10:52.786064Z","shell.execute_reply.started":"2024-07-15T12:10:52.785874Z","shell.execute_reply":"2024-07-15T12:10:52.785888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install mamba-ssm","metadata":{"id":"MOw_xjbrHFy0","execution":{"iopub.status.busy":"2024-07-15T12:10:52.786985Z","iopub.status.idle":"2024-07-15T12:10:52.787280Z","shell.execute_reply.started":"2024-07-15T12:10:52.787131Z","shell.execute_reply":"2024-07-15T12:10:52.787143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ones = lambda *size: torch.ones(*size).float().cuda()\nzeros = lambda *size: torch.zeros(*size).float().cuda()\narange = lambda n: torch.arange(n).float().cuda()\nrand = lambda size: torch.rand(*size).abs().float().cuda()\n\ndef create_torch(S = 128, Ba = 2, D = 4, N = 4):\n    x = rand((Ba, 1, D, S))\n    a = -ones((Ba, N, D, 1))\n    b = ones((Ba, N, 1, S)) * 0.1\n    c = rand((Ba, N, 1, S)) * 0.1\n    delta = rand((Ba, 1, D, S)) * 0.1\n    return x, a, b, c, delta","metadata":{"id":"W_PAnYcEOR22","execution":{"iopub.status.busy":"2024-07-15T12:10:52.788650Z","iopub.status.idle":"2024-07-15T12:10:52.788996Z","shell.execute_reply.started":"2024-07-15T12:10:52.788812Z","shell.execute_reply":"2024-07-15T12:10:52.788826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import selective_scan_cuda\n\nxx, aa, bb, cc, ddelta = create_torch()\ny_from_repo = selective_scan_cuda.fwd(xx.squeeze(1), ddelta.squeeze(1), aa[0].squeeze(-1).T, bb.squeeze(-2)[:, None, :, :], cc.squeeze(-2)[:, None, :, :], None, None, None, False)\ny_from_repo","metadata":{"id":"ykh4GTvtOrak","execution":{"iopub.status.busy":"2024-07-15T12:10:52.790309Z","iopub.status.idle":"2024-07-15T12:10:52.790646Z","shell.execute_reply.started":"2024-07-15T12:10:52.790471Z","shell.execute_reply":"2024-07-15T12:10:52.790484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def discretize(a, b, delta):\n    da = delta * a\n    a_ = jnp.exp(da)\n    b_ = b * delta\n    return a_, b_\n\ndef ssm(x, a, b, c, delta):\n    \"Jax Implementation\"\n    y = []\n    h = 0\n    a_, b_ = discretize(a, b, delta)\n    for k in range(x.shape[-1]):\n        h = a_[..., k] * h + b_[..., k] * x[..., k]\n        y.append((c[..., k] * h).sum(1, keepdims=True))\n    return h, jnp.stack(y, -1)\n","metadata":{"id":"NEdG1yPNOtxU","execution":{"iopub.status.busy":"2024-07-15T12:10:52.791888Z","iopub.status.idle":"2024-07-15T12:10:52.792320Z","shell.execute_reply.started":"2024-07-15T12:10:52.792104Z","shell.execute_reply":"2024-07-15T12:10:52.792122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_, y_ = ssm(xx.cpu().numpy(), aa.cpu().numpy(), bb.cpu().numpy(), cc.cpu().numpy(), ddelta.cpu().numpy())","metadata":{"id":"GEjNcZSZPIp_","execution":{"iopub.status.busy":"2024-07-15T12:10:52.793323Z","iopub.status.idle":"2024-07-15T12:10:52.793741Z","shell.execute_reply.started":"2024-07-15T12:10:52.793526Z","shell.execute_reply":"2024-07-15T12:10:52.793544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"tWlqZZOmPnYk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from mamba_ssm import Mamba as Mamba_T\ntorch_mamba = Mamba_T(\n      # This module uses roughly 3 * expand * d_model^2 parameters\n      d_model=n_embd, # Model dimension d_model\n      d_state=16,  # SSM state expansion factor\n      d_conv=4,    # Local convolution width\n      expand=2,    # Block expansion factor\n)","metadata":{"id":"5RHAE_I1Pql9","execution":{"iopub.status.busy":"2024-07-15T12:10:52.794849Z","iopub.status.idle":"2024-07-15T12:10:52.795174Z","shell.execute_reply.started":"2024-07-15T12:10:52.795020Z","shell.execute_reply":"2024-07-15T12:10:52.795034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xm = x = rand((1, 1, n_embd, 32))\nxm.shape","metadata":{"id":"l9zw_M-USrDt","execution":{"iopub.status.busy":"2024-07-15T12:10:52.796401Z","iopub.status.idle":"2024-07-15T12:10:52.796705Z","shell.execute_reply.started":"2024-07-15T12:10:52.796557Z","shell.execute_reply":"2024-07-15T12:10:52.796570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch_mamba(xm.squeeze(1))","metadata":{"id":"gGmA2EWlTCo0","execution":{"iopub.status.busy":"2024-07-15T12:10:52.797700Z","iopub.status.idle":"2024-07-15T12:10:52.798026Z","shell.execute_reply.started":"2024-07-15T12:10:52.797844Z","shell.execute_reply":"2024-07-15T12:10:52.797857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch_mamba.in_proj","metadata":{"id":"73ek9mx9UBBl","execution":{"iopub.status.busy":"2024-07-15T12:10:52.799028Z","iopub.status.idle":"2024-07-15T12:10:52.799324Z","shell.execute_reply.started":"2024-07-15T12:10:52.799174Z","shell.execute_reply":"2024-07-15T12:10:52.799186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import CLIPTokenizer\ntokenizer_1 = CLIPTokenizer.from_pretrained('openai/clip-vit-base-patch32')","metadata":{"id":"P3l_ssIYbiYT","execution":{"iopub.status.busy":"2024-07-15T12:10:52.800587Z","iopub.status.idle":"2024-07-15T12:10:52.800911Z","shell.execute_reply.started":"2024-07-15T12:10:52.800751Z","shell.execute_reply":"2024-07-15T12:10:52.800765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenise_prompts(prompt):\n    inputs = []\n    for tokenizer in [tokenizer_1, tokenizer_2]:\n        text_inputs = tokenizer(\n            positive_prompt,\n            padding=\"max_length\",\n            max_length=tokenizer.model_max_length,\n            truncation=True,\n            return_tensors=\"np\",\n        )\n        inputs.append(text_inputs.input_ids)\n    return jnp.stack(inputs, axis=1)","metadata":{"id":"-X7hXQRMZhl3","execution":{"iopub.status.busy":"2024-07-15T12:10:52.801847Z","iopub.status.idle":"2024-07-15T12:10:52.802189Z","shell.execute_reply.started":"2024-07-15T12:10:52.802031Z","shell.execute_reply":"2024-07-15T12:10:52.802045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CNN(nn.Module):\n    \"\"\"A simple CNN model.\"\"\"\n\n    @nn.compact\n    def __call__(self, x):\n        x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n        print(x.shape)\n        x = nn.relu(x)\n        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n        print(x.shape)\n        x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n        print(x.shape)\n        x = nn.relu(x)\n        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n        print(x.shape)\n        x = x.reshape((x.shape[0], -1))  # flatten\n        print(x.shape)\n        x = nn.Dense(features=256)(x)\n        print(x.shape)\n        x = nn.relu(x)\n        x = nn.Dense(features=10)(x)\n        print(x.shape)\n        return x","metadata":{"id":"rJhKQ_Oua9Gy","execution":{"iopub.status.busy":"2024-07-15T12:10:52.803239Z","iopub.status.idle":"2024-07-15T12:10:52.803533Z","shell.execute_reply.started":"2024-07-15T12:10:52.803386Z","shell.execute_reply":"2024-07-15T12:10:52.803399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rng = jax.random.PRNGKey(20)\ncnn = CNN()\nparams = cnn.init(rng, jnp.ones([1, 28, 28, 1]))['params']","metadata":{"id":"mzkoYrSVkoJj","execution":{"iopub.status.busy":"2024-07-15T12:10:52.804697Z","iopub.status.idle":"2024-07-15T12:10:52.805047Z","shell.execute_reply.started":"2024-07-15T12:10:52.804857Z","shell.execute_reply":"2024-07-15T12:10:52.804870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Conv1d(nn.Module):\n    \"\"\"A simple 1D CNN model.\"\"\"\n\n    @nn.compact\n    def __call__(self, x):\n        x = nn.Embed(65, 256)(x) + nn.Embed(64, 256)(jnp.arange(64))\n        print(x.shape)\n#         x = nn.Dense(features=512)(x)\n#         print(x.shape)\n        x = nn.Conv(features=256, kernel_size=3, padding=1)(x)\n        print(x.shape)\n        x = nn.avg_pool(x, window_shape=(2,), strides=(2,))\n        print(x.shape)\n        x = nn.Conv(features=512, kernel_size=3, padding=1)(x)\n        print(x.shape)\n        x = nn.avg_pool(x, window_shape=(2,), strides=(2,))\n        print(x.shape)\n        x = nn.ConvTranspose(features=512, kernel_size=2, padding='same')(x)\n        print(x.shape)\n        x = nn.Dense(features=65)(x)\n        print(x.shape)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-07-15T12:10:52.806237Z","iopub.status.idle":"2024-07-15T12:10:52.806553Z","shell.execute_reply.started":"2024-07-15T12:10:52.806399Z","shell.execute_reply":"2024-07-15T12:10:52.806411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rng = jax.random.PRNGKey(204)\nmycon1d = Conv1d()\nparams = mycon1d.init(rng, jnp.ones((1, 64), dtype=jnp.int32))['params']","metadata":{"execution":{"iopub.status.busy":"2024-07-15T12:10:52.808141Z","iopub.status.idle":"2024-07-15T12:10:52.808445Z","shell.execute_reply.started":"2024-07-15T12:10:52.808291Z","shell.execute_reply":"2024-07-15T12:10:52.808305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}