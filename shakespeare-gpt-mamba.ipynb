{"metadata":{"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q clu","metadata":{"id":"gS6euWNvHFye","outputId":"45b149a7-9450-439c-da67-ab8678a3b0d0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt","metadata":{"id":"7jjCLfuUHFyg","outputId":"dfe048f0-dd44-40ef-edf3-2fa56558672f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from functools import partial\nimport jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\nfrom jax.nn.initializers import lecun_normal, normal\nfrom jax.numpy.linalg import eigh, inv, matrix_power\nfrom jax.scipy.signal import convolve\n\nimport torch\n\nfrom dataclasses import dataclass\n\nfrom typing import Union\n\nimport matplotlib.pyplot as plt\nimport seaborn\n\nfrom clu import metrics\nfrom flax.training import train_state  # Useful dataclass to keep train state\nfrom flax import struct                # Flax dataclasses\nimport optax                           # Common loss functions and optimizers\nfrom tqdm import tqdm","metadata":{"id":"YXSCJzupHFyh","execution":{"iopub.status.busy":"2024-05-24T13:32:32.579893Z","iopub.execute_input":"2024-05-24T13:32:32.580232Z","iopub.status.idle":"2024-05-24T13:32:36.174511Z","shell.execute_reply.started":"2024-05-24T13:32:32.580197Z","shell.execute_reply":"2024-05-24T13:32:36.173743Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# read it in to inspect it\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()","metadata":{"id":"KpJoV3KQHFyh","execution":{"iopub.status.busy":"2024-05-24T13:32:36.175856Z","iopub.execute_input":"2024-05-24T13:32:36.176323Z","iopub.status.idle":"2024-05-24T13:32:36.181885Z","shell.execute_reply.started":"2024-05-24T13:32:36.176292Z","shell.execute_reply":"2024-05-24T13:32:36.181233Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)","metadata":{"id":"PsWxZqyRHFyi","outputId":"b1730724-647e-45cd-edfa-97af24995830","execution":{"iopub.status.busy":"2024-05-24T13:32:36.183029Z","iopub.execute_input":"2024-05-24T13:32:36.183306Z","iopub.status.idle":"2024-05-24T13:32:36.212613Z","shell.execute_reply.started":"2024-05-24T13:32:36.183278Z","shell.execute_reply":"2024-05-24T13:32:36.211792Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n65\n","output_type":"stream"}]},{"cell_type":"code","source":"# create a mapping from characters to integers\nstoi = { ch: i for i,ch in enumerate(chars) }\nitos = { i: ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))","metadata":{"id":"S-mzLOk1HFyi","outputId":"f56e2f85-5a1c-4099-87df-436ba39f4363","execution":{"iopub.status.busy":"2024-05-24T13:32:36.214468Z","iopub.execute_input":"2024-05-24T13:32:36.214757Z","iopub.status.idle":"2024-05-24T13:32:36.223621Z","shell.execute_reply.started":"2024-05-24T13:32:36.214729Z","shell.execute_reply":"2024-05-24T13:32:36.222823Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"[46, 47, 47, 1, 58, 46, 43, 56, 43]\nhii there\n","output_type":"stream"}]},{"cell_type":"code","source":"data = jnp.array(encode(text), dtype=jnp.int32)\nprint(data.shape, data.dtype)\nprint(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this","metadata":{"id":"HImuqDd8HFyj","outputId":"91dcd15f-f068-4551-ad29-e6e41e52fd91","execution":{"iopub.status.busy":"2024-05-24T13:32:36.224551Z","iopub.execute_input":"2024-05-24T13:32:36.224872Z","iopub.status.idle":"2024-05-24T13:32:40.744605Z","shell.execute_reply.started":"2024-05-24T13:32:36.224844Z","shell.execute_reply":"2024-05-24T13:32:40.743745Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"(1115394,) int32\n[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n  0 37 53 59  1 39 56 43  1 39 50 50  1 56 43 57 53 50 60 43 42  1 56 39\n 58 46 43 56  1 58 53  1 42 47 43  1 58 46 39 52  1 58 53  1 44 39 51 47\n 57 46 12  0  0 13 50 50 10  0 30 43 57 53 50 60 43 42  8  1 56 43 57 53\n 50 60 43 42  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 18 47\n 56 57 58  6  1 63 53 59  1 49 52 53 61  1 15 39 47 59 57  1 25 39 56 41\n 47 59 57  1 47 57  1 41 46 47 43 44  1 43 52 43 51 63  1 58 53  1 58 46\n 43  1 54 43 53 54 50 43  8  0  0 13 50 50 10  0 35 43  1 49 52 53 61  5\n 58  6  1 61 43  1 49 52 53 61  5 58  8  0  0 18 47 56 57 58  1 15 47 58\n 47 64 43 52 10  0 24 43 58  1 59 57  1 49 47 50 50  1 46 47 51  6  1 39\n 52 42  1 61 43  5 50 50  1 46 39 60 43  1 41 53 56 52  1 39 58  1 53 59\n 56  1 53 61 52  1 54 56 47 41 43  8  0 21 57  5 58  1 39  1 60 43 56 42\n 47 41 58 12  0  0 13 50 50 10  0 26 53  1 51 53 56 43  1 58 39 50 49 47\n 52 45  1 53 52  5 58 11  1 50 43 58  1 47 58  1 40 43  1 42 53 52 43 10\n  1 39 61 39 63  6  1 39 61 39 63  2  0  0 31 43 41 53 52 42  1 15 47 58\n 47 64 43 52 10  0 27 52 43  1 61 53 56 42  6  1 45 53 53 42  1 41 47 58\n 47 64 43 52 57  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 35\n 43  1 39 56 43  1 39 41 41 53 59 52 58 43 42  1 54 53 53 56  1 41 47 58\n 47 64 43 52 57  6  1 58 46 43  1 54 39 58 56 47 41 47 39 52 57  1 45 53\n 53 42  8  0 35 46 39 58  1 39 59 58 46 53 56 47 58 63  1 57 59 56 44 43\n 47 58 57  1 53 52  1 61 53 59 50 42  1 56 43 50 47 43 60 43  1 59 57 10\n  1 47 44  1 58 46 43 63  0 61 53 59 50 42  1 63 47 43 50 42  1 59 57  1\n 40 59 58  1 58 46 43  1 57 59 54 43 56 44 50 59 47 58 63  6  1 61 46 47\n 50 43  1 47 58  1 61 43 56 43  0 61 46 53 50 43 57 53 51 43  6  1 61 43\n  1 51 47 45 46 58  1 45 59 43 57 57  1 58 46 43 63  1 56 43 50 47 43 60\n 43 42  1 59 57  1 46 59 51 39 52 43 50 63 11  0 40 59 58  1 58 46 43 63\n  1 58 46 47 52 49  1 61 43  1 39 56 43  1 58 53 53  1 42 43 39 56 10  1\n 58 46 43  1 50 43 39 52 52 43 57 57  1 58 46 39 58  0 39 44 44 50 47 41\n 58 57  1 59 57  6  1 58 46 43  1 53 40 48 43 41 58  1 53 44  1 53 59 56\n  1 51 47 57 43 56 63  6  1 47 57  1 39 57  1 39 52  0 47 52 60 43 52 58\n 53 56 63  1 58 53  1 54 39 56 58 47 41 59 50 39 56 47 57 43  1 58 46 43\n 47 56  1 39 40 59 52 42 39 52 41 43 11  1 53 59 56  0 57 59 44 44 43 56\n 39 52 41 43  1 47 57  1 39  1 45 39 47 52  1 58 53  1 58 46 43 51  1 24\n 43 58  1 59 57  1 56 43 60 43 52 45 43  1 58 46 47 57  1 61 47 58 46  0\n 53 59 56  1 54 47 49 43 57  6  1 43 56 43  1 61 43  1 40 43 41 53 51 43\n  1 56 39 49 43 57 10  1 44 53 56  1 58 46 43  1 45 53 42 57  1 49 52 53\n 61  1 21  0 57 54 43 39 49  1 58 46 47 57  1 47 52  1 46 59 52 45 43 56\n  1 44 53 56  1 40 56 43 39 42  6  1 52 53 58  1 47 52  1 58 46 47 56 57\n 58  1 44 53 56  1 56 43 60 43 52 45 43  8  0  0]\n","output_type":"stream"}]},{"cell_type":"code","source":"train_test_split = 0.9\nn = int(train_test_split*len(data))\ntrain_data = data[:n]\ntest_data = data[n:]","metadata":{"id":"pXrAqMxRHFyj","execution":{"iopub.status.busy":"2024-05-24T13:32:40.745644Z","iopub.execute_input":"2024-05-24T13:32:40.746230Z","iopub.status.idle":"2024-05-24T13:32:40.899143Z","shell.execute_reply.started":"2024-05-24T13:32:40.746199Z","shell.execute_reply":"2024-05-24T13:32:40.898188Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"block_size = 8\ntrain_data[:block_size+1]","metadata":{"id":"ahhKyiAzHFyj","outputId":"98306c96-5082-4dfa-ba66-915051831fc8","execution":{"iopub.status.busy":"2024-05-24T13:32:40.900117Z","iopub.execute_input":"2024-05-24T13:32:40.900398Z","iopub.status.idle":"2024-05-24T13:32:40.936812Z","shell.execute_reply.started":"2024-05-24T13:32:40.900372Z","shell.execute_reply":"2024-05-24T13:32:40.936010Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"Array([18, 47, 56, 57, 58,  1, 15, 47, 58], dtype=int32)"},"metadata":{}}]},{"cell_type":"code","source":"x = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")","metadata":{"id":"HIpsznQmHFyk","outputId":"be9d197b-0b79-43ed-f3a9-e74295d51c79","execution":{"iopub.status.busy":"2024-05-24T13:32:40.937812Z","iopub.execute_input":"2024-05-24T13:32:40.938141Z","iopub.status.idle":"2024-05-24T13:32:41.217112Z","shell.execute_reply.started":"2024-05-24T13:32:40.938110Z","shell.execute_reply":"2024-05-24T13:32:41.216239Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"when input is [18] the target: 47\nwhen input is [18 47] the target: 56\nwhen input is [18 47 56] the target: 57\nwhen input is [18 47 56 57] the target: 58\nwhen input is [18 47 56 57 58] the target: 1\nwhen input is [18 47 56 57 58  1] the target: 15\nwhen input is [18 47 56 57 58  1 15] the target: 47\nwhen input is [18 47 56 57 58  1 15 47] the target: 58\n","output_type":"stream"}]},{"cell_type":"code","source":"batch_size = 128 # how many independent sequences will we process in parallel?\nblock_size = 64 # what is the maximum context length for predictions?\nmax_iters = 50000\nlearning_rate = 1e-3\n# device = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 100\nn_embd = 256\nexpans = 2\nn_heads = 1\nchannel_size = n_embd // n_heads\nn_layers = 6\ndropout = 0.2\nconv_k_size = 3\nn_latent_dim = 16\nn_tokens = 6\n\nrng_key = jax.random.PRNGKey(1564)\n\ndynamic_slice_vmap = jax.vmap(jax.lax.dynamic_slice, in_axes=(None, 0, None))\n\n@jax.jit\ndef get_batch(random_key, data):\n    \"\"\"Prepares a random batch of training data.\n\n    Args:\n      random_key: A random seed for sampling a batch.\n      data: The complete training dataset.\n\n    Returns:\n      x: Input sequences.\n      y: Target sequences (shifted inputs).\n    \"\"\"\n    ix = jax.random.randint(\n      random_key, shape=(batch_size, 1), minval=0, maxval=len(data) - block_size\n    )\n    x = dynamic_slice_vmap(data, ix, (block_size,))\n    y = dynamic_slice_vmap(data, ix + n_tokens, (block_size,))\n    return x, y\n\nxb, yb = get_batch(rng_key, train_data)\ntrain_shape = xb.shape\nprint('inputs:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\n# print('----')\n\n# for b in range(batch_size): # batch dimension\n#     for t in range(block_size): # time dimension\n#         context = xb[b, :t+1]\n#         target = yb[b,t]\n#         print(f\"when input is {context} the target: {target}\")","metadata":{"id":"UuAjtqPeHFyk","outputId":"6a88fb2b-b798-4ee9-9f4f-f38ce898d576","execution":{"iopub.status.busy":"2024-05-24T13:32:41.218168Z","iopub.execute_input":"2024-05-24T13:32:41.218446Z","iopub.status.idle":"2024-05-24T13:32:41.686728Z","shell.execute_reply.started":"2024-05-24T13:32:41.218418Z","shell.execute_reply":"2024-05-24T13:32:41.685816Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"inputs:\n(128, 64)\n[[ 1 63 53 ... 40 63  1]\n [43  1 52 ... 46 39 42]\n [63  1 45 ... 50  1 61]\n ...\n [46 47 41 ... 58 43  1]\n [ 1 61 43 ... 53 59  1]\n [49  1 58 ... 53 61  1]]\ntargets:\n(128, 64)\n[[ 8  0  0 ... 57  1 45]\n [11  0 20 ...  1  5 57]\n [ 1 50 53 ... 43 39 56]\n ...\n [53 59 50 ... 45 56 43]\n [ 1 39 52 ...  1 41 53]\n [46 43 39 ...  1 52 53]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Mamba Block\nDense --> Conv1D --> Silu --> SSM --> Silu -->","metadata":{"id":"yOccqzJlHFym"}},{"cell_type":"code","source":"print(xb[0])\nprint(yb[0])","metadata":{"execution":{"iopub.status.busy":"2024-05-24T13:32:41.690071Z","iopub.execute_input":"2024-05-24T13:32:41.690680Z","iopub.status.idle":"2024-05-24T13:32:41.761266Z","shell.execute_reply.started":"2024-05-24T13:32:41.690643Z","shell.execute_reply":"2024-05-24T13:32:41.760284Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"[ 1 63 53 59 56 57  8  0  0 24 17 27 26 32 17 31 10  0 13  1 52 43 57 58\n  1 53 44  1 58 56 39 47 58 53 56 57  2  0  0 13 26 32 21 19 27 26 33 31\n 10  0 21  1 39 51  1 52 53 52 43  6  1 40 63  1]\n[ 8  0  0 24 17 27 26 32 17 31 10  0 13  1 52 43 57 58  1 53 44  1 58 56\n 39 47 58 53 56 57  2  0  0 13 26 32 21 19 27 26 33 31 10  0 21  1 39 51\n  1 52 53 52 43  6  1 40 63  1 58 46 47 57  1 45]\n","output_type":"stream"}]},{"cell_type":"code","source":"yb[0,-n_tokens]","metadata":{"execution":{"iopub.status.busy":"2024-05-24T13:32:41.762337Z","iopub.execute_input":"2024-05-24T13:32:41.762643Z","iopub.status.idle":"2024-05-24T13:32:41.932805Z","shell.execute_reply.started":"2024-05-24T13:32:41.762612Z","shell.execute_reply":"2024-05-24T13:32:41.931967Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"Array(58, dtype=int32)"},"metadata":{}}]},{"cell_type":"code","source":"# hidden_state = [jnp.zeros((1,n_latent_dim, n_embd * expans)) for _ in range(n_layers)]","metadata":{"execution":{"iopub.status.busy":"2024-05-24T13:32:41.933652Z","iopub.execute_input":"2024-05-24T13:32:41.933890Z","iopub.status.idle":"2024-05-24T13:32:41.937023Z","shell.execute_reply.started":"2024-05-24T13:32:41.933866Z","shell.execute_reply":"2024-05-24T13:32:41.936311Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# hidden_state[0].shape","metadata":{"execution":{"iopub.status.busy":"2024-05-24T13:32:41.937842Z","iopub.execute_input":"2024-05-24T13:32:41.938179Z","iopub.status.idle":"2024-05-24T13:32:41.948228Z","shell.execute_reply.started":"2024-05-24T13:32:41.938150Z","shell.execute_reply":"2024-05-24T13:32:41.947527Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class Mamba(nn.Module):\n\n    def setup(self):\n        emb_features = n_embd * expans\n        self.in_proj1 = nn.Dense(features=emb_features)\n        self.in_proj2 = nn.Dense(features=emb_features)\n\n        # Adjusted for Flax. Flax does not have nn.Conv1d, so you might need to reshape or use a different approach\n        self.conv1d = nn.Conv(features=emb_features,\n                              kernel_size=conv_k_size,\n                              padding=1,\n                              )\n\n        self.A = -1*self.param('A', nn.initializers.ones, (1, n_latent_dim, emb_features, 1))\n        self.B = 0.1*self.param('B', nn.initializers.ones, (1, n_latent_dim, 1, block_size))\n        self.C = self.param('C', jax.random.normal, (1, n_latent_dim, 1, block_size))\n#         self.D = self.param('D', jax.random.normal, (1, self.args.d_state, self.args.d_model, 1))\n        self.delta = self.param('delta', jax.random.normal, (1, 1,emb_features, block_size))\n\n        self.out_proj = nn.Dense(n_embd // n_heads)\n        \n        self.hidden_state = self.variable('other_variables','hidden_state', \n                                          jnp.zeros, \n                                          (1,n_latent_dim, emb_features))\n        self.rms_norm = nn.RMSNorm()\n\n    def __call__(self, embeds):\n        x = self.in_proj1(embeds)\n        x = self.conv1d(x)\n        x = jax.nn.silu(x)\n        x = x.reshape((x.shape[0],1,x.shape[2],x.shape[1]))\n        x = self.ssm(x)\n        x = x.reshape((x.shape[0],x.shape[3],x.shape[2]))\n        x = x*jax.nn.silu(self.in_proj2(embeds))\n\n        x = self.out_proj(x)\n\n        x = self.rms_norm(x)\n\n        return x\n    def discretize(self):\n        da = self.delta * self.A\n        a_ = jnp.exp(da)\n        b_ = self.B * self.delta\n        return a_, b_\n\n    def ssm(self, x):\n        y = []\n        a_, b_ = self.discretize()\n        h = 0\n        for k in range(x.shape[-1]):\n            h = a_[..., k] * h + b_[..., k] * x[..., k]\n            y.append((self.C[..., k] * h).sum(1, keepdims=True))     \n        \n        self.hidden_state.value = jax.nn.standardize(h.mean(0, keepdims=True))\n        return jnp.stack(y, -1)","metadata":{"id":"4qOdblU5HFyo","execution":{"iopub.status.busy":"2024-05-24T13:32:41.949255Z","iopub.execute_input":"2024-05-24T13:32:41.949518Z","iopub.status.idle":"2024-05-24T13:32:41.962305Z","shell.execute_reply.started":"2024-05-24T13:32:41.949486Z","shell.execute_reply":"2024-05-24T13:32:41.961669Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# class MultiHeadMamba(nn.Module):\n#     def setup(self):\n#         self.layernorm\n#         self.heads = [Mamba() for _ in range(n_heads)]\n#         self.rms_norm = nn.RMSNorm()\n\n#     def __call__(self, x):\n#         out = jnp.concatenate([h(x) for h in self.heads], axis=-1)\n#         x = self.rms_norm(out)\n#         return x","metadata":{"id":"0bH9vlLZHFyq","execution":{"iopub.status.busy":"2024-05-24T13:32:41.963151Z","iopub.execute_input":"2024-05-24T13:32:41.963396Z","iopub.status.idle":"2024-05-24T13:32:41.975758Z","shell.execute_reply.started":"2024-05-24T13:32:41.963372Z","shell.execute_reply":"2024-05-24T13:32:41.975121Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# class FeedForward(nn.Module):\n#     def setup(self):\n#         self.ffn = nn.Sequential([\n#             nn.Dense(4 * n_embd),\n#             nn.relu,\n#             nn.Dense(n_embd)]\n#         )\n#     def __call__(self, x):\n#         return self.ffn(x)","metadata":{"execution":{"iopub.status.busy":"2024-05-24T13:32:41.976656Z","iopub.execute_input":"2024-05-24T13:32:41.976949Z","iopub.status.idle":"2024-05-24T13:32:41.991623Z","shell.execute_reply.started":"2024-05-24T13:32:41.976920Z","shell.execute_reply":"2024-05-24T13:32:41.990946Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# class MambaBlock(nn.Module):\n#     def setup(self):\n#         self.mamba_block = Mamba()\n#         self.ln1 = nn.RMSNorm()\n#         self.ffn = FeedForward()\n#         self.ln2 = nn.LayerNorm()\n\n#     def __call__(self, x):\n#         x = x + self.mamba_block(self.ln2(x))\n#         x = x + self.ffn(self.ln1(x))\n#         return x\n","metadata":{"id":"UiCxIjoEp2QA","execution":{"iopub.status.busy":"2024-05-24T13:32:41.992503Z","iopub.execute_input":"2024-05-24T13:32:41.992736Z","iopub.status.idle":"2024-05-24T13:32:42.000708Z","shell.execute_reply.started":"2024-05-24T13:32:41.992712Z","shell.execute_reply":"2024-05-24T13:32:42.000105Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# class MambaModel(nn.Module):\n\n#     def setup(self):\n#         self.tok_embeddings = nn.Embed(vocab_size, n_embd)\n#         self.pos_embeddings = nn.Embed(block_size, n_embd)\n#         self.ln = nn.LayerNorm()\n#         self.mamba_layers = [MambaBlock() for _ in range(n_layers)]\n#         self.preds_out = nn.Dense(vocab_size)\n\n#     def __call__(self, x, training: bool):\n#         x = self.tok_embeddings(x) + self.pos_embeddings(jnp.arange(block_size))\n# #         x = self.ln(x)\n#         for layer in self.mamba_layers:\n#             x = layer(x)\n            \n#         return self.preds_out(x)\n\n#     @jax.jit\n#     def generate(self, idx, max_new_tokens, params):\n#     # idx is (B, T) array of indices in the current context\n#         for _ in range(max_new_tokens):\n#             # crop idx to the last block_size tokens\n#             idx_cond = idx[:, -block_size:]\n#             # get the predictions\n#             logits = self.apply(params, idx_cond)\n#             # focus only on the last time step\n#             logits = logits[:, -1, :] # becomes (B, C)\n#             # apply softmax to get probabilities\n#             ##probs = tf.keras.activations.softmax(logits, dim=-1) # (B, C)\n#             # sample from the distribution\n#             idx_next = jax.random.categorical(jax.random.PRNGKey(52), logits) # (B, 1)\n#             # append sampled index to the running sequence\n#             idx = jax.numpy.expand_dims(jnp.concatenate([idx[0], idx_next], axis=0), 0) # (B, T+1)\n#     #         print(idx_next)\n#     #         print(idx)\n\n#         return idx","metadata":{"id":"y4C7OWL8HFyq","execution":{"iopub.status.busy":"2024-05-24T13:32:42.001630Z","iopub.execute_input":"2024-05-24T13:32:42.001932Z","iopub.status.idle":"2024-05-24T13:32:42.010723Z","shell.execute_reply.started":"2024-05-24T13:32:42.001905Z","shell.execute_reply":"2024-05-24T13:32:42.009981Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# model = Mamba()\n# params = model.init(jax.random.key(42), jnp.ones((1,64,256)))\n# # print(params['other_variables']['hidden_state'].shape, params['other_variables']['hidden_state'].min(), params['other_variables']['hidden_state'].max())\n# # print(model.tabulate(jax.random.key(0), jnp.ones((1,64,256)),\n# #                    compute_flops=True, compute_vjp_flops=True))\n# xs = model.apply(params, jnp.ones((1,64,256)), mutable=['other_variables'])\n# # # print(params['other_variables']['hidden_state'].shape, params['other_variables']['hidden_state'].min(), params['other_variables']['hidden_state'].max())\n# xb.shape, xs[0].shape, xs[1].keys()","metadata":{"id":"wTd3jSQWHFyp","execution":{"iopub.status.busy":"2024-05-24T13:32:42.011575Z","iopub.execute_input":"2024-05-24T13:32:42.011799Z","iopub.status.idle":"2024-05-24T13:32:42.022248Z","shell.execute_reply.started":"2024-05-24T13:32:42.011775Z","shell.execute_reply":"2024-05-24T13:32:42.021627Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# print(xs[1]['other_variables']['hidden_state'].shape, xs[1]['other_variables']['hidden_state'].min(), xs[1]['other_variables']['hidden_state'].max())","metadata":{"execution":{"iopub.status.busy":"2024-05-24T13:32:42.023067Z","iopub.execute_input":"2024-05-24T13:32:42.023297Z","iopub.status.idle":"2024-05-24T13:32:42.036352Z","shell.execute_reply.started":"2024-05-24T13:32:42.023273Z","shell.execute_reply":"2024-05-24T13:32:42.035651Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# xfs = model.apply(params, 2*jnp.ones((1,64,256)), mutable=['other_variables'])\n# print(params['other_variables']['hidden_state'].shape, params['other_variables']['hidden_state'].min(), params['other_variables']['hidden_state'].max())\n# print(xfs[1]['other_variables']['hidden_state'].shape, xfs[1]['other_variables']['hidden_state'].min(), xfs[1]['other_variables']['hidden_state'].max())","metadata":{"execution":{"iopub.status.busy":"2024-05-24T13:32:42.037264Z","iopub.execute_input":"2024-05-24T13:32:42.037570Z","iopub.status.idle":"2024-05-24T13:32:42.045111Z","shell.execute_reply.started":"2024-05-24T13:32:42.037542Z","shell.execute_reply":"2024-05-24T13:32:42.044466Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# test_model = Mamba()\n# test_params = test_model.init(jax.random.key(42), xb)\n# n_params = sum(p.size for p in jax.tree_util.tree_leaves(test_params))\n# print(f\"Total number of parameters: {n_params:_}\")\n# # print(fin_model.tabulate(jax.random.key(42), xb,\n# #                    compute_flops=True, compute_vjp_flops=True))\n# xf = test_model.apply(test_params, xb)\n# xb.shape, xf.shape","metadata":{"id":"cm2a0nepHFyq","execution":{"iopub.status.busy":"2024-05-24T13:32:42.045987Z","iopub.execute_input":"2024-05-24T13:32:42.046294Z","iopub.status.idle":"2024-05-24T13:32:42.057401Z","shell.execute_reply.started":"2024-05-24T13:32:42.046261Z","shell.execute_reply":"2024-05-24T13:32:42.056634Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"class NanoLM(nn.Module):\n    \"\"\"NanoLM model.\"\"\"\n    vocab_size: int = 65\n    num_layers: int = 6\n    num_heads: int = 8\n    head_size: int = 32\n    dropout_rate: float = 0.2\n    embed_size: int = 256\n    block_size: int = 64\n\n    @nn.compact\n    def __call__(self, x, training: bool):\n        x = nn.Embed(self.vocab_size, self.embed_size)(x) + nn.Embed(\n            self.block_size, self.embed_size\n        )(jnp.arange(self.block_size))\n        \n        for i in range(self.num_layers):\n            x_norm = nn.LayerNorm()(x)\n#             x = x + nn.MultiHeadDotProductAttention(\n#               num_heads=self.num_heads,\n#               qkv_features=self.head_size,\n#               out_features=self.head_size * self.num_heads,\n#               dropout_rate=self.dropout_rate,\n#             )(\n#               x_norm,\n#               x_norm,\n#               mask=jnp.tril(jnp.ones((x.shape[-2], x.shape[-2]))),\n#               deterministic=not training,\n#             )\n    \n            x = x + Mamba()(x_norm)\n\n#             x = x + nn.Sequential([\n#               nn.Dense(4 * self.embed_size),\n#               nn.relu,\n#               nn.Dropout(self.dropout_rate, deterministic=not training),\n#               nn.Dense(self.embed_size),\n#             ])(nn.LayerNorm()(x))\n\n        x = nn.LayerNorm()(x)\n        return nn.Dense(self.vocab_size)(x)","metadata":{"id":"zuiaFP6WHFyr","execution":{"iopub.status.busy":"2024-05-24T13:32:42.058333Z","iopub.execute_input":"2024-05-24T13:32:42.058647Z","iopub.status.idle":"2024-05-24T13:32:42.067987Z","shell.execute_reply.started":"2024-05-24T13:32:42.058609Z","shell.execute_reply":"2024-05-24T13:32:42.067284Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# key = jax.random.key(42)\n\n# # fin_model = MambaModel()\n# # fin_params = fin_model.init(key, xb, training=False)\n\n\n# fin_model = NanoLM(\n#     vocab_size=vocab_size,\n#     num_layers=n_layers,\n#     num_heads=8,\n#     head_size=32,\n#     dropout_rate=0.2,\n#     embed_size=n_embd,\n#     block_size=block_size,\n# )\n\n# fin_params = fin_model.init(\n#     {'params': key},\n#     jnp.ones((batch_size, block_size), dtype=jnp.int32),\n#     training=False\n# )\n\n# n_params = sum(p.size for p in jax.tree_util.tree_leaves(fin_params))\n# print(f\"Total number of parameters: {n_params:_}\")\n# # print(fin_model.tabulate(jax.random.key(42), xb,\n# #                    compute_flops=True, compute_vjp_flops=True))\n# xf = fin_model.apply(fin_params, xb, training=False)[0]\n# xb.shape, xf.shape","metadata":{"id":"fnUQPyuvHFys","outputId":"f04ebf31-d67f-4488-dd5d-7fd5b20dd1ea","execution":{"iopub.status.busy":"2024-05-24T13:32:42.068898Z","iopub.execute_input":"2024-05-24T13:32:42.069245Z","iopub.status.idle":"2024-05-24T13:32:42.080004Z","shell.execute_reply.started":"2024-05-24T13:32:42.069199Z","shell.execute_reply":"2024-05-24T13:32:42.079324Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def loss_fun(params, x, y, var_params,dropout_key):\n    logits, updated_variables = model.apply({'params': params, **var_params}, x, training=True, rngs={\"dropout\": dropout_key}, mutable=['other_variables'])\n    accuracy = jnp.mean(jnp.argmax(logits, axis=-1) == y)\n    return optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=y).mean(), (updated_variables, accuracy)\n\n@jax.jit\ndef eval_step(params, x, y, var_params):\n    logits, _ = model.apply({'params': params, **var_params}, x, training=False, mutable=['other_variables'])\n    accuracy = jnp.mean(jnp.argmax(logits, axis=-1) == y)\n    return optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=y).mean(), accuracy","metadata":{"execution":{"iopub.status.busy":"2024-05-24T13:32:42.080870Z","iopub.execute_input":"2024-05-24T13:32:42.081122Z","iopub.status.idle":"2024-05-24T13:32:42.093099Z","shell.execute_reply.started":"2024-05-24T13:32:42.081095Z","shell.execute_reply":"2024-05-24T13:32:42.092463Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"key = jax.random.PRNGKey(42)\nkey, subkey = jax.random.split(key)\n\nmodel = NanoLM(\n    vocab_size=vocab_size,\n    num_layers=n_layers,\n    num_heads=8,\n    head_size=32,\n    dropout_rate=0.2,\n    embed_size=n_embd,\n    block_size=block_size,\n)\n\nvar_params = model.init(\n    key,\n    jnp.ones((batch_size, block_size), dtype=jnp.int32),\n    training=False,\n)\nprint(var_params.keys())\nn_params = sum(p.size for p in jax.tree_util.tree_leaves(var_params))\n\nprint(f\"Total number of parameters: {n_params:_}\")","metadata":{"id":"PKpb3864HFyt","execution":{"iopub.status.busy":"2024-05-24T13:32:42.093960Z","iopub.execute_input":"2024-05-24T13:32:42.094296Z","iopub.status.idle":"2024-05-24T13:32:51.706715Z","shell.execute_reply.started":"2024-05-24T13:32:42.094251Z","shell.execute_reply":"2024-05-24T13:32:51.705797Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"dict_keys(['params', 'other_variables'])\nTotal number of parameters: 7_450_689\n","output_type":"stream"}]},{"cell_type":"code","source":"params = var_params.pop('params')","metadata":{"execution":{"iopub.status.busy":"2024-05-24T13:32:51.707709Z","iopub.execute_input":"2024-05-24T13:32:51.708026Z","iopub.status.idle":"2024-05-24T13:32:51.711916Z","shell.execute_reply.started":"2024-05-24T13:32:51.707980Z","shell.execute_reply":"2024-05-24T13:32:51.711162Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"var_params = jax.tree.map(lambda x: jnp.zeros_like(x), var_params)","metadata":{"execution":{"iopub.status.busy":"2024-05-24T13:32:51.717299Z","iopub.execute_input":"2024-05-24T13:32:51.717724Z","iopub.status.idle":"2024-05-24T13:32:51.728828Z","shell.execute_reply.started":"2024-05-24T13:32:51.717696Z","shell.execute_reply":"2024-05-24T13:32:51.728111Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# decay_rate = 0.96\n# learning_rate_schedule = optax.exponential_decay(learning_rate, decay_rate, max_iters//1000)\nopt = optax.adamw(learning_rate=learning_rate)\n\nopt_state = opt.init(params)","metadata":{"execution":{"iopub.status.busy":"2024-05-24T13:32:51.729635Z","iopub.execute_input":"2024-05-24T13:32:51.729940Z","iopub.status.idle":"2024-05-24T13:32:51.998101Z","shell.execute_reply.started":"2024-05-24T13:32:51.729911Z","shell.execute_reply":"2024-05-24T13:32:51.997220Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"%%time\n\nall_train_losses = []\nall_eval_losses = []\n\nall_train_accuracy =  []\nall_test_accuracy = []\n\n# we define one iteration of the optimizer and JIT this function\n@jax.jit\ndef step(key, params, var_params, opt_state):\n    key, subkey = jax.random.split(key)\n    xb, yb = get_batch(key, train_data)\n    (loss, aux_data), grad = jax.value_and_grad(loss_fun, has_aux=True)(params, xb, yb, var_params, subkey)\n    var_params, train_accuracy = aux_data\n    updates, opt_state = opt.update(grad, opt_state, params)\n    params = optax.apply_updates(params, updates)\n    return params, key, opt_state, loss, var_params, train_accuracy\n\n# for i in tqdm(range(max_iters)):\ncounter = 0\nloss = 10\nwhile counter<max_iters or loss > 1.0:\n\n    params, key, opt_state, loss, var_params, train_accuracy = step(key, params, var_params, opt_state)\n    \n\n    # once every N_FREQ_EVAL we compute loss on the validation set\n    if counter % eval_iters == 0:\n        key, subkey = jax.random.split(key)\n        eval_loss, eval_accuracy = eval_step(params, *get_batch(subkey, test_data), var_params)\n        all_train_losses.append(loss)\n        all_eval_losses.append(eval_loss)\n        all_train_accuracy.append(train_accuracy)\n        all_test_accuracy.append(eval_accuracy)\n        print('##########################################################')\n        print(f\"Step: {counter}\\t train loss: {loss}\\t train accuracy: {train_accuracy}\")\n        print(f\"Step: {counter}\\t eval loss: {eval_loss}\\t eval accuracy: {eval_accuracy}\")\n        \n    counter += 1\n        ","metadata":{"execution":{"iopub.status.busy":"2024-05-24T13:32:51.999200Z","iopub.execute_input":"2024-05-24T13:32:51.999483Z","iopub.status.idle":"2024-05-24T14:28:28.754651Z","shell.execute_reply.started":"2024-05-24T13:32:51.999455Z","shell.execute_reply":"2024-05-24T14:28:28.753591Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"##########################################################\nStep: 0\t train loss: 4.670287609100342\t train accuracy: 0.0166015625\nStep: 0\t eval loss: 4.616762638092041\t eval accuracy: 0.01708984375\n##########################################################\nStep: 100\t train loss: 3.341031789779663\t train accuracy: 0.154052734375\nStep: 100\t eval loss: 3.327639102935791\t eval accuracy: 0.1512451171875\n##########################################################\nStep: 200\t train loss: 3.326127290725708\t train accuracy: 0.150146484375\nStep: 200\t eval loss: 3.3565595149993896\t eval accuracy: 0.1474609375\n##########################################################\nStep: 300\t train loss: 3.3189868927001953\t train accuracy: 0.15283203125\nStep: 300\t eval loss: 3.3930160999298096\t eval accuracy: 0.1453857421875\n##########################################################\nStep: 400\t train loss: 3.347130298614502\t train accuracy: 0.1502685546875\nStep: 400\t eval loss: 3.294177532196045\t eval accuracy: 0.1522216796875\n##########################################################\nStep: 500\t train loss: 3.2795310020446777\t train accuracy: 0.1510009765625\nStep: 500\t eval loss: 3.331437349319458\t eval accuracy: 0.144287109375\n##########################################################\nStep: 600\t train loss: 3.2327911853790283\t train accuracy: 0.1541748046875\nStep: 600\t eval loss: 3.2896840572357178\t eval accuracy: 0.14892578125\n##########################################################\nStep: 700\t train loss: 3.2428464889526367\t train accuracy: 0.1539306640625\nStep: 700\t eval loss: 3.2519919872283936\t eval accuracy: 0.1549072265625\n##########################################################\nStep: 800\t train loss: 3.2223451137542725\t train accuracy: 0.1575927734375\nStep: 800\t eval loss: 3.2399582862854004\t eval accuracy: 0.1551513671875\n##########################################################\nStep: 900\t train loss: 3.232666492462158\t train accuracy: 0.1536865234375\nStep: 900\t eval loss: 3.2176077365875244\t eval accuracy: 0.1558837890625\n##########################################################\nStep: 1000\t train loss: 3.2253777980804443\t train accuracy: 0.1563720703125\nStep: 1000\t eval loss: 3.2333104610443115\t eval accuracy: 0.1552734375\n##########################################################\nStep: 1100\t train loss: 3.2234644889831543\t train accuracy: 0.1534423828125\nStep: 1100\t eval loss: 3.2334134578704834\t eval accuracy: 0.154052734375\n##########################################################\nStep: 1200\t train loss: 3.219294786453247\t train accuracy: 0.1578369140625\nStep: 1200\t eval loss: 3.2158193588256836\t eval accuracy: 0.1552734375\n##########################################################\nStep: 1300\t train loss: 3.1911025047302246\t train accuracy: 0.1610107421875\nStep: 1300\t eval loss: 3.2146337032318115\t eval accuracy: 0.1558837890625\n##########################################################\nStep: 1400\t train loss: 3.214740037918091\t train accuracy: 0.15380859375\nStep: 1400\t eval loss: 3.2205095291137695\t eval accuracy: 0.156494140625\n##########################################################\nStep: 1500\t train loss: 3.1983985900878906\t train accuracy: 0.1591796875\nStep: 1500\t eval loss: 3.1907966136932373\t eval accuracy: 0.1607666015625\n##########################################################\nStep: 1600\t train loss: 3.215014696121216\t train accuracy: 0.155517578125\nStep: 1600\t eval loss: 3.237217903137207\t eval accuracy: 0.1529541015625\n##########################################################\nStep: 1700\t train loss: 3.218480348587036\t train accuracy: 0.155517578125\nStep: 1700\t eval loss: 3.192521572113037\t eval accuracy: 0.1561279296875\n##########################################################\nStep: 1800\t train loss: 3.2139997482299805\t train accuracy: 0.1556396484375\nStep: 1800\t eval loss: 3.2296886444091797\t eval accuracy: 0.1536865234375\n##########################################################\nStep: 1900\t train loss: 3.209296941757202\t train accuracy: 0.157470703125\nStep: 1900\t eval loss: 3.2076923847198486\t eval accuracy: 0.160400390625\n##########################################################\nStep: 2000\t train loss: 3.175149440765381\t train accuracy: 0.156982421875\nStep: 2000\t eval loss: 3.2187862396240234\t eval accuracy: 0.1510009765625\n##########################################################\nStep: 2100\t train loss: 3.2239251136779785\t train accuracy: 0.1539306640625\nStep: 2100\t eval loss: 3.234595537185669\t eval accuracy: 0.152587890625\n##########################################################\nStep: 2200\t train loss: 3.1863059997558594\t train accuracy: 0.1578369140625\nStep: 2200\t eval loss: 3.197862148284912\t eval accuracy: 0.1561279296875\n##########################################################\nStep: 2300\t train loss: 3.1887364387512207\t train accuracy: 0.1573486328125\nStep: 2300\t eval loss: 3.1909122467041016\t eval accuracy: 0.1591796875\n##########################################################\nStep: 2400\t train loss: 3.1853079795837402\t train accuracy: 0.1607666015625\nStep: 2400\t eval loss: 3.194471597671509\t eval accuracy: 0.1573486328125\n##########################################################\nStep: 2500\t train loss: 3.1852657794952393\t train accuracy: 0.1590576171875\nStep: 2500\t eval loss: 3.194669246673584\t eval accuracy: 0.155517578125\n##########################################################\nStep: 2600\t train loss: 3.198359966278076\t train accuracy: 0.1580810546875\nStep: 2600\t eval loss: 3.173740863800049\t eval accuracy: 0.1571044921875\n##########################################################\nStep: 2700\t train loss: 3.1829845905303955\t train accuracy: 0.15869140625\nStep: 2700\t eval loss: 3.16567325592041\t eval accuracy: 0.161865234375\n##########################################################\nStep: 2800\t train loss: 3.166642904281616\t train accuracy: 0.1617431640625\nStep: 2800\t eval loss: 3.190053939819336\t eval accuracy: 0.15673828125\n##########################################################\nStep: 2900\t train loss: 3.188702344894409\t train accuracy: 0.152099609375\nStep: 2900\t eval loss: 3.1873018741607666\t eval accuracy: 0.1544189453125\n##########################################################\nStep: 3000\t train loss: 3.2003746032714844\t train accuracy: 0.1551513671875\nStep: 3000\t eval loss: 3.1553194522857666\t eval accuracy: 0.162109375\n##########################################################\nStep: 3100\t train loss: 3.1559629440307617\t train accuracy: 0.1617431640625\nStep: 3100\t eval loss: 3.1851015090942383\t eval accuracy: 0.1553955078125\n##########################################################\nStep: 3200\t train loss: 3.175499439239502\t train accuracy: 0.1591796875\nStep: 3200\t eval loss: 3.1923086643218994\t eval accuracy: 0.1553955078125\n##########################################################\nStep: 3300\t train loss: 3.1860711574554443\t train accuracy: 0.1566162109375\nStep: 3300\t eval loss: 3.1898722648620605\t eval accuracy: 0.158447265625\n##########################################################\nStep: 3400\t train loss: 3.1719467639923096\t train accuracy: 0.1607666015625\nStep: 3400\t eval loss: 3.2072253227233887\t eval accuracy: 0.1563720703125\n##########################################################\nStep: 3500\t train loss: 3.205059289932251\t train accuracy: 0.155029296875\nStep: 3500\t eval loss: 3.1766836643218994\t eval accuracy: 0.1588134765625\n##########################################################\nStep: 3600\t train loss: 3.1719398498535156\t train accuracy: 0.1571044921875\nStep: 3600\t eval loss: 3.176649808883667\t eval accuracy: 0.1593017578125\n##########################################################\nStep: 3700\t train loss: 3.1985790729522705\t train accuracy: 0.15625\nStep: 3700\t eval loss: 3.201807975769043\t eval accuracy: 0.15576171875\n##########################################################\nStep: 3800\t train loss: 3.181358575820923\t train accuracy: 0.159423828125\nStep: 3800\t eval loss: 3.20365309715271\t eval accuracy: 0.154052734375\n##########################################################\nStep: 3900\t train loss: 3.1616263389587402\t train accuracy: 0.1588134765625\nStep: 3900\t eval loss: 3.17639422416687\t eval accuracy: 0.161865234375\n##########################################################\nStep: 4000\t train loss: 3.176873207092285\t train accuracy: 0.15966796875\nStep: 4000\t eval loss: 3.150632858276367\t eval accuracy: 0.163330078125\n##########################################################\nStep: 4100\t train loss: 3.177201986312866\t train accuracy: 0.1568603515625\nStep: 4100\t eval loss: 3.1813299655914307\t eval accuracy: 0.1640625\n##########################################################\nStep: 4200\t train loss: 3.1694698333740234\t train accuracy: 0.1583251953125\nStep: 4200\t eval loss: 3.1880674362182617\t eval accuracy: 0.157958984375\n##########################################################\nStep: 4300\t train loss: 3.1464037895202637\t train accuracy: 0.1639404296875\nStep: 4300\t eval loss: 3.1697604656219482\t eval accuracy: 0.165771484375\n##########################################################\nStep: 4400\t train loss: 3.160722494125366\t train accuracy: 0.160888671875\nStep: 4400\t eval loss: 3.1572391986846924\t eval accuracy: 0.1654052734375\n##########################################################\nStep: 4500\t train loss: 3.1646578311920166\t train accuracy: 0.1614990234375\nStep: 4500\t eval loss: 3.169766426086426\t eval accuracy: 0.163330078125\n##########################################################\nStep: 4600\t train loss: 3.148026704788208\t train accuracy: 0.1651611328125\nStep: 4600\t eval loss: 3.1493093967437744\t eval accuracy: 0.158935546875\n##########################################################\nStep: 4700\t train loss: 3.1374592781066895\t train accuracy: 0.166748046875\nStep: 4700\t eval loss: 3.1542234420776367\t eval accuracy: 0.160400390625\n##########################################################\nStep: 4800\t train loss: 3.1466991901397705\t train accuracy: 0.1673583984375\nStep: 4800\t eval loss: 3.139734983444214\t eval accuracy: 0.1614990234375\n##########################################################\nStep: 4900\t train loss: 3.1418955326080322\t train accuracy: 0.1617431640625\nStep: 4900\t eval loss: 3.1352992057800293\t eval accuracy: 0.1651611328125\n##########################################################\nStep: 5000\t train loss: 3.1329729557037354\t train accuracy: 0.1663818359375\nStep: 5000\t eval loss: 3.132690668106079\t eval accuracy: 0.16552734375\n##########################################################\nStep: 5100\t train loss: 3.138554096221924\t train accuracy: 0.162109375\nStep: 5100\t eval loss: 3.135342836380005\t eval accuracy: 0.1695556640625\n##########################################################\nStep: 5200\t train loss: 3.127103328704834\t train accuracy: 0.166748046875\nStep: 5200\t eval loss: 3.1222262382507324\t eval accuracy: 0.1678466796875\n##########################################################\nStep: 5300\t train loss: 3.110595464706421\t train accuracy: 0.171875\nStep: 5300\t eval loss: 3.119041681289673\t eval accuracy: 0.1666259765625\n##########################################################\nStep: 5400\t train loss: 3.1046626567840576\t train accuracy: 0.1693115234375\nStep: 5400\t eval loss: 3.133634328842163\t eval accuracy: 0.1654052734375\n##########################################################\nStep: 5500\t train loss: 3.1361019611358643\t train accuracy: 0.1617431640625\nStep: 5500\t eval loss: 3.123492956161499\t eval accuracy: 0.168701171875\n##########################################################\nStep: 5600\t train loss: 3.146578311920166\t train accuracy: 0.1630859375\nStep: 5600\t eval loss: 3.136908531188965\t eval accuracy: 0.166748046875\n##########################################################\nStep: 5700\t train loss: 3.1225337982177734\t train accuracy: 0.1654052734375\nStep: 5700\t eval loss: 3.138015031814575\t eval accuracy: 0.1636962890625\n##########################################################\nStep: 5800\t train loss: 3.121466636657715\t train accuracy: 0.1690673828125\nStep: 5800\t eval loss: 3.1079158782958984\t eval accuracy: 0.1712646484375\n##########################################################\nStep: 5900\t train loss: 3.1035749912261963\t train accuracy: 0.1695556640625\nStep: 5900\t eval loss: 3.121234178543091\t eval accuracy: 0.1676025390625\n##########################################################\nStep: 6000\t train loss: 3.13215708732605\t train accuracy: 0.16845703125\nStep: 6000\t eval loss: 3.0860090255737305\t eval accuracy: 0.1724853515625\n##########################################################\nStep: 6100\t train loss: 3.1142358779907227\t train accuracy: 0.164794921875\nStep: 6100\t eval loss: 3.088106393814087\t eval accuracy: 0.1676025390625\n##########################################################\nStep: 6200\t train loss: 3.132856607437134\t train accuracy: 0.1641845703125\nStep: 6200\t eval loss: 3.11452579498291\t eval accuracy: 0.17041015625\n##########################################################\nStep: 6300\t train loss: 3.097060203552246\t train accuracy: 0.1700439453125\nStep: 6300\t eval loss: 3.11303448677063\t eval accuracy: 0.1717529296875\n##########################################################\nStep: 6400\t train loss: 3.1342246532440186\t train accuracy: 0.17041015625\nStep: 6400\t eval loss: 3.132274627685547\t eval accuracy: 0.1617431640625\n##########################################################\nStep: 6500\t train loss: 3.0834014415740967\t train accuracy: 0.1732177734375\nStep: 6500\t eval loss: 3.124864339828491\t eval accuracy: 0.1663818359375\n##########################################################\nStep: 6600\t train loss: 3.0891153812408447\t train accuracy: 0.1707763671875\nStep: 6600\t eval loss: 3.0913448333740234\t eval accuracy: 0.1697998046875\n##########################################################\nStep: 6700\t train loss: 3.100473642349243\t train accuracy: 0.17236328125\nStep: 6700\t eval loss: 3.0883986949920654\t eval accuracy: 0.166259765625\n##########################################################\nStep: 6800\t train loss: 3.1112842559814453\t train accuracy: 0.165771484375\nStep: 6800\t eval loss: 3.108961820602417\t eval accuracy: 0.1685791015625\n##########################################################\nStep: 6900\t train loss: 3.086578130722046\t train accuracy: 0.1773681640625\nStep: 6900\t eval loss: 3.1206307411193848\t eval accuracy: 0.1671142578125\n##########################################################\nStep: 7000\t train loss: 3.066173791885376\t train accuracy: 0.174560546875\nStep: 7000\t eval loss: 3.0676777362823486\t eval accuracy: 0.1732177734375\n##########################################################\nStep: 7100\t train loss: 3.050295829772949\t train accuracy: 0.1810302734375\nStep: 7100\t eval loss: 3.061166763305664\t eval accuracy: 0.1785888671875\n##########################################################\nStep: 7200\t train loss: 3.0635123252868652\t train accuracy: 0.17626953125\nStep: 7200\t eval loss: 3.0580315589904785\t eval accuracy: 0.17578125\n##########################################################\nStep: 7300\t train loss: 3.0472185611724854\t train accuracy: 0.1776123046875\nStep: 7300\t eval loss: 3.0529255867004395\t eval accuracy: 0.17919921875\n##########################################################\nStep: 7400\t train loss: 3.0509402751922607\t train accuracy: 0.1767578125\nStep: 7400\t eval loss: 3.0453996658325195\t eval accuracy: 0.1776123046875\n##########################################################\nStep: 7500\t train loss: 3.0436553955078125\t train accuracy: 0.17822265625\nStep: 7500\t eval loss: 3.049813747406006\t eval accuracy: 0.174072265625\n##########################################################\nStep: 7600\t train loss: 3.042854070663452\t train accuracy: 0.1842041015625\nStep: 7600\t eval loss: 3.040485143661499\t eval accuracy: 0.1771240234375\n##########################################################\nStep: 7700\t train loss: 3.0527420043945312\t train accuracy: 0.17626953125\nStep: 7700\t eval loss: 3.0411016941070557\t eval accuracy: 0.1834716796875\n##########################################################\nStep: 7800\t train loss: 3.0426406860351562\t train accuracy: 0.18017578125\nStep: 7800\t eval loss: 3.0510616302490234\t eval accuracy: 0.1751708984375\n##########################################################\nStep: 7900\t train loss: 3.0288214683532715\t train accuracy: 0.1822509765625\nStep: 7900\t eval loss: 3.0278446674346924\t eval accuracy: 0.1795654296875\n##########################################################\nStep: 8000\t train loss: 3.0201303958892822\t train accuracy: 0.183349609375\nStep: 8000\t eval loss: 3.0256195068359375\t eval accuracy: 0.1845703125\n##########################################################\nStep: 8100\t train loss: 3.029465436935425\t train accuracy: 0.18603515625\nStep: 8100\t eval loss: 3.020188570022583\t eval accuracy: 0.1876220703125\n##########################################################\nStep: 8200\t train loss: 3.0302987098693848\t train accuracy: 0.1806640625\nStep: 8200\t eval loss: 3.0346531867980957\t eval accuracy: 0.182373046875\n##########################################################\nStep: 8300\t train loss: 3.010465621948242\t train accuracy: 0.1868896484375\nStep: 8300\t eval loss: 3.0256145000457764\t eval accuracy: 0.181396484375\n##########################################################\nStep: 8400\t train loss: 3.005803108215332\t train accuracy: 0.190185546875\nStep: 8400\t eval loss: 3.021526336669922\t eval accuracy: 0.1817626953125\n##########################################################\nStep: 8500\t train loss: 3.0069241523742676\t train accuracy: 0.189208984375\nStep: 8500\t eval loss: 3.0150699615478516\t eval accuracy: 0.191162109375\n##########################################################\nStep: 8600\t train loss: 2.9896063804626465\t train accuracy: 0.1903076171875\nStep: 8600\t eval loss: 3.0040173530578613\t eval accuracy: 0.1824951171875\n##########################################################\nStep: 8700\t train loss: 2.9737021923065186\t train accuracy: 0.1917724609375\nStep: 8700\t eval loss: 2.9838390350341797\t eval accuracy: 0.1845703125\n##########################################################\nStep: 8800\t train loss: 2.9947874546051025\t train accuracy: 0.1861572265625\nStep: 8800\t eval loss: 2.9750635623931885\t eval accuracy: 0.1947021484375\n##########################################################\nStep: 8900\t train loss: 3.004444122314453\t train accuracy: 0.184326171875\nStep: 8900\t eval loss: 2.971357822418213\t eval accuracy: 0.1932373046875\n##########################################################\nStep: 9000\t train loss: 3.0003128051757812\t train accuracy: 0.192138671875\nStep: 9000\t eval loss: 2.995835542678833\t eval accuracy: 0.185302734375\n##########################################################\nStep: 9100\t train loss: 2.9568417072296143\t train accuracy: 0.19482421875\nStep: 9100\t eval loss: 2.963596820831299\t eval accuracy: 0.1971435546875\n##########################################################\nStep: 9200\t train loss: 2.9632949829101562\t train accuracy: 0.192138671875\nStep: 9200\t eval loss: 2.9879062175750732\t eval accuracy: 0.196044921875\n##########################################################\nStep: 9300\t train loss: 2.9636359214782715\t train accuracy: 0.1915283203125\nStep: 9300\t eval loss: 2.988607406616211\t eval accuracy: 0.191650390625\n##########################################################\nStep: 9400\t train loss: 2.972362995147705\t train accuracy: 0.188232421875\nStep: 9400\t eval loss: 2.9571032524108887\t eval accuracy: 0.192138671875\n##########################################################\nStep: 9500\t train loss: 2.982675075531006\t train accuracy: 0.1925048828125\nStep: 9500\t eval loss: 2.966937303543091\t eval accuracy: 0.1903076171875\n##########################################################\nStep: 9600\t train loss: 2.979043483734131\t train accuracy: 0.1939697265625\nStep: 9600\t eval loss: 2.9844417572021484\t eval accuracy: 0.1881103515625\n##########################################################\nStep: 9700\t train loss: 2.9706172943115234\t train accuracy: 0.1925048828125\nStep: 9700\t eval loss: 2.9944956302642822\t eval accuracy: 0.1854248046875\n##########################################################\nStep: 9800\t train loss: 2.9457623958587646\t train accuracy: 0.1954345703125\nStep: 9800\t eval loss: 2.9775140285491943\t eval accuracy: 0.193603515625\n##########################################################\nStep: 9900\t train loss: 2.948957920074463\t train accuracy: 0.1947021484375\nStep: 9900\t eval loss: 2.936328887939453\t eval accuracy: 0.1993408203125\n##########################################################\nStep: 10000\t train loss: 2.9045772552490234\t train accuracy: 0.203369140625\nStep: 10000\t eval loss: 2.947327136993408\t eval accuracy: 0.2008056640625\n##########################################################\nStep: 10100\t train loss: 2.9272103309631348\t train accuracy: 0.2010498046875\nStep: 10100\t eval loss: 2.9441089630126953\t eval accuracy: 0.194580078125\n##########################################################\nStep: 10200\t train loss: 2.9453675746917725\t train accuracy: 0.2000732421875\nStep: 10200\t eval loss: 2.9301679134368896\t eval accuracy: 0.1966552734375\n##########################################################\nStep: 10300\t train loss: 2.9169423580169678\t train accuracy: 0.203857421875\nStep: 10300\t eval loss: 2.9442970752716064\t eval accuracy: 0.1962890625\n##########################################################\nStep: 10400\t train loss: 2.917726516723633\t train accuracy: 0.2037353515625\nStep: 10400\t eval loss: 2.9561336040496826\t eval accuracy: 0.1966552734375\n##########################################################\nStep: 10500\t train loss: 2.9181482791900635\t train accuracy: 0.209228515625\nStep: 10500\t eval loss: 2.9350640773773193\t eval accuracy: 0.199951171875\n##########################################################\nStep: 10600\t train loss: 2.903226375579834\t train accuracy: 0.2076416015625\nStep: 10600\t eval loss: 2.908250093460083\t eval accuracy: 0.2008056640625\n##########################################################\nStep: 10700\t train loss: 2.894035577774048\t train accuracy: 0.2113037109375\nStep: 10700\t eval loss: 2.9266324043273926\t eval accuracy: 0.2030029296875\n##########################################################\nStep: 10800\t train loss: 2.906548023223877\t train accuracy: 0.2115478515625\nStep: 10800\t eval loss: 2.919550657272339\t eval accuracy: 0.2066650390625\n##########################################################\nStep: 10900\t train loss: 2.889554977416992\t train accuracy: 0.2078857421875\nStep: 10900\t eval loss: 2.9195237159729004\t eval accuracy: 0.204345703125\n##########################################################\nStep: 11000\t train loss: 2.876349925994873\t train accuracy: 0.2135009765625\nStep: 11000\t eval loss: 2.8974051475524902\t eval accuracy: 0.20751953125\n##########################################################\nStep: 11100\t train loss: 2.8960354328155518\t train accuracy: 0.213623046875\nStep: 11100\t eval loss: 2.8967528343200684\t eval accuracy: 0.2059326171875\n##########################################################\nStep: 11200\t train loss: 2.8652803897857666\t train accuracy: 0.2115478515625\nStep: 11200\t eval loss: 2.8782925605773926\t eval accuracy: 0.214111328125\n##########################################################\nStep: 11300\t train loss: 2.856764793395996\t train accuracy: 0.214599609375\nStep: 11300\t eval loss: 2.8800106048583984\t eval accuracy: 0.2137451171875\n##########################################################\nStep: 11400\t train loss: 2.8248143196105957\t train accuracy: 0.221435546875\nStep: 11400\t eval loss: 2.879741668701172\t eval accuracy: 0.2080078125\n##########################################################\nStep: 11500\t train loss: 2.8316805362701416\t train accuracy: 0.220947265625\nStep: 11500\t eval loss: 2.85542893409729\t eval accuracy: 0.2120361328125\n##########################################################\nStep: 11600\t train loss: 2.85119891166687\t train accuracy: 0.21875\nStep: 11600\t eval loss: 2.8544631004333496\t eval accuracy: 0.213134765625\n##########################################################\nStep: 11700\t train loss: 2.8151462078094482\t train accuracy: 0.2244873046875\nStep: 11700\t eval loss: 2.8583571910858154\t eval accuracy: 0.211181640625\n##########################################################\nStep: 11800\t train loss: 2.8333418369293213\t train accuracy: 0.2205810546875\nStep: 11800\t eval loss: 2.8365933895111084\t eval accuracy: 0.2164306640625\n##########################################################\nStep: 11900\t train loss: 2.820796489715576\t train accuracy: 0.2213134765625\nStep: 11900\t eval loss: 2.834003210067749\t eval accuracy: 0.218994140625\n##########################################################\nStep: 12000\t train loss: 2.80074143409729\t train accuracy: 0.22705078125\nStep: 12000\t eval loss: 2.8407769203186035\t eval accuracy: 0.214599609375\n##########################################################\nStep: 12100\t train loss: 2.8623664379119873\t train accuracy: 0.215576171875\nStep: 12100\t eval loss: 2.8399198055267334\t eval accuracy: 0.212158203125\n##########################################################\nStep: 12200\t train loss: 2.836871862411499\t train accuracy: 0.21630859375\nStep: 12200\t eval loss: 2.8097925186157227\t eval accuracy: 0.22216796875\n##########################################################\nStep: 12300\t train loss: 2.8199164867401123\t train accuracy: 0.216552734375\nStep: 12300\t eval loss: 2.8414411544799805\t eval accuracy: 0.2144775390625\n##########################################################\nStep: 12400\t train loss: 2.8396620750427246\t train accuracy: 0.21435546875\nStep: 12400\t eval loss: 2.8356611728668213\t eval accuracy: 0.2115478515625\n##########################################################\nStep: 12500\t train loss: 2.831655502319336\t train accuracy: 0.2225341796875\nStep: 12500\t eval loss: 2.848114013671875\t eval accuracy: 0.2130126953125\n##########################################################\nStep: 12600\t train loss: 2.8638710975646973\t train accuracy: 0.2191162109375\nStep: 12600\t eval loss: 2.8565726280212402\t eval accuracy: 0.20947265625\n##########################################################\nStep: 12700\t train loss: 2.8186233043670654\t train accuracy: 0.22607421875\nStep: 12700\t eval loss: 2.8151628971099854\t eval accuracy: 0.2197265625\n##########################################################\nStep: 12800\t train loss: 2.829707145690918\t train accuracy: 0.2176513671875\nStep: 12800\t eval loss: 2.8172106742858887\t eval accuracy: 0.2181396484375\n##########################################################\nStep: 12900\t train loss: 2.7746293544769287\t train accuracy: 0.23046875\nStep: 12900\t eval loss: 2.816751718521118\t eval accuracy: 0.22509765625\n##########################################################\nStep: 13000\t train loss: 2.7767927646636963\t train accuracy: 0.228759765625\nStep: 13000\t eval loss: 2.8320729732513428\t eval accuracy: 0.223388671875\n##########################################################\nStep: 13100\t train loss: 2.767374277114868\t train accuracy: 0.2335205078125\nStep: 13100\t eval loss: 2.804948568344116\t eval accuracy: 0.22705078125\n##########################################################\nStep: 13200\t train loss: 2.797618865966797\t train accuracy: 0.2249755859375\nStep: 13200\t eval loss: 2.80015230178833\t eval accuracy: 0.225341796875\n##########################################################\nStep: 13300\t train loss: 2.7396039962768555\t train accuracy: 0.2430419921875\nStep: 13300\t eval loss: 2.7925078868865967\t eval accuracy: 0.2249755859375\n##########################################################\nStep: 13400\t train loss: 2.7401206493377686\t train accuracy: 0.2415771484375\nStep: 13400\t eval loss: 2.752187728881836\t eval accuracy: 0.228515625\n##########################################################\nStep: 13500\t train loss: 2.7337276935577393\t train accuracy: 0.2449951171875\nStep: 13500\t eval loss: 2.7315454483032227\t eval accuracy: 0.2349853515625\n##########################################################\nStep: 13600\t train loss: 2.7312607765197754\t train accuracy: 0.240966796875\nStep: 13600\t eval loss: 2.7303383350372314\t eval accuracy: 0.239990234375\n##########################################################\nStep: 13700\t train loss: 2.706490993499756\t train accuracy: 0.246337890625\nStep: 13700\t eval loss: 2.73323917388916\t eval accuracy: 0.23779296875\n##########################################################\nStep: 13800\t train loss: 2.7392749786376953\t train accuracy: 0.24169921875\nStep: 13800\t eval loss: 2.772714614868164\t eval accuracy: 0.23291015625\n##########################################################\nStep: 13900\t train loss: 2.6883673667907715\t train accuracy: 0.2537841796875\nStep: 13900\t eval loss: 2.7224788665771484\t eval accuracy: 0.245849609375\n##########################################################\nStep: 14000\t train loss: 2.689882516860962\t train accuracy: 0.2528076171875\nStep: 14000\t eval loss: 2.7178955078125\t eval accuracy: 0.2471923828125\n##########################################################\nStep: 14100\t train loss: 2.6849989891052246\t train accuracy: 0.256103515625\nStep: 14100\t eval loss: 2.7037112712860107\t eval accuracy: 0.2425537109375\n##########################################################\nStep: 14200\t train loss: 2.6631195545196533\t train accuracy: 0.2620849609375\nStep: 14200\t eval loss: 2.705608606338501\t eval accuracy: 0.24853515625\n##########################################################\nStep: 14300\t train loss: 2.676705837249756\t train accuracy: 0.2537841796875\nStep: 14300\t eval loss: 2.716160774230957\t eval accuracy: 0.2415771484375\n##########################################################\nStep: 14400\t train loss: 2.6496376991271973\t train accuracy: 0.25634765625\nStep: 14400\t eval loss: 2.657214879989624\t eval accuracy: 0.25732421875\n##########################################################\nStep: 14500\t train loss: 2.6331396102905273\t train accuracy: 0.2606201171875\nStep: 14500\t eval loss: 2.627969980239868\t eval accuracy: 0.2601318359375\n##########################################################\nStep: 14600\t train loss: 2.625119209289551\t train accuracy: 0.2694091796875\nStep: 14600\t eval loss: 2.6463451385498047\t eval accuracy: 0.2510986328125\n##########################################################\nStep: 14700\t train loss: 2.688262462615967\t train accuracy: 0.255615234375\nStep: 14700\t eval loss: 2.6517837047576904\t eval accuracy: 0.2490234375\n##########################################################\nStep: 14800\t train loss: 2.619716167449951\t train accuracy: 0.2642822265625\nStep: 14800\t eval loss: 2.628211259841919\t eval accuracy: 0.2620849609375\n##########################################################\nStep: 14900\t train loss: 2.603424549102783\t train accuracy: 0.2650146484375\nStep: 14900\t eval loss: 2.6564242839813232\t eval accuracy: 0.2647705078125\n##########################################################\nStep: 15000\t train loss: 2.616107940673828\t train accuracy: 0.273681640625\nStep: 15000\t eval loss: 2.6167116165161133\t eval accuracy: 0.2705078125\n##########################################################\nStep: 15100\t train loss: 2.6062171459198\t train accuracy: 0.27001953125\nStep: 15100\t eval loss: 2.6284961700439453\t eval accuracy: 0.2625732421875\n##########################################################\nStep: 15200\t train loss: 2.5485188961029053\t train accuracy: 0.2860107421875\nStep: 15200\t eval loss: 2.577852725982666\t eval accuracy: 0.27294921875\n##########################################################\nStep: 15300\t train loss: 2.550502300262451\t train accuracy: 0.276611328125\nStep: 15300\t eval loss: 2.630084753036499\t eval accuracy: 0.2633056640625\n##########################################################\nStep: 15400\t train loss: 2.522005558013916\t train accuracy: 0.292236328125\nStep: 15400\t eval loss: 2.557337999343872\t eval accuracy: 0.27685546875\n##########################################################\nStep: 15500\t train loss: 2.4789650440216064\t train accuracy: 0.2978515625\nStep: 15500\t eval loss: 2.534942865371704\t eval accuracy: 0.2900390625\n##########################################################\nStep: 15600\t train loss: 2.518929958343506\t train accuracy: 0.2860107421875\nStep: 15600\t eval loss: 2.532789945602417\t eval accuracy: 0.291259765625\n##########################################################\nStep: 15700\t train loss: 2.4653241634368896\t train accuracy: 0.3048095703125\nStep: 15700\t eval loss: 2.5206453800201416\t eval accuracy: 0.296630859375\n##########################################################\nStep: 15800\t train loss: 2.471315622329712\t train accuracy: 0.3011474609375\nStep: 15800\t eval loss: 2.487138271331787\t eval accuracy: 0.296875\n##########################################################\nStep: 15900\t train loss: 2.445113182067871\t train accuracy: 0.3095703125\nStep: 15900\t eval loss: 2.4883804321289062\t eval accuracy: 0.2958984375\n##########################################################\nStep: 16000\t train loss: 2.442014455795288\t train accuracy: 0.3128662109375\nStep: 16000\t eval loss: 2.4832394123077393\t eval accuracy: 0.298583984375\n##########################################################\nStep: 16100\t train loss: 2.443204641342163\t train accuracy: 0.308837890625\nStep: 16100\t eval loss: 2.478264808654785\t eval accuracy: 0.2996826171875\n##########################################################\nStep: 16200\t train loss: 2.4129295349121094\t train accuracy: 0.3192138671875\nStep: 16200\t eval loss: 2.446596384048462\t eval accuracy: 0.3109130859375\n##########################################################\nStep: 16300\t train loss: 2.407557725906372\t train accuracy: 0.31640625\nStep: 16300\t eval loss: 2.449119806289673\t eval accuracy: 0.3052978515625\n##########################################################\nStep: 16400\t train loss: 2.420520782470703\t train accuracy: 0.326171875\nStep: 16400\t eval loss: 2.442570686340332\t eval accuracy: 0.31201171875\n##########################################################\nStep: 16500\t train loss: 2.4216842651367188\t train accuracy: 0.318603515625\nStep: 16500\t eval loss: 2.4740869998931885\t eval accuracy: 0.3050537109375\n##########################################################\nStep: 16600\t train loss: 2.4128048419952393\t train accuracy: 0.31494140625\nStep: 16600\t eval loss: 2.456963539123535\t eval accuracy: 0.3109130859375\n##########################################################\nStep: 16700\t train loss: 2.3850936889648438\t train accuracy: 0.326904296875\nStep: 16700\t eval loss: 2.420577049255371\t eval accuracy: 0.321533203125\n##########################################################\nStep: 16800\t train loss: 2.386648178100586\t train accuracy: 0.326904296875\nStep: 16800\t eval loss: 2.4092020988464355\t eval accuracy: 0.3206787109375\n##########################################################\nStep: 16900\t train loss: 2.3520467281341553\t train accuracy: 0.336669921875\nStep: 16900\t eval loss: 2.3670170307159424\t eval accuracy: 0.3275146484375\n##########################################################\nStep: 17000\t train loss: 2.3427274227142334\t train accuracy: 0.3343505859375\nStep: 17000\t eval loss: 2.363920211791992\t eval accuracy: 0.3277587890625\n##########################################################\nStep: 17100\t train loss: 2.3242852687835693\t train accuracy: 0.34130859375\nStep: 17100\t eval loss: 2.3542635440826416\t eval accuracy: 0.3297119140625\n##########################################################\nStep: 17200\t train loss: 2.310080051422119\t train accuracy: 0.33984375\nStep: 17200\t eval loss: 2.3491244316101074\t eval accuracy: 0.328369140625\n##########################################################\nStep: 17300\t train loss: 2.2997894287109375\t train accuracy: 0.3485107421875\nStep: 17300\t eval loss: 2.341343879699707\t eval accuracy: 0.333740234375\n##########################################################\nStep: 17400\t train loss: 2.2742371559143066\t train accuracy: 0.3533935546875\nStep: 17400\t eval loss: 2.319648027420044\t eval accuracy: 0.3411865234375\n##########################################################\nStep: 17500\t train loss: 2.3105132579803467\t train accuracy: 0.3358154296875\nStep: 17500\t eval loss: 2.3384265899658203\t eval accuracy: 0.3326416015625\n##########################################################\nStep: 17600\t train loss: 2.254833698272705\t train accuracy: 0.354736328125\nStep: 17600\t eval loss: 2.2890145778656006\t eval accuracy: 0.3529052734375\n##########################################################\nStep: 17700\t train loss: 2.2511448860168457\t train accuracy: 0.3563232421875\nStep: 17700\t eval loss: 2.2767810821533203\t eval accuracy: 0.353515625\n##########################################################\nStep: 17800\t train loss: 2.253283977508545\t train accuracy: 0.35791015625\nStep: 17800\t eval loss: 2.3111977577209473\t eval accuracy: 0.3399658203125\n##########################################################\nStep: 17900\t train loss: 2.2256758213043213\t train accuracy: 0.3702392578125\nStep: 17900\t eval loss: 2.2514398097991943\t eval accuracy: 0.358642578125\n##########################################################\nStep: 18000\t train loss: 2.273224353790283\t train accuracy: 0.3590087890625\nStep: 18000\t eval loss: 2.271130084991455\t eval accuracy: 0.3560791015625\n##########################################################\nStep: 18100\t train loss: 2.2139923572540283\t train accuracy: 0.3695068359375\nStep: 18100\t eval loss: 2.258417844772339\t eval accuracy: 0.346923828125\n##########################################################\nStep: 18200\t train loss: 2.219356060028076\t train accuracy: 0.36572265625\nStep: 18200\t eval loss: 2.2789509296417236\t eval accuracy: 0.35107421875\n##########################################################\nStep: 18300\t train loss: 2.2630693912506104\t train accuracy: 0.355712890625\nStep: 18300\t eval loss: 2.3167760372161865\t eval accuracy: 0.341552734375\n##########################################################\nStep: 18400\t train loss: 2.1896462440490723\t train accuracy: 0.3809814453125\nStep: 18400\t eval loss: 2.2795701026916504\t eval accuracy: 0.3631591796875\n##########################################################\nStep: 18500\t train loss: 2.2872848510742188\t train accuracy: 0.3463134765625\nStep: 18500\t eval loss: 2.3083250522613525\t eval accuracy: 0.34521484375\n##########################################################\nStep: 18600\t train loss: 2.2364845275878906\t train accuracy: 0.3681640625\nStep: 18600\t eval loss: 2.2799880504608154\t eval accuracy: 0.3599853515625\n##########################################################\nStep: 18700\t train loss: 2.278196334838867\t train accuracy: 0.3548583984375\nStep: 18700\t eval loss: 2.2832045555114746\t eval accuracy: 0.353271484375\n##########################################################\nStep: 18800\t train loss: 2.246030569076538\t train accuracy: 0.361083984375\nStep: 18800\t eval loss: 2.2992353439331055\t eval accuracy: 0.3433837890625\n##########################################################\nStep: 18900\t train loss: 2.244518995285034\t train accuracy: 0.3580322265625\nStep: 18900\t eval loss: 2.3226065635681152\t eval accuracy: 0.3433837890625\n##########################################################\nStep: 19000\t train loss: 2.234954357147217\t train accuracy: 0.3643798828125\nStep: 19000\t eval loss: 2.300145149230957\t eval accuracy: 0.3489990234375\n##########################################################\nStep: 19100\t train loss: 2.252653121948242\t train accuracy: 0.3658447265625\nStep: 19100\t eval loss: 2.2478299140930176\t eval accuracy: 0.3621826171875\n##########################################################\nStep: 19200\t train loss: 2.281946897506714\t train accuracy: 0.359130859375\nStep: 19200\t eval loss: 2.312242031097412\t eval accuracy: 0.34033203125\n##########################################################\nStep: 19300\t train loss: 2.239518165588379\t train accuracy: 0.3614501953125\nStep: 19300\t eval loss: 2.281362533569336\t eval accuracy: 0.354248046875\n##########################################################\nStep: 19400\t train loss: 2.2707087993621826\t train accuracy: 0.3621826171875\nStep: 19400\t eval loss: 2.2917063236236572\t eval accuracy: 0.3563232421875\n##########################################################\nStep: 19500\t train loss: 2.1999175548553467\t train accuracy: 0.3795166015625\nStep: 19500\t eval loss: 2.295689105987549\t eval accuracy: 0.3533935546875\n##########################################################\nStep: 19600\t train loss: 2.218500852584839\t train accuracy: 0.3702392578125\nStep: 19600\t eval loss: 2.2454938888549805\t eval accuracy: 0.3572998046875\n##########################################################\nStep: 19700\t train loss: 2.2319793701171875\t train accuracy: 0.3638916015625\nStep: 19700\t eval loss: 2.260158061981201\t eval accuracy: 0.35791015625\n##########################################################\nStep: 19800\t train loss: 2.265655517578125\t train accuracy: 0.3623046875\nStep: 19800\t eval loss: 2.288996696472168\t eval accuracy: 0.3525390625\n##########################################################\nStep: 19900\t train loss: 2.2192256450653076\t train accuracy: 0.3741455078125\nStep: 19900\t eval loss: 2.2733585834503174\t eval accuracy: 0.3592529296875\n##########################################################\nStep: 20000\t train loss: 2.2244691848754883\t train accuracy: 0.373779296875\nStep: 20000\t eval loss: 2.25819993019104\t eval accuracy: 0.367431640625\n##########################################################\nStep: 20100\t train loss: 2.1862707138061523\t train accuracy: 0.381591796875\nStep: 20100\t eval loss: 2.281035900115967\t eval accuracy: 0.36328125\n##########################################################\nStep: 20200\t train loss: 2.1874356269836426\t train accuracy: 0.37451171875\nStep: 20200\t eval loss: 2.253089666366577\t eval accuracy: 0.3607177734375\n##########################################################\nStep: 20300\t train loss: 2.21570086479187\t train accuracy: 0.3739013671875\nStep: 20300\t eval loss: 2.2457289695739746\t eval accuracy: 0.36474609375\n##########################################################\nStep: 20400\t train loss: 2.178753137588501\t train accuracy: 0.3841552734375\nStep: 20400\t eval loss: 2.2521681785583496\t eval accuracy: 0.3646240234375\n##########################################################\nStep: 20500\t train loss: 2.2174112796783447\t train accuracy: 0.3702392578125\nStep: 20500\t eval loss: 2.236659526824951\t eval accuracy: 0.3741455078125\n##########################################################\nStep: 20600\t train loss: 2.1350042819976807\t train accuracy: 0.3875732421875\nStep: 20600\t eval loss: 2.2208001613616943\t eval accuracy: 0.36767578125\n##########################################################\nStep: 20700\t train loss: 2.14643931388855\t train accuracy: 0.3858642578125\nStep: 20700\t eval loss: 2.215574264526367\t eval accuracy: 0.3648681640625\n##########################################################\nStep: 20800\t train loss: 2.1329262256622314\t train accuracy: 0.384033203125\nStep: 20800\t eval loss: 2.191854476928711\t eval accuracy: 0.3746337890625\n##########################################################\nStep: 20900\t train loss: 2.1736063957214355\t train accuracy: 0.3804931640625\nStep: 20900\t eval loss: 2.223062038421631\t eval accuracy: 0.368408203125\n##########################################################\nStep: 21000\t train loss: 2.113960027694702\t train accuracy: 0.39697265625\nStep: 21000\t eval loss: 2.2609877586364746\t eval accuracy: 0.3775634765625\n##########################################################\nStep: 21100\t train loss: 2.1570746898651123\t train accuracy: 0.382080078125\nStep: 21100\t eval loss: 2.209465742111206\t eval accuracy: 0.374267578125\n##########################################################\nStep: 21200\t train loss: 2.2004854679107666\t train accuracy: 0.3765869140625\nStep: 21200\t eval loss: 2.2902374267578125\t eval accuracy: 0.361083984375\n##########################################################\nStep: 21300\t train loss: 2.167006492614746\t train accuracy: 0.3826904296875\nStep: 21300\t eval loss: 2.2266652584075928\t eval accuracy: 0.3681640625\n##########################################################\nStep: 21400\t train loss: 2.1583168506622314\t train accuracy: 0.391845703125\nStep: 21400\t eval loss: 2.180089235305786\t eval accuracy: 0.376708984375\n##########################################################\nStep: 21500\t train loss: 2.1324574947357178\t train accuracy: 0.40185546875\nStep: 21500\t eval loss: 2.2010982036590576\t eval accuracy: 0.3701171875\n##########################################################\nStep: 21600\t train loss: 2.104530096054077\t train accuracy: 0.40234375\nStep: 21600\t eval loss: 2.163815498352051\t eval accuracy: 0.38720703125\n##########################################################\nStep: 21700\t train loss: 2.1250505447387695\t train accuracy: 0.392333984375\nStep: 21700\t eval loss: 2.164400100708008\t eval accuracy: 0.3837890625\n##########################################################\nStep: 21800\t train loss: 2.1286869049072266\t train accuracy: 0.396484375\nStep: 21800\t eval loss: 2.134413242340088\t eval accuracy: 0.39404296875\n##########################################################\nStep: 21900\t train loss: 2.0813632011413574\t train accuracy: 0.40234375\nStep: 21900\t eval loss: 2.153529644012451\t eval accuracy: 0.394287109375\n##########################################################\nStep: 22000\t train loss: 2.137753486633301\t train accuracy: 0.396484375\nStep: 22000\t eval loss: 2.188047170639038\t eval accuracy: 0.382080078125\n##########################################################\nStep: 22100\t train loss: 2.130117893218994\t train accuracy: 0.40283203125\nStep: 22100\t eval loss: 2.1655945777893066\t eval accuracy: 0.3897705078125\n##########################################################\nStep: 22200\t train loss: 2.0877511501312256\t train accuracy: 0.4058837890625\nStep: 22200\t eval loss: 2.1150972843170166\t eval accuracy: 0.404296875\n##########################################################\nStep: 22300\t train loss: 2.097832202911377\t train accuracy: 0.409423828125\nStep: 22300\t eval loss: 2.1008963584899902\t eval accuracy: 0.3994140625\n##########################################################\nStep: 22400\t train loss: 2.069139242172241\t train accuracy: 0.41650390625\nStep: 22400\t eval loss: 2.1451854705810547\t eval accuracy: 0.3958740234375\n##########################################################\nStep: 22500\t train loss: 2.1009769439697266\t train accuracy: 0.405029296875\nStep: 22500\t eval loss: 2.1541905403137207\t eval accuracy: 0.3870849609375\n##########################################################\nStep: 22600\t train loss: 2.1032845973968506\t train accuracy: 0.4049072265625\nStep: 22600\t eval loss: 2.1374564170837402\t eval accuracy: 0.4000244140625\n##########################################################\nStep: 22700\t train loss: 2.043839693069458\t train accuracy: 0.4168701171875\nStep: 22700\t eval loss: 2.1215286254882812\t eval accuracy: 0.401611328125\n##########################################################\nStep: 22800\t train loss: 2.1889171600341797\t train accuracy: 0.3720703125\nStep: 22800\t eval loss: 2.1960484981536865\t eval accuracy: 0.3720703125\n##########################################################\nStep: 22900\t train loss: 2.0553228855133057\t train accuracy: 0.4149169921875\nStep: 22900\t eval loss: 2.093975067138672\t eval accuracy: 0.4046630859375\n##########################################################\nStep: 23000\t train loss: 2.0133395195007324\t train accuracy: 0.4234619140625\nStep: 23000\t eval loss: 2.099794387817383\t eval accuracy: 0.4052734375\n##########################################################\nStep: 23100\t train loss: 2.064653158187866\t train accuracy: 0.413330078125\nStep: 23100\t eval loss: 2.1181857585906982\t eval accuracy: 0.405517578125\n##########################################################\nStep: 23200\t train loss: 2.0488932132720947\t train accuracy: 0.415771484375\nStep: 23200\t eval loss: 2.1063013076782227\t eval accuracy: 0.407470703125\n##########################################################\nStep: 23300\t train loss: 2.110934257507324\t train accuracy: 0.396728515625\nStep: 23300\t eval loss: 2.1803863048553467\t eval accuracy: 0.3780517578125\n##########################################################\nStep: 23400\t train loss: 2.2146944999694824\t train accuracy: 0.369384765625\nStep: 23400\t eval loss: 2.2480947971343994\t eval accuracy: 0.359619140625\n##########################################################\nStep: 23500\t train loss: 2.115715980529785\t train accuracy: 0.40380859375\nStep: 23500\t eval loss: 2.1735191345214844\t eval accuracy: 0.3839111328125\n##########################################################\nStep: 23600\t train loss: 2.1007134914398193\t train accuracy: 0.4034423828125\nStep: 23600\t eval loss: 2.1168062686920166\t eval accuracy: 0.39990234375\n##########################################################\nStep: 23700\t train loss: 2.098813056945801\t train accuracy: 0.410400390625\nStep: 23700\t eval loss: 2.1520702838897705\t eval accuracy: 0.3953857421875\n##########################################################\nStep: 23800\t train loss: 2.0524818897247314\t train accuracy: 0.4129638671875\nStep: 23800\t eval loss: 2.1173465251922607\t eval accuracy: 0.3995361328125\n##########################################################\nStep: 23900\t train loss: 2.0410664081573486\t train accuracy: 0.416015625\nStep: 23900\t eval loss: 2.0908377170562744\t eval accuracy: 0.4068603515625\n##########################################################\nStep: 24000\t train loss: 2.0339906215667725\t train accuracy: 0.420166015625\nStep: 24000\t eval loss: 2.0946366786956787\t eval accuracy: 0.40576171875\n##########################################################\nStep: 24100\t train loss: 2.0133893489837646\t train accuracy: 0.42041015625\nStep: 24100\t eval loss: 2.0918898582458496\t eval accuracy: 0.407958984375\n##########################################################\nStep: 24200\t train loss: 1.9946070909500122\t train accuracy: 0.4307861328125\nStep: 24200\t eval loss: 2.0871386528015137\t eval accuracy: 0.4083251953125\n##########################################################\nStep: 24300\t train loss: 2.034259796142578\t train accuracy: 0.4193115234375\nStep: 24300\t eval loss: 2.156494140625\t eval accuracy: 0.394287109375\n##########################################################\nStep: 24400\t train loss: 2.0390074253082275\t train accuracy: 0.4134521484375\nStep: 24400\t eval loss: 2.0695128440856934\t eval accuracy: 0.41064453125\n##########################################################\nStep: 24500\t train loss: 1.9800597429275513\t train accuracy: 0.4383544921875\nStep: 24500\t eval loss: 2.0743205547332764\t eval accuracy: 0.4168701171875\n##########################################################\nStep: 24600\t train loss: 1.9840067625045776\t train accuracy: 0.4327392578125\nStep: 24600\t eval loss: 2.083428144454956\t eval accuracy: 0.4061279296875\n##########################################################\nStep: 24700\t train loss: 2.0787370204925537\t train accuracy: 0.41552734375\nStep: 24700\t eval loss: 2.14449143409729\t eval accuracy: 0.39697265625\n##########################################################\nStep: 24800\t train loss: 2.054029703140259\t train accuracy: 0.4173583984375\nStep: 24800\t eval loss: 2.1038432121276855\t eval accuracy: 0.4024658203125\n##########################################################\nStep: 24900\t train loss: 2.0086593627929688\t train accuracy: 0.423828125\nStep: 24900\t eval loss: 2.069365978240967\t eval accuracy: 0.4100341796875\n##########################################################\nStep: 25000\t train loss: 1.9826081991195679\t train accuracy: 0.430908203125\nStep: 25000\t eval loss: 2.0897045135498047\t eval accuracy: 0.4112548828125\n##########################################################\nStep: 25100\t train loss: 1.982420802116394\t train accuracy: 0.4345703125\nStep: 25100\t eval loss: 2.0567166805267334\t eval accuracy: 0.42041015625\n##########################################################\nStep: 25200\t train loss: 2.03131103515625\t train accuracy: 0.4248046875\nStep: 25200\t eval loss: 2.0553789138793945\t eval accuracy: 0.4124755859375\n##########################################################\nStep: 25300\t train loss: 1.9977872371673584\t train accuracy: 0.4266357421875\nStep: 25300\t eval loss: 2.0254459381103516\t eval accuracy: 0.4197998046875\n##########################################################\nStep: 25400\t train loss: 2.020766019821167\t train accuracy: 0.4185791015625\nStep: 25400\t eval loss: 2.0404458045959473\t eval accuracy: 0.423828125\n##########################################################\nStep: 25500\t train loss: 1.9751572608947754\t train accuracy: 0.43505859375\nStep: 25500\t eval loss: 2.036661386489868\t eval accuracy: 0.4268798828125\n##########################################################\nStep: 25600\t train loss: 1.9521852731704712\t train accuracy: 0.4412841796875\nStep: 25600\t eval loss: 2.0066964626312256\t eval accuracy: 0.4287109375\n##########################################################\nStep: 25700\t train loss: 2.051851987838745\t train accuracy: 0.420166015625\nStep: 25700\t eval loss: 2.0346274375915527\t eval accuracy: 0.4195556640625\n##########################################################\nStep: 25800\t train loss: 1.9905049800872803\t train accuracy: 0.4337158203125\nStep: 25800\t eval loss: 2.040459632873535\t eval accuracy: 0.4185791015625\n##########################################################\nStep: 25900\t train loss: 1.951244831085205\t train accuracy: 0.4464111328125\nStep: 25900\t eval loss: 2.017213821411133\t eval accuracy: 0.426513671875\n##########################################################\nStep: 26000\t train loss: 1.9649927616119385\t train accuracy: 0.44189453125\nStep: 26000\t eval loss: 2.0427911281585693\t eval accuracy: 0.4193115234375\n##########################################################\nStep: 26100\t train loss: 1.9708179235458374\t train accuracy: 0.4356689453125\nStep: 26100\t eval loss: 2.0451242923736572\t eval accuracy: 0.421630859375\n##########################################################\nStep: 26200\t train loss: 2.017538070678711\t train accuracy: 0.423583984375\nStep: 26200\t eval loss: 2.063920736312866\t eval accuracy: 0.4163818359375\n##########################################################\nStep: 26300\t train loss: 2.0264627933502197\t train accuracy: 0.4288330078125\nStep: 26300\t eval loss: 2.070194721221924\t eval accuracy: 0.4127197265625\n##########################################################\nStep: 26400\t train loss: 1.9774798154830933\t train accuracy: 0.43798828125\nStep: 26400\t eval loss: 2.0060653686523438\t eval accuracy: 0.4288330078125\n##########################################################\nStep: 26500\t train loss: 1.958260416984558\t train accuracy: 0.444580078125\nStep: 26500\t eval loss: 1.999961495399475\t eval accuracy: 0.428955078125\n##########################################################\nStep: 26600\t train loss: 1.9440865516662598\t train accuracy: 0.4495849609375\nStep: 26600\t eval loss: 2.00547456741333\t eval accuracy: 0.4326171875\n##########################################################\nStep: 26700\t train loss: 1.9347429275512695\t train accuracy: 0.4461669921875\nStep: 26700\t eval loss: 2.0118515491485596\t eval accuracy: 0.427978515625\n##########################################################\nStep: 26800\t train loss: 1.9212961196899414\t train accuracy: 0.4598388671875\nStep: 26800\t eval loss: 1.999045491218567\t eval accuracy: 0.43505859375\n##########################################################\nStep: 26900\t train loss: 1.8943661451339722\t train accuracy: 0.4571533203125\nStep: 26900\t eval loss: 1.94314706325531\t eval accuracy: 0.4422607421875\n##########################################################\nStep: 27000\t train loss: 1.914570927619934\t train accuracy: 0.455322265625\nStep: 27000\t eval loss: 1.9747638702392578\t eval accuracy: 0.4395751953125\n##########################################################\nStep: 27100\t train loss: 1.863675832748413\t train accuracy: 0.458740234375\nStep: 27100\t eval loss: 2.0259556770324707\t eval accuracy: 0.42919921875\n##########################################################\nStep: 27200\t train loss: 1.9162352085113525\t train accuracy: 0.4510498046875\nStep: 27200\t eval loss: 1.9552266597747803\t eval accuracy: 0.447265625\n##########################################################\nStep: 27300\t train loss: 1.9458812475204468\t train accuracy: 0.4422607421875\nStep: 27300\t eval loss: 1.9826544523239136\t eval accuracy: 0.4383544921875\n##########################################################\nStep: 27400\t train loss: 1.9015082120895386\t train accuracy: 0.4603271484375\nStep: 27400\t eval loss: 1.964355707168579\t eval accuracy: 0.4505615234375\n##########################################################\nStep: 27500\t train loss: 1.8518236875534058\t train accuracy: 0.4722900390625\nStep: 27500\t eval loss: 1.9603116512298584\t eval accuracy: 0.4442138671875\n##########################################################\nStep: 27600\t train loss: 1.908002257347107\t train accuracy: 0.4617919921875\nStep: 27600\t eval loss: 1.9584053754806519\t eval accuracy: 0.44140625\n##########################################################\nStep: 27700\t train loss: 1.8835355043411255\t train accuracy: 0.4688720703125\nStep: 27700\t eval loss: 1.9265117645263672\t eval accuracy: 0.4471435546875\n##########################################################\nStep: 27800\t train loss: 1.947632074356079\t train accuracy: 0.44775390625\nStep: 27800\t eval loss: 1.9913114309310913\t eval accuracy: 0.4412841796875\n##########################################################\nStep: 27900\t train loss: 1.8656227588653564\t train accuracy: 0.466552734375\nStep: 27900\t eval loss: 1.954951524734497\t eval accuracy: 0.4478759765625\n##########################################################\nStep: 28000\t train loss: 1.9106606245040894\t train accuracy: 0.4517822265625\nStep: 28000\t eval loss: 1.9577906131744385\t eval accuracy: 0.44580078125\n##########################################################\nStep: 28100\t train loss: 2.0179762840270996\t train accuracy: 0.4310302734375\nStep: 28100\t eval loss: 2.0517587661743164\t eval accuracy: 0.41748046875\n##########################################################\nStep: 28200\t train loss: 1.9008394479751587\t train accuracy: 0.4527587890625\nStep: 28200\t eval loss: 1.976239562034607\t eval accuracy: 0.434326171875\n##########################################################\nStep: 28300\t train loss: 1.9084359407424927\t train accuracy: 0.4522705078125\nStep: 28300\t eval loss: 1.9829446077346802\t eval accuracy: 0.4466552734375\n##########################################################\nStep: 28400\t train loss: 1.8913196325302124\t train accuracy: 0.459716796875\nStep: 28400\t eval loss: 1.926166296005249\t eval accuracy: 0.45263671875\n##########################################################\nStep: 28500\t train loss: 1.9767754077911377\t train accuracy: 0.43603515625\nStep: 28500\t eval loss: 2.0282249450683594\t eval accuracy: 0.4215087890625\n##########################################################\nStep: 28600\t train loss: 1.9221161603927612\t train accuracy: 0.4449462890625\nStep: 28600\t eval loss: 1.9813103675842285\t eval accuracy: 0.4342041015625\n##########################################################\nStep: 28700\t train loss: 1.991527795791626\t train accuracy: 0.43017578125\nStep: 28700\t eval loss: 2.007418632507324\t eval accuracy: 0.41845703125\n##########################################################\nStep: 28800\t train loss: 1.9018001556396484\t train accuracy: 0.4544677734375\nStep: 28800\t eval loss: 1.9617266654968262\t eval accuracy: 0.445068359375\n##########################################################\nStep: 28900\t train loss: 1.883736252784729\t train accuracy: 0.46044921875\nStep: 28900\t eval loss: 1.9400198459625244\t eval accuracy: 0.4443359375\n##########################################################\nStep: 29000\t train loss: 1.9309533834457397\t train accuracy: 0.4471435546875\nStep: 29000\t eval loss: 1.9499962329864502\t eval accuracy: 0.442626953125\n##########################################################\nStep: 29100\t train loss: 1.9039191007614136\t train accuracy: 0.4586181640625\nStep: 29100\t eval loss: 1.9801394939422607\t eval accuracy: 0.44091796875\n##########################################################\nStep: 29200\t train loss: 1.8624604940414429\t train accuracy: 0.4727783203125\nStep: 29200\t eval loss: 1.927086353302002\t eval accuracy: 0.4527587890625\n##########################################################\nStep: 29300\t train loss: 1.8795862197875977\t train accuracy: 0.4739990234375\nStep: 29300\t eval loss: 1.9670591354370117\t eval accuracy: 0.44384765625\n##########################################################\nStep: 29400\t train loss: 1.8766388893127441\t train accuracy: 0.457275390625\nStep: 29400\t eval loss: 1.89895761013031\t eval accuracy: 0.4576416015625\n##########################################################\nStep: 29500\t train loss: 1.8753941059112549\t train accuracy: 0.468505859375\nStep: 29500\t eval loss: 1.9298630952835083\t eval accuracy: 0.450927734375\n##########################################################\nStep: 29600\t train loss: 1.8665295839309692\t train accuracy: 0.4686279296875\nStep: 29600\t eval loss: 1.946884036064148\t eval accuracy: 0.4530029296875\n##########################################################\nStep: 29700\t train loss: 1.855433464050293\t train accuracy: 0.463134765625\nStep: 29700\t eval loss: 1.9049997329711914\t eval accuracy: 0.4532470703125\n##########################################################\nStep: 29800\t train loss: 1.8663580417633057\t train accuracy: 0.46337890625\nStep: 29800\t eval loss: 1.9350786209106445\t eval accuracy: 0.454345703125\n##########################################################\nStep: 29900\t train loss: 1.8488528728485107\t train accuracy: 0.474853515625\nStep: 29900\t eval loss: 1.934653401374817\t eval accuracy: 0.4501953125\n##########################################################\nStep: 30000\t train loss: 1.8453466892242432\t train accuracy: 0.47607421875\nStep: 30000\t eval loss: 1.8915311098098755\t eval accuracy: 0.463623046875\n##########################################################\nStep: 30100\t train loss: 1.8251440525054932\t train accuracy: 0.4793701171875\nStep: 30100\t eval loss: 1.9137463569641113\t eval accuracy: 0.460205078125\n##########################################################\nStep: 30200\t train loss: 1.853835105895996\t train accuracy: 0.47265625\nStep: 30200\t eval loss: 1.9305871725082397\t eval accuracy: 0.4552001953125\n##########################################################\nStep: 30300\t train loss: 1.8628675937652588\t train accuracy: 0.4698486328125\nStep: 30300\t eval loss: 1.9077036380767822\t eval accuracy: 0.4644775390625\n##########################################################\nStep: 30400\t train loss: 1.8795335292816162\t train accuracy: 0.4637451171875\nStep: 30400\t eval loss: 1.9445750713348389\t eval accuracy: 0.451904296875\n##########################################################\nStep: 30500\t train loss: 1.8608282804489136\t train accuracy: 0.469970703125\nStep: 30500\t eval loss: 1.931609869003296\t eval accuracy: 0.4510498046875\n##########################################################\nStep: 30600\t train loss: 1.8341877460479736\t train accuracy: 0.480224609375\nStep: 30600\t eval loss: 1.8766696453094482\t eval accuracy: 0.469970703125\n##########################################################\nStep: 30700\t train loss: 1.8614766597747803\t train accuracy: 0.472412109375\nStep: 30700\t eval loss: 1.8780033588409424\t eval accuracy: 0.4578857421875\n##########################################################\nStep: 30800\t train loss: 1.8146134614944458\t train accuracy: 0.47021484375\nStep: 30800\t eval loss: 1.8823001384735107\t eval accuracy: 0.4654541015625\n##########################################################\nStep: 30900\t train loss: 1.8186085224151611\t train accuracy: 0.4803466796875\nStep: 30900\t eval loss: 1.8544762134552002\t eval accuracy: 0.474609375\n##########################################################\nStep: 31000\t train loss: 1.830564022064209\t train accuracy: 0.4761962890625\nStep: 31000\t eval loss: 1.903601050376892\t eval accuracy: 0.45556640625\n##########################################################\nStep: 31100\t train loss: 1.8278803825378418\t train accuracy: 0.4765625\nStep: 31100\t eval loss: 2.0069358348846436\t eval accuracy: 0.4451904296875\n##########################################################\nStep: 31200\t train loss: 1.7956660985946655\t train accuracy: 0.485595703125\nStep: 31200\t eval loss: 1.8580069541931152\t eval accuracy: 0.468017578125\n##########################################################\nStep: 31300\t train loss: 1.7837716341018677\t train accuracy: 0.4879150390625\nStep: 31300\t eval loss: 1.8380067348480225\t eval accuracy: 0.47119140625\n##########################################################\nStep: 31400\t train loss: 1.7772173881530762\t train accuracy: 0.491943359375\nStep: 31400\t eval loss: 1.8347634077072144\t eval accuracy: 0.471435546875\n##########################################################\nStep: 31500\t train loss: 1.7868130207061768\t train accuracy: 0.4847412109375\nStep: 31500\t eval loss: 1.8674730062484741\t eval accuracy: 0.4710693359375\n##########################################################\nStep: 31600\t train loss: 1.7874250411987305\t train accuracy: 0.4931640625\nStep: 31600\t eval loss: 1.8371829986572266\t eval accuracy: 0.47509765625\n##########################################################\nStep: 31700\t train loss: 1.7682405710220337\t train accuracy: 0.499267578125\nStep: 31700\t eval loss: 1.8655600547790527\t eval accuracy: 0.4759521484375\n##########################################################\nStep: 31800\t train loss: 1.7706820964813232\t train accuracy: 0.49755859375\nStep: 31800\t eval loss: 1.8183174133300781\t eval accuracy: 0.48291015625\n##########################################################\nStep: 31900\t train loss: 1.7784085273742676\t train accuracy: 0.4853515625\nStep: 31900\t eval loss: 1.8511275053024292\t eval accuracy: 0.4698486328125\n##########################################################\nStep: 32000\t train loss: 1.776881217956543\t train accuracy: 0.48876953125\nStep: 32000\t eval loss: 1.8430132865905762\t eval accuracy: 0.470458984375\n##########################################################\nStep: 32100\t train loss: 1.8716667890548706\t train accuracy: 0.4609375\nStep: 32100\t eval loss: 1.9234591722488403\t eval accuracy: 0.450927734375\n##########################################################\nStep: 32200\t train loss: 1.7934085130691528\t train accuracy: 0.4844970703125\nStep: 32200\t eval loss: 1.8638664484024048\t eval accuracy: 0.466064453125\n##########################################################\nStep: 32300\t train loss: 1.795966625213623\t train accuracy: 0.487548828125\nStep: 32300\t eval loss: 1.8233481645584106\t eval accuracy: 0.4786376953125\n##########################################################\nStep: 32400\t train loss: 1.775412917137146\t train accuracy: 0.4852294921875\nStep: 32400\t eval loss: 1.8590433597564697\t eval accuracy: 0.4715576171875\n##########################################################\nStep: 32500\t train loss: 1.8020027875900269\t train accuracy: 0.48583984375\nStep: 32500\t eval loss: 1.8720881938934326\t eval accuracy: 0.4676513671875\n##########################################################\nStep: 32600\t train loss: 1.7976288795471191\t train accuracy: 0.4837646484375\nStep: 32600\t eval loss: 1.8648446798324585\t eval accuracy: 0.466796875\n##########################################################\nStep: 32700\t train loss: 1.7908629179000854\t train accuracy: 0.48779296875\nStep: 32700\t eval loss: 1.8258512020111084\t eval accuracy: 0.4798583984375\n##########################################################\nStep: 32800\t train loss: 1.750261902809143\t train accuracy: 0.4923095703125\nStep: 32800\t eval loss: 1.8240294456481934\t eval accuracy: 0.4793701171875\n##########################################################\nStep: 32900\t train loss: 1.7621124982833862\t train accuracy: 0.4970703125\nStep: 32900\t eval loss: 1.8180463314056396\t eval accuracy: 0.4854736328125\n##########################################################\nStep: 33000\t train loss: 1.7461509704589844\t train accuracy: 0.5018310546875\nStep: 33000\t eval loss: 1.8315991163253784\t eval accuracy: 0.475341796875\n##########################################################\nStep: 33100\t train loss: 1.7431005239486694\t train accuracy: 0.5030517578125\nStep: 33100\t eval loss: 1.8538904190063477\t eval accuracy: 0.4718017578125\n##########################################################\nStep: 33200\t train loss: 1.7720553874969482\t train accuracy: 0.4971923828125\nStep: 33200\t eval loss: 1.8137855529785156\t eval accuracy: 0.4837646484375\n##########################################################\nStep: 33300\t train loss: 1.7612862586975098\t train accuracy: 0.4952392578125\nStep: 33300\t eval loss: 1.8053592443466187\t eval accuracy: 0.487548828125\n##########################################################\nStep: 33400\t train loss: 1.7557791471481323\t train accuracy: 0.4969482421875\nStep: 33400\t eval loss: 1.8028440475463867\t eval accuracy: 0.4793701171875\n##########################################################\nStep: 33500\t train loss: 1.7470062971115112\t train accuracy: 0.497314453125\nStep: 33500\t eval loss: 1.7750805616378784\t eval accuracy: 0.4906005859375\n##########################################################\nStep: 33600\t train loss: 1.7221577167510986\t train accuracy: 0.5115966796875\nStep: 33600\t eval loss: 1.7646963596343994\t eval accuracy: 0.5052490234375\n##########################################################\nStep: 33700\t train loss: 1.7537461519241333\t train accuracy: 0.50048828125\nStep: 33700\t eval loss: 1.7488505840301514\t eval accuracy: 0.5020751953125\n##########################################################\nStep: 33800\t train loss: 1.7619799375534058\t train accuracy: 0.4932861328125\nStep: 33800\t eval loss: 1.820318579673767\t eval accuracy: 0.4923095703125\n##########################################################\nStep: 33900\t train loss: 1.6886273622512817\t train accuracy: 0.518310546875\nStep: 33900\t eval loss: 1.7842693328857422\t eval accuracy: 0.497314453125\n##########################################################\nStep: 34000\t train loss: 1.7223953008651733\t train accuracy: 0.510986328125\nStep: 34000\t eval loss: 1.8078498840332031\t eval accuracy: 0.4901123046875\n##########################################################\nStep: 34100\t train loss: 1.7652167081832886\t train accuracy: 0.496337890625\nStep: 34100\t eval loss: 1.8163840770721436\t eval accuracy: 0.487060546875\n##########################################################\nStep: 34200\t train loss: 1.7044661045074463\t train accuracy: 0.5081787109375\nStep: 34200\t eval loss: 1.7891085147857666\t eval accuracy: 0.494140625\n##########################################################\nStep: 34300\t train loss: 1.8628547191619873\t train accuracy: 0.470703125\nStep: 34300\t eval loss: 1.8992934226989746\t eval accuracy: 0.4630126953125\n##########################################################\nStep: 34400\t train loss: 1.709611415863037\t train accuracy: 0.5126953125\nStep: 34400\t eval loss: 1.7965853214263916\t eval accuracy: 0.4866943359375\n##########################################################\nStep: 34500\t train loss: 1.6999894380569458\t train accuracy: 0.5106201171875\nStep: 34500\t eval loss: 1.7940218448638916\t eval accuracy: 0.49267578125\n##########################################################\nStep: 34600\t train loss: 1.7013306617736816\t train accuracy: 0.5064697265625\nStep: 34600\t eval loss: 1.8046672344207764\t eval accuracy: 0.4847412109375\n##########################################################\nStep: 34700\t train loss: 1.7036628723144531\t train accuracy: 0.5125732421875\nStep: 34700\t eval loss: 1.7776037454605103\t eval accuracy: 0.4925537109375\n##########################################################\nStep: 34800\t train loss: 1.7224109172821045\t train accuracy: 0.5067138671875\nStep: 34800\t eval loss: 1.7832977771759033\t eval accuracy: 0.4893798828125\n##########################################################\nStep: 34900\t train loss: 1.7857892513275146\t train accuracy: 0.4815673828125\nStep: 34900\t eval loss: 1.8061636686325073\t eval accuracy: 0.487060546875\n##########################################################\nStep: 35000\t train loss: 1.7444267272949219\t train accuracy: 0.508056640625\nStep: 35000\t eval loss: 1.7892276048660278\t eval accuracy: 0.4959716796875\n##########################################################\nStep: 35100\t train loss: 1.7025017738342285\t train accuracy: 0.513427734375\nStep: 35100\t eval loss: 1.799222469329834\t eval accuracy: 0.4969482421875\n##########################################################\nStep: 35200\t train loss: 1.6770070791244507\t train accuracy: 0.5213623046875\nStep: 35200\t eval loss: 1.7677028179168701\t eval accuracy: 0.5010986328125\n##########################################################\nStep: 35300\t train loss: 1.7342896461486816\t train accuracy: 0.50341796875\nStep: 35300\t eval loss: 1.8031126260757446\t eval accuracy: 0.4874267578125\n##########################################################\nStep: 35400\t train loss: 1.6887807846069336\t train accuracy: 0.51953125\nStep: 35400\t eval loss: 1.7576751708984375\t eval accuracy: 0.5045166015625\n##########################################################\nStep: 35500\t train loss: 1.6717503070831299\t train accuracy: 0.5224609375\nStep: 35500\t eval loss: 1.7623682022094727\t eval accuracy: 0.49560546875\n##########################################################\nStep: 35600\t train loss: 1.6982883214950562\t train accuracy: 0.509765625\nStep: 35600\t eval loss: 1.7804235219955444\t eval accuracy: 0.4912109375\n##########################################################\nStep: 35700\t train loss: 1.7555934190750122\t train accuracy: 0.491455078125\nStep: 35700\t eval loss: 1.8042454719543457\t eval accuracy: 0.4871826171875\n##########################################################\nStep: 35800\t train loss: 1.6670715808868408\t train accuracy: 0.5201416015625\nStep: 35800\t eval loss: 1.7546465396881104\t eval accuracy: 0.503662109375\n##########################################################\nStep: 35900\t train loss: 1.6611170768737793\t train accuracy: 0.528076171875\nStep: 35900\t eval loss: 1.7759432792663574\t eval accuracy: 0.502197265625\n##########################################################\nStep: 36000\t train loss: 1.7636330127716064\t train accuracy: 0.49609375\nStep: 36000\t eval loss: 1.8378794193267822\t eval accuracy: 0.47998046875\n##########################################################\nStep: 36100\t train loss: 1.6655237674713135\t train accuracy: 0.5203857421875\nStep: 36100\t eval loss: 1.7606327533721924\t eval accuracy: 0.49951171875\n##########################################################\nStep: 36200\t train loss: 1.6661782264709473\t train accuracy: 0.52734375\nStep: 36200\t eval loss: 1.7326889038085938\t eval accuracy: 0.513427734375\n##########################################################\nStep: 36300\t train loss: 1.660111427307129\t train accuracy: 0.52294921875\nStep: 36300\t eval loss: 1.714694619178772\t eval accuracy: 0.5155029296875\n##########################################################\nStep: 36400\t train loss: 1.7103817462921143\t train accuracy: 0.5118408203125\nStep: 36400\t eval loss: 1.7379896640777588\t eval accuracy: 0.5054931640625\n##########################################################\nStep: 36500\t train loss: 1.6876344680786133\t train accuracy: 0.5169677734375\nStep: 36500\t eval loss: 1.7594425678253174\t eval accuracy: 0.4993896484375\n##########################################################\nStep: 36600\t train loss: 1.7017626762390137\t train accuracy: 0.51318359375\nStep: 36600\t eval loss: 1.780872106552124\t eval accuracy: 0.48876953125\n##########################################################\nStep: 36700\t train loss: 1.6728994846343994\t train accuracy: 0.52734375\nStep: 36700\t eval loss: 1.7272439002990723\t eval accuracy: 0.506103515625\n##########################################################\nStep: 36800\t train loss: 1.658432960510254\t train accuracy: 0.52880859375\nStep: 36800\t eval loss: 1.7616769075393677\t eval accuracy: 0.5037841796875\n##########################################################\nStep: 36900\t train loss: 1.6603971719741821\t train accuracy: 0.52880859375\nStep: 36900\t eval loss: 1.744225263595581\t eval accuracy: 0.505859375\n##########################################################\nStep: 37000\t train loss: 1.6733121871948242\t train accuracy: 0.516845703125\nStep: 37000\t eval loss: 1.7125191688537598\t eval accuracy: 0.5172119140625\n##########################################################\nStep: 37100\t train loss: 1.6833845376968384\t train accuracy: 0.5184326171875\nStep: 37100\t eval loss: 1.7695519924163818\t eval accuracy: 0.494140625\n##########################################################\nStep: 37200\t train loss: 1.6472781896591187\t train accuracy: 0.5281982421875\nStep: 37200\t eval loss: 1.7483187913894653\t eval accuracy: 0.5072021484375\n##########################################################\nStep: 37300\t train loss: 1.7296125888824463\t train accuracy: 0.50146484375\nStep: 37300\t eval loss: 1.7713993787765503\t eval accuracy: 0.5001220703125\n##########################################################\nStep: 37400\t train loss: 1.6305747032165527\t train accuracy: 0.5333251953125\nStep: 37400\t eval loss: 1.688743233680725\t eval accuracy: 0.517578125\n##########################################################\nStep: 37500\t train loss: 1.6542322635650635\t train accuracy: 0.53076171875\nStep: 37500\t eval loss: 1.7209672927856445\t eval accuracy: 0.5107421875\n##########################################################\nStep: 37600\t train loss: 1.6316766738891602\t train accuracy: 0.5323486328125\nStep: 37600\t eval loss: 1.7600822448730469\t eval accuracy: 0.5052490234375\n##########################################################\nStep: 37700\t train loss: 1.7108075618743896\t train accuracy: 0.5123291015625\nStep: 37700\t eval loss: 1.761409878730774\t eval accuracy: 0.4993896484375\n##########################################################\nStep: 37800\t train loss: 1.6578001976013184\t train accuracy: 0.5257568359375\nStep: 37800\t eval loss: 1.7216099500656128\t eval accuracy: 0.5072021484375\n##########################################################\nStep: 37900\t train loss: 1.6176031827926636\t train accuracy: 0.5343017578125\nStep: 37900\t eval loss: 1.7014909982681274\t eval accuracy: 0.5169677734375\n##########################################################\nStep: 38000\t train loss: 1.6330817937850952\t train accuracy: 0.53955078125\nStep: 38000\t eval loss: 1.7104735374450684\t eval accuracy: 0.50830078125\n##########################################################\nStep: 38100\t train loss: 1.640483021736145\t train accuracy: 0.52783203125\nStep: 38100\t eval loss: 1.6797457933425903\t eval accuracy: 0.5201416015625\n##########################################################\nStep: 38200\t train loss: 1.6545541286468506\t train accuracy: 0.5194091796875\nStep: 38200\t eval loss: 1.8166825771331787\t eval accuracy: 0.4912109375\n##########################################################\nStep: 38300\t train loss: 1.6385226249694824\t train accuracy: 0.5272216796875\nStep: 38300\t eval loss: 1.6866387128829956\t eval accuracy: 0.518310546875\n##########################################################\nStep: 38400\t train loss: 1.6698108911514282\t train accuracy: 0.5250244140625\nStep: 38400\t eval loss: 1.7492740154266357\t eval accuracy: 0.513916015625\n##########################################################\nStep: 38500\t train loss: 1.6134142875671387\t train accuracy: 0.53759765625\nStep: 38500\t eval loss: 1.686100959777832\t eval accuracy: 0.5205078125\n##########################################################\nStep: 38600\t train loss: 1.6777281761169434\t train accuracy: 0.5167236328125\nStep: 38600\t eval loss: 1.750156044960022\t eval accuracy: 0.506103515625\n##########################################################\nStep: 38700\t train loss: 1.6188499927520752\t train accuracy: 0.5401611328125\nStep: 38700\t eval loss: 1.6861947774887085\t eval accuracy: 0.5206298828125\n##########################################################\nStep: 38800\t train loss: 1.6116564273834229\t train accuracy: 0.536865234375\nStep: 38800\t eval loss: 1.7531347274780273\t eval accuracy: 0.5035400390625\n##########################################################\nStep: 38900\t train loss: 1.611220359802246\t train accuracy: 0.54345703125\nStep: 38900\t eval loss: 1.691058874130249\t eval accuracy: 0.5218505859375\n##########################################################\nStep: 39000\t train loss: 1.611780047416687\t train accuracy: 0.5352783203125\nStep: 39000\t eval loss: 1.6904510259628296\t eval accuracy: 0.514404296875\n##########################################################\nStep: 39100\t train loss: 1.5915780067443848\t train accuracy: 0.544921875\nStep: 39100\t eval loss: 1.6605308055877686\t eval accuracy: 0.5220947265625\n##########################################################\nStep: 39200\t train loss: 1.59492027759552\t train accuracy: 0.5418701171875\nStep: 39200\t eval loss: 1.6684908866882324\t eval accuracy: 0.5286865234375\n##########################################################\nStep: 39300\t train loss: 1.621289610862732\t train accuracy: 0.537841796875\nStep: 39300\t eval loss: 1.677239179611206\t eval accuracy: 0.526123046875\n##########################################################\nStep: 39400\t train loss: 1.5835390090942383\t train accuracy: 0.5435791015625\nStep: 39400\t eval loss: 1.692569613456726\t eval accuracy: 0.5208740234375\n##########################################################\nStep: 39500\t train loss: 1.5955619812011719\t train accuracy: 0.5435791015625\nStep: 39500\t eval loss: 1.6764765977859497\t eval accuracy: 0.5198974609375\n##########################################################\nStep: 39600\t train loss: 1.6585097312927246\t train accuracy: 0.527099609375\nStep: 39600\t eval loss: 1.7269976139068604\t eval accuracy: 0.50830078125\n##########################################################\nStep: 39700\t train loss: 1.5947275161743164\t train accuracy: 0.539306640625\nStep: 39700\t eval loss: 1.6582601070404053\t eval accuracy: 0.529541015625\n##########################################################\nStep: 39800\t train loss: 1.5759762525558472\t train accuracy: 0.5516357421875\nStep: 39800\t eval loss: 1.6765735149383545\t eval accuracy: 0.5186767578125\n##########################################################\nStep: 39900\t train loss: 1.5895031690597534\t train accuracy: 0.542724609375\nStep: 39900\t eval loss: 1.7275216579437256\t eval accuracy: 0.5072021484375\n##########################################################\nStep: 40000\t train loss: 1.6140817403793335\t train accuracy: 0.540771484375\nStep: 40000\t eval loss: 1.742464303970337\t eval accuracy: 0.509033203125\n##########################################################\nStep: 40100\t train loss: 1.5787439346313477\t train accuracy: 0.5521240234375\nStep: 40100\t eval loss: 1.7001581192016602\t eval accuracy: 0.520751953125\n##########################################################\nStep: 40200\t train loss: 1.6448369026184082\t train accuracy: 0.534423828125\nStep: 40200\t eval loss: 1.6652195453643799\t eval accuracy: 0.52001953125\n##########################################################\nStep: 40300\t train loss: 1.6035056114196777\t train accuracy: 0.5400390625\nStep: 40300\t eval loss: 1.6980977058410645\t eval accuracy: 0.5230712890625\n##########################################################\nStep: 40400\t train loss: 1.668939232826233\t train accuracy: 0.524169921875\nStep: 40400\t eval loss: 1.7047797441482544\t eval accuracy: 0.5196533203125\n##########################################################\nStep: 40500\t train loss: 1.6000322103500366\t train accuracy: 0.5467529296875\nStep: 40500\t eval loss: 1.6858969926834106\t eval accuracy: 0.5247802734375\n##########################################################\nStep: 40600\t train loss: 1.6219441890716553\t train accuracy: 0.5303955078125\nStep: 40600\t eval loss: 1.892398476600647\t eval accuracy: 0.510986328125\n##########################################################\nStep: 40700\t train loss: 1.7234281301498413\t train accuracy: 0.5091552734375\nStep: 40700\t eval loss: 1.8740127086639404\t eval accuracy: 0.473876953125\n##########################################################\nStep: 40800\t train loss: 1.6455299854278564\t train accuracy: 0.5267333984375\nStep: 40800\t eval loss: 1.711808204650879\t eval accuracy: 0.51123046875\n##########################################################\nStep: 40900\t train loss: 1.6647664308547974\t train accuracy: 0.522705078125\nStep: 40900\t eval loss: 1.7628871202468872\t eval accuracy: 0.49658203125\n##########################################################\nStep: 41000\t train loss: 1.7463899850845337\t train accuracy: 0.5028076171875\nStep: 41000\t eval loss: 1.816336750984192\t eval accuracy: 0.4886474609375\n##########################################################\nStep: 41100\t train loss: 1.7061229944229126\t train accuracy: 0.510009765625\nStep: 41100\t eval loss: 1.7592248916625977\t eval accuracy: 0.50830078125\n##########################################################\nStep: 41200\t train loss: 1.6739569902420044\t train accuracy: 0.5206298828125\nStep: 41200\t eval loss: 1.7425044775009155\t eval accuracy: 0.5050048828125\n##########################################################\nStep: 41300\t train loss: 1.6057519912719727\t train accuracy: 0.5430908203125\nStep: 41300\t eval loss: 1.730734944343567\t eval accuracy: 0.5040283203125\n##########################################################\nStep: 41400\t train loss: 1.678490161895752\t train accuracy: 0.5150146484375\nStep: 41400\t eval loss: 1.782718539237976\t eval accuracy: 0.4935302734375\n##########################################################\nStep: 41500\t train loss: 1.6288790702819824\t train accuracy: 0.529541015625\nStep: 41500\t eval loss: 1.6963673830032349\t eval accuracy: 0.520263671875\n##########################################################\nStep: 41600\t train loss: 1.6147392988204956\t train accuracy: 0.5404052734375\nStep: 41600\t eval loss: 1.6712781190872192\t eval accuracy: 0.5313720703125\n##########################################################\nStep: 41700\t train loss: 1.6763732433319092\t train accuracy: 0.5206298828125\nStep: 41700\t eval loss: 1.6823325157165527\t eval accuracy: 0.5133056640625\n##########################################################\nStep: 41800\t train loss: 1.5948792695999146\t train accuracy: 0.54248046875\nStep: 41800\t eval loss: 1.6965266466140747\t eval accuracy: 0.52099609375\n##########################################################\nStep: 41900\t train loss: 1.5781705379486084\t train accuracy: 0.5433349609375\nStep: 41900\t eval loss: 1.6452771425247192\t eval accuracy: 0.53369140625\n##########################################################\nStep: 42000\t train loss: 1.5534638166427612\t train accuracy: 0.5570068359375\nStep: 42000\t eval loss: 1.661152958869934\t eval accuracy: 0.52685546875\n##########################################################\nStep: 42100\t train loss: 1.636970043182373\t train accuracy: 0.5382080078125\nStep: 42100\t eval loss: 1.7032603025436401\t eval accuracy: 0.517822265625\n##########################################################\nStep: 42200\t train loss: 1.5636454820632935\t train accuracy: 0.552490234375\nStep: 42200\t eval loss: 1.6331787109375\t eval accuracy: 0.5390625\n##########################################################\nStep: 42300\t train loss: 1.5874700546264648\t train accuracy: 0.548095703125\nStep: 42300\t eval loss: 1.6394368410110474\t eval accuracy: 0.53515625\n##########################################################\nStep: 42400\t train loss: 1.5867029428482056\t train accuracy: 0.548583984375\nStep: 42400\t eval loss: 1.6181600093841553\t eval accuracy: 0.5401611328125\n##########################################################\nStep: 42500\t train loss: 1.5411752462387085\t train accuracy: 0.5511474609375\nStep: 42500\t eval loss: 1.6387965679168701\t eval accuracy: 0.532958984375\n##########################################################\nStep: 42600\t train loss: 1.5536088943481445\t train accuracy: 0.5572509765625\nStep: 42600\t eval loss: 1.6576358079910278\t eval accuracy: 0.5302734375\n##########################################################\nStep: 42700\t train loss: 1.5785013437271118\t train accuracy: 0.5445556640625\nStep: 42700\t eval loss: 1.6596623659133911\t eval accuracy: 0.5244140625\n##########################################################\nStep: 42800\t train loss: 1.6028841733932495\t train accuracy: 0.5377197265625\nStep: 42800\t eval loss: 1.6985777616500854\t eval accuracy: 0.519287109375\n##########################################################\nStep: 42900\t train loss: 1.5903136730194092\t train accuracy: 0.5419921875\nStep: 42900\t eval loss: 1.68589186668396\t eval accuracy: 0.519775390625\n##########################################################\nStep: 43000\t train loss: 1.599408507347107\t train accuracy: 0.5452880859375\nStep: 43000\t eval loss: 1.6908901929855347\t eval accuracy: 0.5205078125\n##########################################################\nStep: 43100\t train loss: 1.593106746673584\t train accuracy: 0.5452880859375\nStep: 43100\t eval loss: 1.6804594993591309\t eval accuracy: 0.5250244140625\n##########################################################\nStep: 43200\t train loss: 1.6236505508422852\t train accuracy: 0.533447265625\nStep: 43200\t eval loss: 1.6810749769210815\t eval accuracy: 0.5235595703125\n##########################################################\nStep: 43300\t train loss: 1.558311939239502\t train accuracy: 0.5548095703125\nStep: 43300\t eval loss: 1.601332664489746\t eval accuracy: 0.5411376953125\n##########################################################\nStep: 43400\t train loss: 1.538112998008728\t train accuracy: 0.556640625\nStep: 43400\t eval loss: 1.683514952659607\t eval accuracy: 0.5191650390625\n##########################################################\nStep: 43500\t train loss: 1.532744288444519\t train accuracy: 0.562744140625\nStep: 43500\t eval loss: 1.6206731796264648\t eval accuracy: 0.5421142578125\n##########################################################\nStep: 43600\t train loss: 1.587622046470642\t train accuracy: 0.54443359375\nStep: 43600\t eval loss: 1.6124171018600464\t eval accuracy: 0.543212890625\n##########################################################\nStep: 43700\t train loss: 1.5761536359786987\t train accuracy: 0.5474853515625\nStep: 43700\t eval loss: 1.6612695455551147\t eval accuracy: 0.522705078125\n##########################################################\nStep: 43800\t train loss: 1.5803066492080688\t train accuracy: 0.544677734375\nStep: 43800\t eval loss: 1.6615723371505737\t eval accuracy: 0.529541015625\n##########################################################\nStep: 43900\t train loss: 1.5637390613555908\t train accuracy: 0.554443359375\nStep: 43900\t eval loss: 1.6742379665374756\t eval accuracy: 0.5201416015625\n##########################################################\nStep: 44000\t train loss: 1.5607376098632812\t train accuracy: 0.5587158203125\nStep: 44000\t eval loss: 1.6040159463882446\t eval accuracy: 0.5400390625\n##########################################################\nStep: 44100\t train loss: 1.5372731685638428\t train accuracy: 0.55712890625\nStep: 44100\t eval loss: 1.638472318649292\t eval accuracy: 0.5350341796875\n##########################################################\nStep: 44200\t train loss: 1.57210373878479\t train accuracy: 0.5535888671875\nStep: 44200\t eval loss: 1.6335058212280273\t eval accuracy: 0.5350341796875\n##########################################################\nStep: 44300\t train loss: 1.547102689743042\t train accuracy: 0.552978515625\nStep: 44300\t eval loss: 1.667144536972046\t eval accuracy: 0.52783203125\n##########################################################\nStep: 44400\t train loss: 1.5387210845947266\t train accuracy: 0.562744140625\nStep: 44400\t eval loss: 1.605074405670166\t eval accuracy: 0.541748046875\n##########################################################\nStep: 44500\t train loss: 1.5169737339019775\t train accuracy: 0.5697021484375\nStep: 44500\t eval loss: 1.6317059993743896\t eval accuracy: 0.5362548828125\n##########################################################\nStep: 44600\t train loss: 1.5266544818878174\t train accuracy: 0.5655517578125\nStep: 44600\t eval loss: 1.658988118171692\t eval accuracy: 0.52734375\n##########################################################\nStep: 44700\t train loss: 1.5213326215744019\t train accuracy: 0.5595703125\nStep: 44700\t eval loss: 1.5750093460083008\t eval accuracy: 0.552734375\n##########################################################\nStep: 44800\t train loss: 1.53337824344635\t train accuracy: 0.5574951171875\nStep: 44800\t eval loss: 1.9208064079284668\t eval accuracy: 0.4830322265625\n##########################################################\nStep: 44900\t train loss: 1.5996612310409546\t train accuracy: 0.54052734375\nStep: 44900\t eval loss: 1.6489458084106445\t eval accuracy: 0.53515625\n##########################################################\nStep: 45000\t train loss: 1.5303428173065186\t train accuracy: 0.56201171875\nStep: 45000\t eval loss: 1.608109951019287\t eval accuracy: 0.54052734375\n##########################################################\nStep: 45100\t train loss: 1.5854448080062866\t train accuracy: 0.5423583984375\nStep: 45100\t eval loss: 1.6438075304031372\t eval accuracy: 0.532958984375\n##########################################################\nStep: 45200\t train loss: 1.5248619318008423\t train accuracy: 0.567138671875\nStep: 45200\t eval loss: 1.6024763584136963\t eval accuracy: 0.5390625\n##########################################################\nStep: 45300\t train loss: 1.516929268836975\t train accuracy: 0.56787109375\nStep: 45300\t eval loss: 1.618209958076477\t eval accuracy: 0.5384521484375\n##########################################################\nStep: 45400\t train loss: 1.5762460231781006\t train accuracy: 0.547607421875\nStep: 45400\t eval loss: 1.6327638626098633\t eval accuracy: 0.534423828125\n##########################################################\nStep: 45500\t train loss: 1.5562188625335693\t train accuracy: 0.554443359375\nStep: 45500\t eval loss: 1.6311700344085693\t eval accuracy: 0.531005859375\n##########################################################\nStep: 45600\t train loss: 1.574166178703308\t train accuracy: 0.5552978515625\nStep: 45600\t eval loss: 1.597913384437561\t eval accuracy: 0.54345703125\n##########################################################\nStep: 45700\t train loss: 1.5148816108703613\t train accuracy: 0.5595703125\nStep: 45700\t eval loss: 1.6372954845428467\t eval accuracy: 0.5396728515625\n##########################################################\nStep: 45800\t train loss: 1.5581961870193481\t train accuracy: 0.5611572265625\nStep: 45800\t eval loss: 1.5796194076538086\t eval accuracy: 0.552734375\n##########################################################\nStep: 45900\t train loss: 1.5056523084640503\t train accuracy: 0.5654296875\nStep: 45900\t eval loss: 1.6196393966674805\t eval accuracy: 0.5404052734375\n##########################################################\nStep: 46000\t train loss: 1.527523398399353\t train accuracy: 0.5673828125\nStep: 46000\t eval loss: 1.6284279823303223\t eval accuracy: 0.5428466796875\n##########################################################\nStep: 46100\t train loss: 1.511549472808838\t train accuracy: 0.56494140625\nStep: 46100\t eval loss: 1.577846646308899\t eval accuracy: 0.5472412109375\n##########################################################\nStep: 46200\t train loss: 1.5299314260482788\t train accuracy: 0.558837890625\nStep: 46200\t eval loss: 1.6151539087295532\t eval accuracy: 0.5374755859375\n##########################################################\nStep: 46300\t train loss: 1.5127720832824707\t train accuracy: 0.5714111328125\nStep: 46300\t eval loss: 1.5895159244537354\t eval accuracy: 0.538818359375\n##########################################################\nStep: 46400\t train loss: 1.4966261386871338\t train accuracy: 0.5743408203125\nStep: 46400\t eval loss: 1.5853468179702759\t eval accuracy: 0.547119140625\n##########################################################\nStep: 46500\t train loss: 1.4936740398406982\t train accuracy: 0.571533203125\nStep: 46500\t eval loss: 1.5653021335601807\t eval accuracy: 0.552490234375\n##########################################################\nStep: 46600\t train loss: 1.4510170221328735\t train accuracy: 0.583740234375\nStep: 46600\t eval loss: 1.5378004312515259\t eval accuracy: 0.5614013671875\n##########################################################\nStep: 46700\t train loss: 1.518530011177063\t train accuracy: 0.56298828125\nStep: 46700\t eval loss: 1.5949097871780396\t eval accuracy: 0.55224609375\n##########################################################\nStep: 46800\t train loss: 1.5143612623214722\t train accuracy: 0.560302734375\nStep: 46800\t eval loss: 1.622372031211853\t eval accuracy: 0.5379638671875\n##########################################################\nStep: 46900\t train loss: 1.5608367919921875\t train accuracy: 0.5643310546875\nStep: 46900\t eval loss: 1.5848859548568726\t eval accuracy: 0.5523681640625\n##########################################################\nStep: 47000\t train loss: 1.486284613609314\t train accuracy: 0.5771484375\nStep: 47000\t eval loss: 1.5658141374588013\t eval accuracy: 0.5555419921875\n##########################################################\nStep: 47100\t train loss: 1.5102113485336304\t train accuracy: 0.557861328125\nStep: 47100\t eval loss: 1.59673273563385\t eval accuracy: 0.5472412109375\n##########################################################\nStep: 47200\t train loss: 1.4495784044265747\t train accuracy: 0.58251953125\nStep: 47200\t eval loss: 1.5554282665252686\t eval accuracy: 0.5634765625\n##########################################################\nStep: 47300\t train loss: 1.4180423021316528\t train accuracy: 0.5867919921875\nStep: 47300\t eval loss: 1.5508785247802734\t eval accuracy: 0.559326171875\n##########################################################\nStep: 47400\t train loss: 1.4614758491516113\t train accuracy: 0.5814208984375\nStep: 47400\t eval loss: 1.531065821647644\t eval accuracy: 0.5631103515625\n##########################################################\nStep: 47500\t train loss: 1.6261935234069824\t train accuracy: 0.5419921875\nStep: 47500\t eval loss: 1.7473093271255493\t eval accuracy: 0.5089111328125\n##########################################################\nStep: 47600\t train loss: 1.484308123588562\t train accuracy: 0.58447265625\nStep: 47600\t eval loss: 1.5702372789382935\t eval accuracy: 0.557861328125\n##########################################################\nStep: 47700\t train loss: 1.4781155586242676\t train accuracy: 0.56494140625\nStep: 47700\t eval loss: 1.5371274948120117\t eval accuracy: 0.562744140625\n##########################################################\nStep: 47800\t train loss: 1.493506908416748\t train accuracy: 0.5704345703125\nStep: 47800\t eval loss: 1.5953227281570435\t eval accuracy: 0.5440673828125\n##########################################################\nStep: 47900\t train loss: 1.4704368114471436\t train accuracy: 0.5794677734375\nStep: 47900\t eval loss: 1.5782088041305542\t eval accuracy: 0.5552978515625\n##########################################################\nStep: 48000\t train loss: 1.5525139570236206\t train accuracy: 0.563720703125\nStep: 48000\t eval loss: 1.6580390930175781\t eval accuracy: 0.5322265625\n##########################################################\nStep: 48100\t train loss: 1.5003082752227783\t train accuracy: 0.578369140625\nStep: 48100\t eval loss: 1.5634790658950806\t eval accuracy: 0.55908203125\n##########################################################\nStep: 48200\t train loss: 1.4870803356170654\t train accuracy: 0.57080078125\nStep: 48200\t eval loss: 1.570334792137146\t eval accuracy: 0.5513916015625\n##########################################################\nStep: 48300\t train loss: 1.55136239528656\t train accuracy: 0.5626220703125\nStep: 48300\t eval loss: 1.6747163534164429\t eval accuracy: 0.53125\n##########################################################\nStep: 48400\t train loss: 1.5000417232513428\t train accuracy: 0.5673828125\nStep: 48400\t eval loss: 1.5779099464416504\t eval accuracy: 0.5557861328125\n##########################################################\nStep: 48500\t train loss: 1.4938111305236816\t train accuracy: 0.57421875\nStep: 48500\t eval loss: 1.5767052173614502\t eval accuracy: 0.5506591796875\n##########################################################\nStep: 48600\t train loss: 1.4666506052017212\t train accuracy: 0.5758056640625\nStep: 48600\t eval loss: 1.5830320119857788\t eval accuracy: 0.55322265625\n##########################################################\nStep: 48700\t train loss: 1.4709751605987549\t train accuracy: 0.577392578125\nStep: 48700\t eval loss: 1.5951834917068481\t eval accuracy: 0.5457763671875\n##########################################################\nStep: 48800\t train loss: 1.5011399984359741\t train accuracy: 0.5699462890625\nStep: 48800\t eval loss: 1.540307641029358\t eval accuracy: 0.56103515625\n##########################################################\nStep: 48900\t train loss: 1.4574066400527954\t train accuracy: 0.5767822265625\nStep: 48900\t eval loss: 1.6271616220474243\t eval accuracy: 0.5352783203125\n##########################################################\nStep: 49000\t train loss: 1.4398349523544312\t train accuracy: 0.584716796875\nStep: 49000\t eval loss: 1.5139966011047363\t eval accuracy: 0.5672607421875\n##########################################################\nStep: 49100\t train loss: 1.4270294904708862\t train accuracy: 0.5948486328125\nStep: 49100\t eval loss: 1.5013318061828613\t eval accuracy: 0.5772705078125\n##########################################################\nStep: 49200\t train loss: 1.441730260848999\t train accuracy: 0.5845947265625\nStep: 49200\t eval loss: 1.5266116857528687\t eval accuracy: 0.5682373046875\n##########################################################\nStep: 49300\t train loss: 1.4153902530670166\t train accuracy: 0.5933837890625\nStep: 49300\t eval loss: 1.5483825206756592\t eval accuracy: 0.560302734375\n##########################################################\nStep: 49400\t train loss: 1.4900848865509033\t train accuracy: 0.5679931640625\nStep: 49400\t eval loss: 1.5613964796066284\t eval accuracy: 0.5550537109375\n##########################################################\nStep: 49500\t train loss: 1.4335243701934814\t train accuracy: 0.592041015625\nStep: 49500\t eval loss: 1.4955137968063354\t eval accuracy: 0.5660400390625\n##########################################################\nStep: 49600\t train loss: 1.4051661491394043\t train accuracy: 0.5955810546875\nStep: 49600\t eval loss: 1.5439937114715576\t eval accuracy: 0.5635986328125\n##########################################################\nStep: 49700\t train loss: 1.4221341609954834\t train accuracy: 0.5909423828125\nStep: 49700\t eval loss: 1.5446146726608276\t eval accuracy: 0.5650634765625\n##########################################################\nStep: 49800\t train loss: 1.4303284883499146\t train accuracy: 0.5897216796875\nStep: 49800\t eval loss: 1.5205283164978027\t eval accuracy: 0.5679931640625\n##########################################################\nStep: 49900\t train loss: 1.4092392921447754\t train accuracy: 0.5946044921875\nStep: 49900\t eval loss: 1.520503044128418\t eval accuracy: 0.56103515625\n##########################################################\nStep: 50000\t train loss: 1.5117019414901733\t train accuracy: 0.562255859375\nStep: 50000\t eval loss: 1.5781430006027222\t eval accuracy: 0.5517578125\n##########################################################\nStep: 50100\t train loss: 1.438747763633728\t train accuracy: 0.58935546875\nStep: 50100\t eval loss: 1.571241855621338\t eval accuracy: 0.564208984375\n##########################################################\nStep: 50200\t train loss: 1.4282115697860718\t train accuracy: 0.5872802734375\nStep: 50200\t eval loss: 1.5024323463439941\t eval accuracy: 0.5706787109375\n##########################################################\nStep: 50300\t train loss: 1.4228289127349854\t train accuracy: 0.59033203125\nStep: 50300\t eval loss: 1.5373958349227905\t eval accuracy: 0.565185546875\n##########################################################\nStep: 50400\t train loss: 1.4556348323822021\t train accuracy: 0.586181640625\nStep: 50400\t eval loss: 1.5266306400299072\t eval accuracy: 0.5631103515625\n##########################################################\nStep: 50500\t train loss: 1.4202384948730469\t train accuracy: 0.58935546875\nStep: 50500\t eval loss: 1.5246098041534424\t eval accuracy: 0.563720703125\n##########################################################\nStep: 50600\t train loss: 1.4863954782485962\t train accuracy: 0.570556640625\nStep: 50600\t eval loss: 1.5598235130310059\t eval accuracy: 0.5572509765625\n##########################################################\nStep: 50700\t train loss: 1.4454429149627686\t train accuracy: 0.5855712890625\nStep: 50700\t eval loss: 1.549619436264038\t eval accuracy: 0.562255859375\n##########################################################\nStep: 50800\t train loss: 1.4706858396530151\t train accuracy: 0.584228515625\nStep: 50800\t eval loss: 1.554628610610962\t eval accuracy: 0.557373046875\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","File \u001b[0;32m<timed exec>:21\u001b[0m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/jax/_src/array.py:272\u001b[0m, in \u001b[0;36mArrayImpl.__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__bool__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    271\u001b[0m   core\u001b[38;5;241m.\u001b[39mcheck_bool_conversion(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 272\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_value\u001b[49m)\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/jax/_src/profiler.py:335\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    334\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/jax/_src/array.py:621\u001b[0m, in \u001b[0;36mArrayImpl._value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    620\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fully_replicated:\n\u001b[0;32m--> 621\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_single_device_array_to_np_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_npy_value)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt  # Visualization\n\n# Plot loss and accuracy in subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\nax1.set_title('Loss')\nax2.set_title('Accuracy')\n\n\n\nax1.plot(all_train_losses, label='train_loss')\nax1.plot(all_eval_losses, label='eval_loss')\n\nax2.plot(all_train_accuracy, label='train_accuracy')\nax2.plot(all_test_accuracy, label='eval_accuracy')\n\nax1.legend()\nax2.legend()\nplt.show()\nplt.clf()","metadata":{"execution":{"iopub.status.busy":"2024-05-24T14:28:28.755928Z","iopub.execute_input":"2024-05-24T14:28:28.756254Z","iopub.status.idle":"2024-05-24T14:28:29.138580Z","shell.execute_reply.started":"2024-05-24T14:28:28.756225Z","shell.execute_reply":"2024-05-24T14:28:29.137688Z"},"trusted":true},"execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1500x500 with 2 Axes>","image/png":"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"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 0 Axes>"},"metadata":{}}]},{"cell_type":"code","source":"@partial(jax.jit, static_argnames=(\"length\"))\ndef generate_text(rng, params, var_params, length):\n    def _scan_generate(carry, _):\n        random_key, context = carry\n        logits = model.apply({'params': params, **var_params}, context, training=False, mutable=['other_variables'])[0]\n        rng, rng_subkey = jax.random.split(random_key)\n        new_token = jax.random.categorical(\n          rng_subkey, logits[:, -n_tokens, :], axis=-1, shape=(1, 1)\n        )\n        context = jnp.concatenate([context[:, 1:], new_token], axis=1)\n        return (rng, context), new_token\n\n    _, new_tokens = jax.lax.scan(\n    _scan_generate,\n    (rng, jnp.zeros((1, block_size), dtype=jnp.int32)),\n    (),\n    length=length,\n    )\n    return new_tokens","metadata":{"execution":{"iopub.status.busy":"2024-05-24T14:28:29.139761Z","iopub.execute_input":"2024-05-24T14:28:29.140089Z","iopub.status.idle":"2024-05-24T14:28:29.147205Z","shell.execute_reply.started":"2024-05-24T14:28:29.140059Z","shell.execute_reply":"2024-05-24T14:28:29.146487Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"new_tokenz = 1000\nkey, subkey = jax.random.split(jax.random.PRNGKey(156))\n# key, subkey = jax.random.split(key)\n# token_gen = generate_text(jnp.zeros((1,block_size)).astype(jnp.int32), new_tokenz, {'params': state.params})\ntoken_gen = generate_text(key, params, var_params, new_tokenz)[:, 0, 0].tolist()\nprint(token_gen)\nprint(decode(token_gen))","metadata":{"execution":{"iopub.status.busy":"2024-05-24T14:28:29.148267Z","iopub.execute_input":"2024-05-24T14:28:29.148560Z","iopub.status.idle":"2024-05-24T14:28:43.249534Z","shell.execute_reply.started":"2024-05-24T14:28:29.148532Z","shell.execute_reply":"2024-05-24T14:28:43.248478Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"[24, 61, 26, 1, 51, 33, 1, 21, 27, 27, 32, 30, 24, 23, 31, 10, 0, 18, 46, 39, 45, 53, 60, 50, 60, 43, 56, 43, 8, 0, 0, 22, 42, 1, 46, 39, 59, 41, 43, 1, 56, 43, 56, 39, 57, 57, 1, 39, 57, 43, 12, 0, 0, 29, 27, 21, 30, 26, 1, 25, 24, 21, 0, 13, 30, 17, 24, 19, 33, 30, 10, 0, 13, 53, 61, 1, 54, 43, 57, 58, 53, 52, 58, 1, 45, 53, 52, 41, 58, 1, 58, 53, 43, 1, 39, 52, 42, 1, 50, 47, 56, 53, 56, 1, 40, 43, 39, 57, 1, 40, 53, 51, 57, 1, 58, 53, 1, 50, 47, 46, 39, 52, 46, 52, 1, 63, 39, 59, 1, 40, 43, 43, 57, 63, 8, 0, 0, 31, 26, 26, 17, 21, 27, 10, 0, 14, 47, 42, 1, 51, 43, 56, 41, 49, 1, 58, 53, 12, 0, 0, 28, 24, 31, 20, 24, 37, 26, 31, 30, 24, 13, 10, 0, 21, 1, 46, 39, 42, 58, 43, 56, 1, 51, 43, 43, 56, 1, 40, 59, 40, 43, 0, 13, 5, 43, 1, 61, 53, 59, 56, 47, 52, 45, 1, 42, 59, 58, 63, 1, 51, 63, 1, 39, 58, 1, 42, 43, 1, 44, 53, 59, 56, 1, 58, 46, 43, 1, 43, 39, 56, 1, 46, 46, 47, 53, 59, 56, 57, 1, 57, 54, 1, 40, 50, 39, 63, 8, 0, 0, 14, 31, 14, 21, 1, 30, 14, 21, 24, 59, 30, 19, 53, 6, 1, 52, 53, 59, 56, 1, 46, 43, 56, 1, 58, 46, 59, 56, 57, 1, 43, 56, 1, 58, 46, 43, 56, 43, 1, 58, 46, 43, 1, 45, 53, 39, 42, 1, 40, 39, 59, 56, 1, 47, 52, 43, 1, 41, 53, 42, 58, 46, 1, 20, 47, 42, 57, 46, 53, 52, 1, 53, 52, 43, 1, 44, 53, 56, 42, 1, 51, 39, 51, 56, 43, 56, 63, 1, 53, 44, 43, 1, 58, 46, 39, 44, 1, 39, 52, 58, 61, 53, 52, 10, 0, 30, 43, 43, 51, 42, 6, 0, 13, 1, 41, 43, 6, 1, 46, 46, 39, 58, 46, 1, 51, 39, 39, 58, 40, 43, 57, 8, 0, 0, 19, 53, 46, 57, 63, 1, 53, 52, 54, 43, 1, 58, 46, 39, 52, 6, 1, 61, 46, 43, 1, 51, 63, 1, 63, 53, 57, 46, 1, 58, 46, 43, 52, 1, 46, 43, 56, 1, 53, 52, 27, 0, 1, 63, 33, 1, 21, 21, 0, 8, 13, 1, 21, 35, 13, 37, 10, 0, 20, 46, 43, 52, 1, 57, 53, 58, 1, 39, 52, 42, 1, 44, 43, 56, 1, 52, 39, 42, 10, 1, 43, 63, 1, 57, 46, 47, 56, 43, 1, 51, 63, 47, 57, 43, 1, 63, 53, 59, 51, 1, 56, 47, 57, 58, 57, 1, 50, 59, 58, 1, 57, 58, 1, 57, 51, 1, 51, 56, 53, 41, 41, 50, 1, 58, 53, 56, 1, 46, 46, 39, 52, 1, 57, 53, 41, 43, 57, 47, 47, 52, 1, 58, 46, 43, 52, 1, 63, 53, 51, 1, 51, 63, 1, 47, 56, 47, 52, 41, 47, 50, 63, 10, 0, 26, 46, 43, 6, 1, 39, 57, 1, 58, 46, 56, 43, 50, 53, 59, 57, 10, 1, 61, 46, 43, 1, 63, 53, 60, 43, 1, 57, 46, 43, 43, 56, 1, 39, 42, 43, 1, 52, 53, 52, 0, 21, 52, 43, 1, 39, 1, 56, 53, 56, 0, 21, 1, 58, 46, 56, 58, 1, 61, 39, 59, 56, 1, 44, 53, 1, 57, 53, 47, 56, 1, 57, 47, 42, 49, 43, 56, 1, 58, 53, 1, 57, 59, 59, 52, 42, 1, 21, 45, 61, 8, 0, 0, 15, 33, 32, 13, 10, 0, 32, 46, 6, 1, 57, 61, 58, 46, 54, 63, 1, 63, 53, 47, 42, 1, 45, 53, 59, 49, 57, 43, 57, 0, 35, 46, 56, 10, 1, 39, 57, 1, 58, 46, 56, 43, 1, 54, 53, 58, 1, 47, 52, 1, 30, 63, 47, 61, 52, 1, 58, 46, 43, 50, 1, 63, 47, 51, 1, 57, 52, 39, 52, 58, 1, 57, 56, 1, 51, 63, 1, 21, 1, 44, 56, 43, 52, 1, 41, 47, 57, 58, 61, 43, 1, 47, 44, 1, 39, 52, 54, 39, 51, 52, 47, 56, 1, 52, 53, 49, 44, 46, 57, 1, 32, 43, 50, 58, 1, 21, 1, 51, 43, 56, 42, 1, 46, 43, 58, 44, 47, 58, 63, 1, 47, 52, 42, 1, 48, 39, 58, 51, 57, 8, 0, 0, 15, 13, 15, 33, 34, 24, 13, 17, 10, 0, 13, 32, 43, 30, 46, 43, 47, 52, 42, 6, 0, 13, 5, 53, 61, 43, 1, 39, 50, 41, 43, 52, 45, 1, 47, 42, 39, 63, 8, 0, 0, 24, 24, 26, 17, 15, 17, 26, 31, 10, 0, 27, 1, 42, 47, 39, 50, 58, 1, 46, 43, 47, 53, 59, 41, 43, 1, 51, 47, 58, 53, 52, 53, 59, 56, 1, 57, 47, 58, 46, 52, 6, 1, 58, 46, 39, 52, 47, 57, 43, 1, 39, 1, 58, 39, 56, 43, 1, 58, 43, 58, 46, 1, 54, 59, 43, 50, 59, 56, 39, 58, 46, 1, 58, 46, 43, 1, 44, 53, 56, 1, 39, 52, 43, 1, 51, 63, 56, 43, 1, 53, 57, 1, 51, 63, 1, 46, 46, 43, 6, 0, 32, 46, 39, 57, 1, 57, 46, 63, 1, 41, 43, 59, 59, 56, 1, 55, 43, 1, 43, 57, 47, 50, 47, 57, 43, 2, 1, 61, 46, 43, 1, 57, 58, 63, 1, 51, 43, 1, 52, 39, 48, 43, 59, 41, 53, 50, 1, 46, 59, 51, 52, 43, 1, 51, 62, 63, 1, 39, 52, 56, 1, 58, 43, 61, 52, 1, 39, 44, 43, 10, 0, 32, 47, 51, 5, 6, 0, 31]\nLwN mU IOOTRLKS:\nFhagovlvere.\n\nJd hauce rerass ase?\n\nQOIRN MLI\nARELGUR:\nAow pestont gonct toe and liror beas boms to lihanhn yau beesy.\n\nSNNEIO:\nBid merck to?\n\nPLSHLYNSRLA:\nI hadter meer bube\nA'e wouring duty my at de four the ear hhiours sp blay.\n\nBSBI RBILuRGo, nour her thurs er there the goad baur ine codth Hidshon one ford mamrery ofe thaf antwon:\nReemd,\nA ce, hhath maatbes.\n\nGohsy onpe than, whe my yosh then her onO\n yU II\n.A IWAY:\nHhen sot and fer nad: ey shire myise youm rists lut st sm mroccl tor hhan socesiin then yom my irincily:\nNhe, as threlous: whe yove sheer ade non\nIne a ror\nI thrt waur fo soir sidker to suund Igw.\n\nCUTA:\nTh, swthpy yoid goukses\nWhr: as thre pot in Ryiwn thel yim snant sr my I fren cistwe if anpamnir nokfhs Telt I merd hetfity ind jatms.\n\nCACUVLAE:\nATeRheind,\nA'owe alceng iday.\n\nLLNECENS:\nO dialt heiouce mitonour sithn, thanise a tare teth puelurath the for ane myre os my hhe,\nThas shy ceuur qe esilise! whe sty me najeucol humne mxy anr tewn afe:\nTim',\nS\n","output_type":"stream"}]},{"cell_type":"code","source":"dsfsdhfgjdg hfdgjdgjgfjhs'####################","metadata":{"execution":{"iopub.status.busy":"2024-05-24T14:28:43.250838Z","iopub.execute_input":"2024-05-24T14:28:43.251173Z","iopub.status.idle":"2024-05-24T14:28:43.256219Z","shell.execute_reply.started":"2024-05-24T14:28:43.251142Z","shell.execute_reply":"2024-05-24T14:28:43.255201Z"},"trusted":true},"execution_count":34,"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[34], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    dsfsdhfgjdg hfdgjdgjgfjhs'####################\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 1)\n"],"ename":"SyntaxError","evalue":"unterminated string literal (detected at line 1) (2630675753.py, line 1)","output_type":"error"}]},{"cell_type":"code","source":"var_params['other_variables']['Mamba_0']['hidden_state'].shape","metadata":{"execution":{"iopub.status.busy":"2024-05-24T14:28:43.256846Z","iopub.status.idle":"2024-05-24T14:28:43.257200Z","shell.execute_reply.started":"2024-05-24T14:28:43.257015Z","shell.execute_reply":"2024-05-24T14:28:43.257031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params.keys()","metadata":{"execution":{"iopub.status.busy":"2024-05-24T14:28:43.259173Z","iopub.status.idle":"2024-05-24T14:28:43.259532Z","shell.execute_reply.started":"2024-05-24T14:28:43.259361Z","shell.execute_reply":"2024-05-24T14:28:43.259379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params['Dense_12']['kernel'].shape","metadata":{"execution":{"iopub.status.busy":"2024-05-24T14:28:43.260685Z","iopub.status.idle":"2024-05-24T14:28:43.261072Z","shell.execute_reply.started":"2024-05-24T14:28:43.260870Z","shell.execute_reply":"2024-05-24T14:28:43.260889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rngk = jax.random.PRNGKey(389)\nxs, ys = get_batch(rngk, train_data)\nprint(xs[0])\nprint(ys[0])","metadata":{"execution":{"iopub.status.busy":"2024-05-24T14:35:32.281851Z","iopub.execute_input":"2024-05-24T14:35:32.282243Z","iopub.status.idle":"2024-05-24T14:35:32.305832Z","shell.execute_reply.started":"2024-05-24T14:35:32.282212Z","shell.execute_reply":"2024-05-24T14:35:32.304857Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"[53 51 43  1 46 39 52 45 51 39 52  1 51 59 57 58  1 54 59 58  1 53 52  1\n 51 63  1 57 46 56 53 59 42  1 39 52 42  1 50 39 63  1 51 43  0 35 46 43\n 56 43  1 52 53  1 54 56 47 43 57 58  1 57 46 53]\n[52 45 51 39 52  1 51 59 57 58  1 54 59 58  1 53 52  1 51 63  1 57 46 56\n 53 59 42  1 39 52 42  1 50 39 63  1 51 43  0 35 46 43 56 43  1 52 53  1\n 54 56 47 43 57 58  1 57 46 53 60 43 50 57  1 47]\n","output_type":"stream"}]},{"cell_type":"code","source":"logits = model.apply({'params': params, **var_params}, xs[0].reshape((1,64)), training=False, mutable=['other_variables'])[0]\nrng, rng_subkey = jax.random.split(rngk)\nnew_token = jax.random.categorical(\n  rng_subkey, logits[:, -n_tokens, :], axis=-1, shape=(1, 1)\n)\nprint(new_token)","metadata":{"execution":{"iopub.status.busy":"2024-05-24T14:35:33.348541Z","iopub.execute_input":"2024-05-24T14:35:33.349636Z","iopub.status.idle":"2024-05-24T14:35:34.742955Z","shell.execute_reply.started":"2024-05-24T14:35:33.349595Z","shell.execute_reply":"2024-05-24T14:35:34.742040Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"[[51]]\n","output_type":"stream"}]},{"cell_type":"code","source":"ys[0,-32:]","metadata":{"execution":{"iopub.status.busy":"2024-05-24T14:28:43.266374Z","iopub.status.idle":"2024-05-24T14:28:43.266740Z","shell.execute_reply.started":"2024-05-24T14:28:43.266559Z","shell.execute_reply":"2024-05-24T14:28:43.266577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logits.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-24T14:28:43.268216Z","iopub.status.idle":"2024-05-24T14:28:43.268536Z","shell.execute_reply.started":"2024-05-24T14:28:43.268377Z","shell.execute_reply":"2024-05-24T14:28:43.268393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jax.nn.standardize(jnp.array([2.0,3.0,4.0]))","metadata":{"id":"Oe_GIDP2HFyt","outputId":"5d3dce16-fcc2-40b9-c49a-00a8c4013ca2","execution":{"iopub.status.busy":"2024-05-24T14:28:43.269440Z","iopub.status.idle":"2024-05-24T14:28:43.269806Z","shell.execute_reply.started":"2024-05-24T14:28:43.269636Z","shell.execute_reply":"2024-05-24T14:28:43.269653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@struct.dataclass\nclass Metrics(metrics.Collection):\n    accuracy: metrics.Accuracy\n    loss: metrics.Average.from_output('loss')","metadata":{"id":"s3nN1jOiHFyu","execution":{"iopub.status.busy":"2024-05-24T14:28:43.271151Z","iopub.status.idle":"2024-05-24T14:28:43.271496Z","shell.execute_reply.started":"2024-05-24T14:28:43.271337Z","shell.execute_reply":"2024-05-24T14:28:43.271354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainState(train_state.TrainState):\n    metrics: Metrics\n\ndef create_train_state(module, rng, learning_rate, train_shape):\n    \"\"\"Creates an initial `TrainState`.\"\"\"\n    params = module.init(rng, jnp.ones(train_shape).astype(jnp.int32), \n                         training=False)['params'] # initialize parameters by passing a template image\n    tx = optax.adamw(learning_rate)\n    return TrainState.create(\n      apply_fn=module.apply, params=params, tx=tx,\n      metrics=Metrics.empty(),\n    )","metadata":{"id":"7LLDTSFQHFyu","execution":{"iopub.status.busy":"2024-05-24T14:28:43.273243Z","iopub.status.idle":"2024-05-24T14:28:43.273589Z","shell.execute_reply.started":"2024-05-24T14:28:43.273416Z","shell.execute_reply":"2024-05-24T14:28:43.273433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TrainState.create(","metadata":{"execution":{"iopub.status.busy":"2024-05-24T14:28:43.274366Z","iopub.status.idle":"2024-05-24T14:28:43.274715Z","shell.execute_reply.started":"2024-05-24T14:28:43.274547Z","shell.execute_reply":"2024-05-24T14:28:43.274565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@jax.jit\ndef train_step(state, inputs, targets):\n    \"\"\"Train for a single step.\"\"\"\n    def loss_fn(params):\n        logits = state.apply_fn({'params': params}, inputs, training=True, \n                                rngs={\"dropout\": key})[0]\n        loss = optax.softmax_cross_entropy_with_integer_labels(\n            logits=logits, labels=targets).mean()\n        return loss\n    grad_fn = jax.grad(loss_fn)\n    grads = grad_fn(state.params)\n    state = state.apply_gradients(grads=grads)\n    return state","metadata":{"id":"zApWXUDaHFyu","execution":{"iopub.status.busy":"2024-05-24T14:28:43.275948Z","iopub.status.idle":"2024-05-24T14:28:43.276310Z","shell.execute_reply.started":"2024-05-24T14:28:43.276147Z","shell.execute_reply":"2024-05-24T14:28:43.276165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@jax.jit\ndef compute_metrics(*, state, inputs, targets):\n    logits = state.apply_fn({'params': state.params}, inputs, training=False)[0]\n    loss = optax.softmax_cross_entropy_with_integer_labels(\n        logits=logits, labels=targets).mean()\n    metric_updates = state.metrics.single_from_model_output(\n    logits=logits, labels=targets, loss=loss)\n    metrics = state.metrics.merge(metric_updates)\n    state = state.replace(metrics=metrics)\n    return state","metadata":{"id":"VzukZ4iEHFyv","execution":{"iopub.status.busy":"2024-05-24T14:28:43.277727Z","iopub.status.idle":"2024-05-24T14:28:43.278090Z","shell.execute_reply.started":"2024-05-24T14:28:43.277894Z","shell.execute_reply":"2024-05-24T14:28:43.277911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 10\nlearning_rate = 0.005\ninit_rng = jax.random.key(0)","metadata":{"id":"ehYvMeuNHFyv","execution":{"iopub.status.busy":"2024-05-24T14:28:43.279136Z","iopub.status.idle":"2024-05-24T14:28:43.279488Z","shell.execute_reply.started":"2024-05-24T14:28:43.279317Z","shell.execute_reply":"2024-05-24T14:28:43.279336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"state = create_train_state(fin_model, init_rng, learning_rate, train_shape)\ndel init_rng  # Must not be used anymore.","metadata":{"id":"D60UHLFHHFyv","execution":{"iopub.status.busy":"2024-05-24T14:28:43.280674Z","iopub.status.idle":"2024-05-24T14:28:43.281053Z","shell.execute_reply.started":"2024-05-24T14:28:43.280843Z","shell.execute_reply":"2024-05-24T14:28:43.280860Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics_history = {'train_loss': [],\n                   'train_accuracy': [],\n                   'test_loss': [],\n                   'test_accuracy': []}","metadata":{"id":"Jl-9TlHEHFyv","execution":{"iopub.status.busy":"2024-05-24T14:28:43.282310Z","iopub.status.idle":"2024-05-24T14:28:43.282649Z","shell.execute_reply.started":"2024-05-24T14:28:43.282479Z","shell.execute_reply":"2024-05-24T14:28:43.282495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 442\nkey = jax.random.PRNGKey(SEED)\nloss = 10\ncounter = 0\n# for step in tqdm(range(max_iters)): # increase number of steps for good results...\nwhile counter==max_iters or loss > 1.0:\n\n      # sample a batch of data\n    xb, yb = get_batch(key, train_data)\n    state = train_step(state, xb, yb)\n    state = compute_metrics(state=state, inputs=xb, targets=yb)\n\n    key = (jax.random.split(key)[0])\n\n    if step == 0 or (step+1) % 100 == 0: # one training epoch has passed\n        for metric,value in state.metrics.compute().items(): # compute metrics\n            metrics_history[f'train_{metric}'].append(value) # record metrics\n        state = state.replace(metrics=state.metrics.empty()) # reset train_metrics for next training epoch\n\n        # Compute metrics on the test set after each training epoch\n        test_state = state\n        x_test, y_test = get_batch(key, test_data)\n    #     for test_batch in test_ds.as_numpy_iterator():\n        test_state = compute_metrics(state=test_state, inputs=x_test, targets=y_test)\n\n        for metric,value in test_state.metrics.compute().items():\n            metrics_history[f'test_{metric}'].append(value)\n\n        print(f\"train epoch: {(step+1)}, \"\n              f\"loss: {metrics_history['train_loss'][-1]}, \"\n              f\"accuracy: {metrics_history['train_accuracy'][-1] * 100}\")\n        print(f\"test epoch: {(step+1) }, \"\n          f\"loss: {metrics_history['test_loss'][-1]}, \"\n          f\"accuracy: {metrics_history['test_accuracy'][-1] * 100}\")","metadata":{"id":"CaNt9JazHFyw","outputId":"ba447ddf-9940-44a6-f4b2-d27ed78a88c2","execution":{"iopub.status.busy":"2024-05-24T14:28:43.283605Z","iopub.status.idle":"2024-05-24T14:28:43.283929Z","shell.execute_reply.started":"2024-05-24T14:28:43.283768Z","shell.execute_reply":"2024-05-24T14:28:43.283785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt  # Visualization\n\n# Plot loss and accuracy in subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\nax1.set_title('Loss')\nax2.set_title('Accuracy')\nfor dataset in ('train','test'):\n    ax1.plot(metrics_history[f'{dataset}_loss'], label=f'{dataset}_loss')\n    ax2.plot(metrics_history[f'{dataset}_accuracy'], label=f'{dataset}_accuracy')\nax1.legend()\nax2.legend()\nplt.show()\nplt.clf()","metadata":{"id":"Y40JGx1YHFyw","execution":{"iopub.status.busy":"2024-05-24T14:28:43.285156Z","iopub.status.idle":"2024-05-24T14:28:43.285483Z","shell.execute_reply.started":"2024-05-24T14:28:43.285323Z","shell.execute_reply":"2024-05-24T14:28:43.285340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nlogits = fin_model.apply(fin_params, xb, training=False)[0]\nloss = optax.softmax_cross_entropy_with_integer_labels(\n            logits=logits, labels=yb).mean()\n\nprint(loss)","metadata":{"id":"7pJlFXpVHFyw","execution":{"iopub.status.busy":"2024-05-24T14:28:43.286728Z","iopub.status.idle":"2024-05-24T14:28:43.287081Z","shell.execute_reply.started":"2024-05-24T14:28:43.286893Z","shell.execute_reply":"2024-05-24T14:28:43.286910Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def generate_text(idx, max_new_tokens, params):\n# # idx is (B, T) array of indices in the current context\n#     for i in range(max_new_tokens):\n#         # crop idx to the last block_size tokens\n#         idx_cond = idx[:, -block_size:]\n#         # get the predictions\n#         logits = fin_model.apply(params, idx_cond)\n#         # focus only on the last time step\n#         logits = logits[:, -1, :] # becomes (B, C)\n\n#         if i == 0:\n#             rng, rng_subkey = jax.random.split(jax.random.PRNGKey(12))\n#         else:\n#             rng, rng_subkey = jax.random.split(rng)\n\n#         idx_next = jax.random.categorical(rng_subkey, logits, axis=-1, shape=(1, 1)) # (B, 1)\n\n\n#         # append sampled index to the running sequence\n#         idx = jnp.concatenate([idx, idx_next], axis=-1) # (B, T+1)\n\n#     return idx","metadata":{"id":"9d28o-dTHFyx","execution":{"iopub.status.busy":"2024-05-24T14:28:43.288233Z","iopub.status.idle":"2024-05-24T14:28:43.288557Z","shell.execute_reply.started":"2024-05-24T14:28:43.288404Z","shell.execute_reply":"2024-05-24T14:28:43.288419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@partial(jax.jit, static_argnames=(\"self\", \"length\"))\ndef generate_text(rng, params, length):\n    def _scan_generate(carry, _):\n        random_key, context = carry\n        logits = fin_model.apply(params, context, training=False)[0]\n        rng, rng_subkey = jax.random.split(random_key)\n        new_token = jax.random.categorical(\n          rng_subkey, logits[:, -1, :], axis=-1, shape=(1, 1)\n        )\n        context = jnp.concatenate([context[:, 1:], new_token], axis=1)\n        return (rng, context), new_token\n\n    _, new_tokens = jax.lax.scan(\n    _scan_generate,\n    (rng, jnp.zeros((1, block_size), dtype=jnp.int32)),\n    (),\n    length=length,\n    )\n    return new_tokens","metadata":{"id":"WB0og7pAHFyx","execution":{"iopub.status.busy":"2024-05-24T14:28:43.289962Z","iopub.status.idle":"2024-05-24T14:28:43.290372Z","shell.execute_reply.started":"2024-05-24T14:28:43.290203Z","shell.execute_reply":"2024-05-24T14:28:43.290221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_tokenz = 1000\nkey, subkey = jax.random.split(jax.random.PRNGKey(156))\n# key, subkey = jax.random.split(key)\n# token_gen = generate_text(jnp.zeros((1,block_size)).astype(jnp.int32), new_tokenz, {'params': state.params})\ntoken_gen = generate_text(key, {'params': state.params}, new_tokenz)[:, 0, 0].tolist()\nprint(token_gen)\nprint(decode(token_gen))","metadata":{"id":"50Vpg2lEHFyx","execution":{"iopub.status.busy":"2024-05-24T14:28:43.291829Z","iopub.status.idle":"2024-05-24T14:28:43.292204Z","shell.execute_reply.started":"2024-05-24T14:28:43.292015Z","shell.execute_reply":"2024-05-24T14:28:43.292044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sdgh  fs","metadata":{"execution":{"iopub.status.busy":"2024-05-24T14:28:43.293281Z","iopub.status.idle":"2024-05-24T14:28:43.293622Z","shell.execute_reply.started":"2024-05-24T14:28:43.293449Z","shell.execute_reply":"2024-05-24T14:28:43.293466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"state.params","metadata":{"execution":{"iopub.status.busy":"2024-05-24T14:28:43.294791Z","iopub.status.idle":"2024-05-24T14:28:43.295159Z","shell.execute_reply.started":"2024-05-24T14:28:43.294955Z","shell.execute_reply":"2024-05-24T14:28:43.294972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install mamba-ssm","metadata":{"id":"MOw_xjbrHFy0","execution":{"iopub.status.busy":"2024-05-24T14:28:43.296310Z","iopub.status.idle":"2024-05-24T14:28:43.296640Z","shell.execute_reply.started":"2024-05-24T14:28:43.296473Z","shell.execute_reply":"2024-05-24T14:28:43.296489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ones = lambda *size: torch.ones(*size).float().cuda()\nzeros = lambda *size: torch.zeros(*size).float().cuda()\narange = lambda n: torch.arange(n).float().cuda()\nrand = lambda size: torch.rand(*size).abs().float().cuda()\n\ndef create_torch(S = 128, Ba = 2, D = 4, N = 4):\n    x = rand((Ba, 1, D, S))\n    a = -ones((Ba, N, D, 1))\n    b = ones((Ba, N, 1, S)) * 0.1\n    c = rand((Ba, N, 1, S)) * 0.1\n    delta = rand((Ba, 1, D, S)) * 0.1\n    return x, a, b, c, delta","metadata":{"id":"W_PAnYcEOR22","execution":{"iopub.status.busy":"2024-05-24T14:28:43.297520Z","iopub.status.idle":"2024-05-24T14:28:43.297860Z","shell.execute_reply.started":"2024-05-24T14:28:43.297693Z","shell.execute_reply":"2024-05-24T14:28:43.297711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import selective_scan_cuda\n\nxx, aa, bb, cc, ddelta = create_torch()\ny_from_repo = selective_scan_cuda.fwd(xx.squeeze(1), ddelta.squeeze(1), aa[0].squeeze(-1).T, bb.squeeze(-2)[:, None, :, :], cc.squeeze(-2)[:, None, :, :], None, None, None, False)\ny_from_repo","metadata":{"id":"ykh4GTvtOrak","execution":{"iopub.status.busy":"2024-05-24T14:28:43.299254Z","iopub.status.idle":"2024-05-24T14:28:43.299610Z","shell.execute_reply.started":"2024-05-24T14:28:43.299433Z","shell.execute_reply":"2024-05-24T14:28:43.299451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def discretize(a, b, delta):\n    da = delta * a\n    a_ = jnp.exp(da)\n    b_ = b * delta\n    return a_, b_\n\ndef ssm(x, a, b, c, delta):\n    \"Jax Implementation\"\n    y = []\n    h = 0\n    a_, b_ = discretize(a, b, delta)\n    for k in range(x.shape[-1]):\n        h = a_[..., k] * h + b_[..., k] * x[..., k]\n        y.append((c[..., k] * h).sum(1, keepdims=True))\n    return h, jnp.stack(y, -1)\n","metadata":{"id":"NEdG1yPNOtxU","execution":{"iopub.status.busy":"2024-05-24T14:28:43.300847Z","iopub.status.idle":"2024-05-24T14:28:43.301207Z","shell.execute_reply.started":"2024-05-24T14:28:43.301037Z","shell.execute_reply":"2024-05-24T14:28:43.301056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_, y_ = ssm(xx.cpu().numpy(), aa.cpu().numpy(), bb.cpu().numpy(), cc.cpu().numpy(), ddelta.cpu().numpy())","metadata":{"id":"GEjNcZSZPIp_","execution":{"iopub.status.busy":"2024-05-24T14:28:43.302542Z","iopub.status.idle":"2024-05-24T14:28:43.302874Z","shell.execute_reply.started":"2024-05-24T14:28:43.302709Z","shell.execute_reply":"2024-05-24T14:28:43.302726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"tWlqZZOmPnYk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from mamba_ssm import Mamba as Mamba_T\ntorch_mamba = Mamba_T(\n      # This module uses roughly 3 * expand * d_model^2 parameters\n      d_model=n_embd, # Model dimension d_model\n      d_state=16,  # SSM state expansion factor\n      d_conv=4,    # Local convolution width\n      expand=2,    # Block expansion factor\n)","metadata":{"id":"5RHAE_I1Pql9","execution":{"iopub.status.busy":"2024-05-24T14:28:43.304045Z","iopub.status.idle":"2024-05-24T14:28:43.304392Z","shell.execute_reply.started":"2024-05-24T14:28:43.304217Z","shell.execute_reply":"2024-05-24T14:28:43.304234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xm = x = rand((1, 1, n_embd, 32))\nxm.shape","metadata":{"id":"l9zw_M-USrDt","execution":{"iopub.status.busy":"2024-05-24T14:28:43.305918Z","iopub.status.idle":"2024-05-24T14:28:43.306263Z","shell.execute_reply.started":"2024-05-24T14:28:43.306103Z","shell.execute_reply":"2024-05-24T14:28:43.306119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch_mamba(xm.squeeze(1))","metadata":{"id":"gGmA2EWlTCo0","execution":{"iopub.status.busy":"2024-05-24T14:28:43.307408Z","iopub.status.idle":"2024-05-24T14:28:43.307757Z","shell.execute_reply.started":"2024-05-24T14:28:43.307594Z","shell.execute_reply":"2024-05-24T14:28:43.307610Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch_mamba.in_proj","metadata":{"id":"73ek9mx9UBBl","execution":{"iopub.status.busy":"2024-05-24T14:28:43.308511Z","iopub.status.idle":"2024-05-24T14:28:43.308831Z","shell.execute_reply.started":"2024-05-24T14:28:43.308670Z","shell.execute_reply":"2024-05-24T14:28:43.308697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"P3l_ssIYbiYT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_mamba_repo = mamba_inner_fn()","metadata":{"id":"-X7hXQRMZhl3","execution":{"iopub.status.busy":"2024-05-24T14:28:43.310588Z","iopub.status.idle":"2024-05-24T14:28:43.310936Z","shell.execute_reply.started":"2024-05-24T14:28:43.310758Z","shell.execute_reply":"2024-05-24T14:28:43.310776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xm.squeeze(1).shape","metadata":{"id":"rJhKQ_Oua9Gy","execution":{"iopub.status.busy":"2024-05-24T14:28:43.312119Z","iopub.status.idle":"2024-05-24T14:28:43.312478Z","shell.execute_reply.started":"2024-05-24T14:28:43.312314Z","shell.execute_reply":"2024-05-24T14:28:43.312331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"mzkoYrSVkoJj"},"execution_count":null,"outputs":[]}]}