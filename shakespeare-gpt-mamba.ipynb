{"metadata":{"colab":{"provenance":[]},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q clu","metadata":{"id":"gS6euWNvHFye","outputId":"45b149a7-9450-439c-da67-ab8678a3b0d0","execution":{"iopub.status.busy":"2024-05-27T06:19:47.407305Z","iopub.execute_input":"2024-05-27T06:19:47.408302Z","iopub.status.idle":"2024-05-27T06:20:04.302338Z","shell.execute_reply.started":"2024-05-27T06:19:47.408263Z","shell.execute_reply":"2024-05-27T06:20:04.301286Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.2.1 which is incompatible.\ntensorflow 2.15.0 requires ml-dtypes~=0.2.0, but you have ml-dtypes 0.4.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt","metadata":{"id":"7jjCLfuUHFyg","outputId":"dfe048f0-dd44-40ef-edf3-2fa56558672f","execution":{"iopub.status.busy":"2024-05-27T06:20:04.304236Z","iopub.execute_input":"2024-05-27T06:20:04.304537Z","iopub.status.idle":"2024-05-27T06:20:06.018006Z","shell.execute_reply.started":"2024-05-27T06:20:04.304509Z","shell.execute_reply":"2024-05-27T06:20:06.017080Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"--2024-05-27 06:20:05--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1115394 (1.1M) [text/plain]\nSaving to: 'input.txt'\n\ninput.txt           100%[===================>]   1.06M  5.15MB/s    in 0.2s    \n\n2024-05-27 06:20:05 (5.15 MB/s) - 'input.txt' saved [1115394/1115394]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from functools import partial\nimport jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\nfrom jax.nn.initializers import lecun_normal, normal\nfrom jax.numpy.linalg import eigh, inv, matrix_power\nfrom jax.scipy.signal import convolve\n\nimport torch\n\nfrom dataclasses import dataclass\n\nfrom typing import Union\n\nimport matplotlib.pyplot as plt\nimport seaborn\n\nfrom clu import metrics\nfrom flax.training import train_state  # Useful dataclass to keep train state\nfrom flax import struct                # Flax dataclasses\nimport optax                           # Common loss functions and optimizers\nfrom tqdm import tqdm","metadata":{"id":"YXSCJzupHFyh","execution":{"iopub.status.busy":"2024-05-27T06:20:06.019938Z","iopub.execute_input":"2024-05-27T06:20:06.020303Z","iopub.status.idle":"2024-05-27T06:20:13.092062Z","shell.execute_reply.started":"2024-05-27T06:20:06.020265Z","shell.execute_reply":"2024-05-27T06:20:13.091243Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# read it in to inspect it\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()","metadata":{"id":"KpJoV3KQHFyh","execution":{"iopub.status.busy":"2024-05-27T06:20:13.093184Z","iopub.execute_input":"2024-05-27T06:20:13.093582Z","iopub.status.idle":"2024-05-27T06:20:13.099740Z","shell.execute_reply.started":"2024-05-27T06:20:13.093557Z","shell.execute_reply":"2024-05-27T06:20:13.098878Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)","metadata":{"id":"PsWxZqyRHFyi","outputId":"b1730724-647e-45cd-edfa-97af24995830","execution":{"iopub.status.busy":"2024-05-27T06:20:13.102145Z","iopub.execute_input":"2024-05-27T06:20:13.102412Z","iopub.status.idle":"2024-05-27T06:20:13.126903Z","shell.execute_reply.started":"2024-05-27T06:20:13.102388Z","shell.execute_reply":"2024-05-27T06:20:13.126032Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n65\n","output_type":"stream"}]},{"cell_type":"code","source":"# create a mapping from characters to integers\nstoi = { ch: i for i,ch in enumerate(chars) }\nitos = { i: ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))","metadata":{"id":"S-mzLOk1HFyi","outputId":"f56e2f85-5a1c-4099-87df-436ba39f4363","execution":{"iopub.status.busy":"2024-05-27T06:20:13.128025Z","iopub.execute_input":"2024-05-27T06:20:13.128390Z","iopub.status.idle":"2024-05-27T06:20:13.136610Z","shell.execute_reply.started":"2024-05-27T06:20:13.128363Z","shell.execute_reply":"2024-05-27T06:20:13.135667Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"[46, 47, 47, 1, 58, 46, 43, 56, 43]\nhii there\n","output_type":"stream"}]},{"cell_type":"code","source":"data = jnp.array(encode(text), dtype=jnp.int32)\nprint(data.shape, data.dtype)\nprint(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this","metadata":{"id":"HImuqDd8HFyj","outputId":"91dcd15f-f068-4551-ad29-e6e41e52fd91","execution":{"iopub.status.busy":"2024-05-27T06:20:13.137491Z","iopub.execute_input":"2024-05-27T06:20:13.137732Z","iopub.status.idle":"2024-05-27T06:20:16.097781Z","shell.execute_reply.started":"2024-05-27T06:20:13.137710Z","shell.execute_reply":"2024-05-27T06:20:16.096129Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"(1115394,) int32\n[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n  0 37 53 59  1 39 56 43  1 39 50 50  1 56 43 57 53 50 60 43 42  1 56 39\n 58 46 43 56  1 58 53  1 42 47 43  1 58 46 39 52  1 58 53  1 44 39 51 47\n 57 46 12  0  0 13 50 50 10  0 30 43 57 53 50 60 43 42  8  1 56 43 57 53\n 50 60 43 42  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 18 47\n 56 57 58  6  1 63 53 59  1 49 52 53 61  1 15 39 47 59 57  1 25 39 56 41\n 47 59 57  1 47 57  1 41 46 47 43 44  1 43 52 43 51 63  1 58 53  1 58 46\n 43  1 54 43 53 54 50 43  8  0  0 13 50 50 10  0 35 43  1 49 52 53 61  5\n 58  6  1 61 43  1 49 52 53 61  5 58  8  0  0 18 47 56 57 58  1 15 47 58\n 47 64 43 52 10  0 24 43 58  1 59 57  1 49 47 50 50  1 46 47 51  6  1 39\n 52 42  1 61 43  5 50 50  1 46 39 60 43  1 41 53 56 52  1 39 58  1 53 59\n 56  1 53 61 52  1 54 56 47 41 43  8  0 21 57  5 58  1 39  1 60 43 56 42\n 47 41 58 12  0  0 13 50 50 10  0 26 53  1 51 53 56 43  1 58 39 50 49 47\n 52 45  1 53 52  5 58 11  1 50 43 58  1 47 58  1 40 43  1 42 53 52 43 10\n  1 39 61 39 63  6  1 39 61 39 63  2  0  0 31 43 41 53 52 42  1 15 47 58\n 47 64 43 52 10  0 27 52 43  1 61 53 56 42  6  1 45 53 53 42  1 41 47 58\n 47 64 43 52 57  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 35\n 43  1 39 56 43  1 39 41 41 53 59 52 58 43 42  1 54 53 53 56  1 41 47 58\n 47 64 43 52 57  6  1 58 46 43  1 54 39 58 56 47 41 47 39 52 57  1 45 53\n 53 42  8  0 35 46 39 58  1 39 59 58 46 53 56 47 58 63  1 57 59 56 44 43\n 47 58 57  1 53 52  1 61 53 59 50 42  1 56 43 50 47 43 60 43  1 59 57 10\n  1 47 44  1 58 46 43 63  0 61 53 59 50 42  1 63 47 43 50 42  1 59 57  1\n 40 59 58  1 58 46 43  1 57 59 54 43 56 44 50 59 47 58 63  6  1 61 46 47\n 50 43  1 47 58  1 61 43 56 43  0 61 46 53 50 43 57 53 51 43  6  1 61 43\n  1 51 47 45 46 58  1 45 59 43 57 57  1 58 46 43 63  1 56 43 50 47 43 60\n 43 42  1 59 57  1 46 59 51 39 52 43 50 63 11  0 40 59 58  1 58 46 43 63\n  1 58 46 47 52 49  1 61 43  1 39 56 43  1 58 53 53  1 42 43 39 56 10  1\n 58 46 43  1 50 43 39 52 52 43 57 57  1 58 46 39 58  0 39 44 44 50 47 41\n 58 57  1 59 57  6  1 58 46 43  1 53 40 48 43 41 58  1 53 44  1 53 59 56\n  1 51 47 57 43 56 63  6  1 47 57  1 39 57  1 39 52  0 47 52 60 43 52 58\n 53 56 63  1 58 53  1 54 39 56 58 47 41 59 50 39 56 47 57 43  1 58 46 43\n 47 56  1 39 40 59 52 42 39 52 41 43 11  1 53 59 56  0 57 59 44 44 43 56\n 39 52 41 43  1 47 57  1 39  1 45 39 47 52  1 58 53  1 58 46 43 51  1 24\n 43 58  1 59 57  1 56 43 60 43 52 45 43  1 58 46 47 57  1 61 47 58 46  0\n 53 59 56  1 54 47 49 43 57  6  1 43 56 43  1 61 43  1 40 43 41 53 51 43\n  1 56 39 49 43 57 10  1 44 53 56  1 58 46 43  1 45 53 42 57  1 49 52 53\n 61  1 21  0 57 54 43 39 49  1 58 46 47 57  1 47 52  1 46 59 52 45 43 56\n  1 44 53 56  1 40 56 43 39 42  6  1 52 53 58  1 47 52  1 58 46 47 56 57\n 58  1 44 53 56  1 56 43 60 43 52 45 43  8  0  0]\n","output_type":"stream"}]},{"cell_type":"code","source":"train_test_split = 0.9\nn = int(train_test_split*len(data))\ntrain_data = data[:n]\ntest_data = data[n:]","metadata":{"id":"pXrAqMxRHFyj","execution":{"iopub.status.busy":"2024-05-27T06:20:16.099135Z","iopub.execute_input":"2024-05-27T06:20:16.100138Z","iopub.status.idle":"2024-05-27T06:20:16.277238Z","shell.execute_reply.started":"2024-05-27T06:20:16.100100Z","shell.execute_reply":"2024-05-27T06:20:16.276123Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"block_size = 8\ntrain_data[:block_size+1]","metadata":{"id":"ahhKyiAzHFyj","outputId":"98306c96-5082-4dfa-ba66-915051831fc8","execution":{"iopub.status.busy":"2024-05-27T06:20:16.278970Z","iopub.execute_input":"2024-05-27T06:20:16.279337Z","iopub.status.idle":"2024-05-27T06:20:16.362244Z","shell.execute_reply.started":"2024-05-27T06:20:16.279308Z","shell.execute_reply":"2024-05-27T06:20:16.361346Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"Array([18, 47, 56, 57, 58,  1, 15, 47, 58], dtype=int32)"},"metadata":{}}]},{"cell_type":"code","source":"x = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")","metadata":{"id":"HIpsznQmHFyk","outputId":"be9d197b-0b79-43ed-f3a9-e74295d51c79","execution":{"iopub.status.busy":"2024-05-27T06:20:16.363429Z","iopub.execute_input":"2024-05-27T06:20:16.363705Z","iopub.status.idle":"2024-05-27T06:20:16.995159Z","shell.execute_reply.started":"2024-05-27T06:20:16.363681Z","shell.execute_reply":"2024-05-27T06:20:16.994183Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"when input is [18] the target: 47\nwhen input is [18 47] the target: 56\nwhen input is [18 47 56] the target: 57\nwhen input is [18 47 56 57] the target: 58\nwhen input is [18 47 56 57 58] the target: 1\nwhen input is [18 47 56 57 58  1] the target: 15\nwhen input is [18 47 56 57 58  1 15] the target: 47\nwhen input is [18 47 56 57 58  1 15 47] the target: 58\n","output_type":"stream"}]},{"cell_type":"code","source":"batch_size = 128 # how many independent sequences will we process in parallel?\nblock_size = 64 # what is the maximum context length for predictions?\nmax_iters = 10000\nlearning_rate = 1e-3\n# device = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 100\nn_embd = 256\nexpans = 2\nn_heads = 1\nchannel_size = n_embd // n_heads\nn_layers = 1\ndropout = 0.2\nconv_k_size = 3\nn_latent_dim = 16\nn_tokens = 1\n\nrng_key = jax.random.PRNGKey(1564)\n\ndynamic_slice_vmap = jax.vmap(jax.lax.dynamic_slice, in_axes=(None, 0, None))\n\n@jax.jit\ndef get_batch(random_key, data):\n    \"\"\"Prepares a random batch of training data.\n\n    Args:\n      random_key: A random seed for sampling a batch.\n      data: The complete training dataset.\n\n    Returns:\n      x: Input sequences.\n      y: Target sequences (shifted inputs).\n    \"\"\"\n    ix = jax.random.randint(\n      random_key, shape=(batch_size, 1), minval=0, maxval=len(data) - block_size\n    )\n    x = dynamic_slice_vmap(data, ix, (block_size,))\n    y = dynamic_slice_vmap(data, ix + n_tokens, (block_size,))\n    return x, y\n\nxb, yb = get_batch(rng_key, train_data)\ntrain_shape = xb.shape\nprint('inputs:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\n# print('----')\n\n# for b in range(batch_size): # batch dimension\n#     for t in range(block_size): # time dimension\n#         context = xb[b, :t+1]\n#         target = yb[b,t]\n#         print(f\"when input is {context} the target: {target}\")","metadata":{"id":"UuAjtqPeHFyk","outputId":"6a88fb2b-b798-4ee9-9f4f-f38ce898d576","execution":{"iopub.status.busy":"2024-05-27T06:20:16.996235Z","iopub.execute_input":"2024-05-27T06:20:16.996504Z","iopub.status.idle":"2024-05-27T06:20:17.362498Z","shell.execute_reply.started":"2024-05-27T06:20:16.996481Z","shell.execute_reply":"2024-05-27T06:20:17.361519Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"inputs:\n(128, 64)\n[[ 1 63 53 ... 40 63  1]\n [43  1 52 ... 46 39 42]\n [63  1 45 ... 50  1 61]\n ...\n [46 47 41 ... 58 43  1]\n [ 1 61 43 ... 53 59  1]\n [49  1 58 ... 53 61  1]]\ntargets:\n(128, 64)\n[[63 53 59 ... 63  1 58]\n [ 1 52 43 ... 39 42  1]\n [ 1 45 53 ...  1 61 43]\n ...\n [47 41 46 ... 43  1 47]\n [61 43  5 ... 59  1 42]\n [ 1 58 46 ... 61  1 52]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Mamba Block\nDense --> Conv1D --> Silu --> SSM --> Silu -->","metadata":{"id":"yOccqzJlHFym"}},{"cell_type":"code","source":"print(xb[0])\nprint(yb[0])","metadata":{"execution":{"iopub.status.busy":"2024-05-27T06:20:17.363579Z","iopub.execute_input":"2024-05-27T06:20:17.363862Z","iopub.status.idle":"2024-05-27T06:20:17.464555Z","shell.execute_reply.started":"2024-05-27T06:20:17.363836Z","shell.execute_reply":"2024-05-27T06:20:17.463505Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"[ 1 63 53 59 56 57  8  0  0 24 17 27 26 32 17 31 10  0 13  1 52 43 57 58\n  1 53 44  1 58 56 39 47 58 53 56 57  2  0  0 13 26 32 21 19 27 26 33 31\n 10  0 21  1 39 51  1 52 53 52 43  6  1 40 63  1]\n[63 53 59 56 57  8  0  0 24 17 27 26 32 17 31 10  0 13  1 52 43 57 58  1\n 53 44  1 58 56 39 47 58 53 56 57  2  0  0 13 26 32 21 19 27 26 33 31 10\n  0 21  1 39 51  1 52 53 52 43  6  1 40 63  1 58]\n","output_type":"stream"}]},{"cell_type":"code","source":"# hidden_state = [jnp.zeros((1,n_latent_dim, n_embd * expans)) for _ in range(n_layers)]","metadata":{"execution":{"iopub.status.busy":"2024-05-27T06:20:17.465709Z","iopub.execute_input":"2024-05-27T06:20:17.466018Z","iopub.status.idle":"2024-05-27T06:20:17.470384Z","shell.execute_reply.started":"2024-05-27T06:20:17.465992Z","shell.execute_reply":"2024-05-27T06:20:17.469320Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# hidden_state[0].shape","metadata":{"execution":{"iopub.status.busy":"2024-05-27T06:20:17.475518Z","iopub.execute_input":"2024-05-27T06:20:17.475792Z","iopub.status.idle":"2024-05-27T06:20:17.480643Z","shell.execute_reply.started":"2024-05-27T06:20:17.475768Z","shell.execute_reply":"2024-05-27T06:20:17.479715Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class Mamba(nn.Module):\n\n    def setup(self):\n        emb_features = n_embd * expans\n        self.in_proj1 = nn.Dense(features=emb_features)\n        self.in_proj2 = nn.Dense(features=emb_features)\n\n        # Adjusted for Flax. Flax does not have nn.Conv1d, so you might need to reshape or use a different approach\n        self.conv1d = nn.Conv(features=emb_features,\n                              kernel_size=conv_k_size,\n                              padding=1,\n                              )\n\n        self.A = -1*self.param('A', nn.initializers.ones, (1, n_latent_dim, emb_features, 1))\n        self.B = 0.1*self.param('B', nn.initializers.ones, (1, n_latent_dim, 1, block_size))\n        self.C = self.param('C', jax.random.normal, (1, n_latent_dim, 1, block_size))\n#         self.D = self.param('D', jax.random.normal, (1, self.args.d_state, self.args.d_model, 1))\n        self.delta = self.param('delta', jax.random.normal, (1, 1,emb_features, block_size))\n\n        self.out_proj = nn.Dense(n_embd // n_heads)\n        \n        self.hidden_state = self.variable('other_variables','hidden_state', \n                                          jnp.zeros, \n                                          (1,n_latent_dim, emb_features))\n        self.rms_norm = nn.RMSNorm()\n\n    def __call__(self, embeds):\n        x = self.in_proj1(embeds)\n        x = self.conv1d(x)\n        x = jax.nn.silu(x)\n        x = x.reshape((x.shape[0],1,x.shape[2],x.shape[1]))\n        x = self.ssm(x)\n        x = x.reshape((x.shape[0],x.shape[3],x.shape[2]))\n        x = x*jax.nn.silu(self.in_proj2(embeds))\n\n        x = self.out_proj(x)\n\n        x = self.rms_norm(x)\n\n        return x\n    def discretize(self):\n        da = self.delta * self.A\n        a_ = jnp.exp(da)\n        b_ = self.B * self.delta\n        return a_, b_\n\n    def ssm(self, x):\n        y = []\n        a_, b_ = self.discretize()\n        h = 0\n        for k in range(x.shape[-1]):\n            h = a_[..., k] * h + b_[..., k] * x[..., k]\n            \n#         for l in range(x.shape[-1])\n            y.append((self.C[..., k] * h).sum(1, keepdims=True))     \n        \n        self.hidden_state.value = jax.nn.standardize(h.mean(0, keepdims=True))\n        return jnp.stack(y, -1)","metadata":{"id":"4qOdblU5HFyo","execution":{"iopub.status.busy":"2024-05-27T06:20:17.482107Z","iopub.execute_input":"2024-05-27T06:20:17.482486Z","iopub.status.idle":"2024-05-27T06:20:17.498410Z","shell.execute_reply.started":"2024-05-27T06:20:17.482451Z","shell.execute_reply":"2024-05-27T06:20:17.497512Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# class MultiHeadMamba(nn.Module):\n#     def setup(self):\n#         self.layernorm\n#         self.heads = [Mamba() for _ in range(n_heads)]\n#         self.rms_norm = nn.RMSNorm()\n\n#     def __call__(self, x):\n#         out = jnp.concatenate([h(x) for h in self.heads], axis=-1)\n#         x = self.rms_norm(out)\n#         return x","metadata":{"id":"0bH9vlLZHFyq","execution":{"iopub.status.busy":"2024-05-27T06:20:17.499754Z","iopub.execute_input":"2024-05-27T06:20:17.500218Z","iopub.status.idle":"2024-05-27T06:20:17.511238Z","shell.execute_reply.started":"2024-05-27T06:20:17.500183Z","shell.execute_reply":"2024-05-27T06:20:17.510418Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# class FeedForward(nn.Module):\n#     def setup(self):\n#         self.ffn = nn.Sequential([\n#             nn.Dense(4 * n_embd),\n#             nn.relu,\n#             nn.Dense(n_embd)]\n#         )\n#     def __call__(self, x):\n#         return self.ffn(x)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T06:20:17.512291Z","iopub.execute_input":"2024-05-27T06:20:17.512550Z","iopub.status.idle":"2024-05-27T06:20:17.520676Z","shell.execute_reply.started":"2024-05-27T06:20:17.512527Z","shell.execute_reply":"2024-05-27T06:20:17.519817Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# class MambaBlock(nn.Module):\n#     def setup(self):\n#         self.mamba_block = Mamba()\n#         self.ln1 = nn.RMSNorm()\n#         self.ffn = FeedForward()\n#         self.ln2 = nn.LayerNorm()\n\n#     def __call__(self, x):\n#         x = x + self.mamba_block(self.ln2(x))\n#         x = x + self.ffn(self.ln1(x))\n#         return x\n","metadata":{"id":"UiCxIjoEp2QA","execution":{"iopub.status.busy":"2024-05-27T06:20:17.521836Z","iopub.execute_input":"2024-05-27T06:20:17.522245Z","iopub.status.idle":"2024-05-27T06:20:17.530242Z","shell.execute_reply.started":"2024-05-27T06:20:17.522214Z","shell.execute_reply":"2024-05-27T06:20:17.529278Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# class MambaModel(nn.Module):\n\n#     def setup(self):\n#         self.tok_embeddings = nn.Embed(vocab_size, n_embd)\n#         self.pos_embeddings = nn.Embed(block_size, n_embd)\n#         self.ln = nn.LayerNorm()\n#         self.mamba_layers = [MambaBlock() for _ in range(n_layers)]\n#         self.preds_out = nn.Dense(vocab_size)\n\n#     def __call__(self, x, training: bool):\n#         x = self.tok_embeddings(x) + self.pos_embeddings(jnp.arange(block_size))\n# #         x = self.ln(x)\n#         for layer in self.mamba_layers:\n#             x = layer(x)\n            \n#         return self.preds_out(x)\n\n#     @jax.jit\n#     def generate(self, idx, max_new_tokens, params):\n#     # idx is (B, T) array of indices in the current context\n#         for _ in range(max_new_tokens):\n#             # crop idx to the last block_size tokens\n#             idx_cond = idx[:, -block_size:]\n#             # get the predictions\n#             logits = self.apply(params, idx_cond)\n#             # focus only on the last time step\n#             logits = logits[:, -1, :] # becomes (B, C)\n#             # apply softmax to get probabilities\n#             ##probs = tf.keras.activations.softmax(logits, dim=-1) # (B, C)\n#             # sample from the distribution\n#             idx_next = jax.random.categorical(jax.random.PRNGKey(52), logits) # (B, 1)\n#             # append sampled index to the running sequence\n#             idx = jax.numpy.expand_dims(jnp.concatenate([idx[0], idx_next], axis=0), 0) # (B, T+1)\n#     #         print(idx_next)\n#     #         print(idx)\n\n#         return idx","metadata":{"id":"y4C7OWL8HFyq","execution":{"iopub.status.busy":"2024-05-27T06:20:17.531258Z","iopub.execute_input":"2024-05-27T06:20:17.533256Z","iopub.status.idle":"2024-05-27T06:20:17.539949Z","shell.execute_reply.started":"2024-05-27T06:20:17.533230Z","shell.execute_reply":"2024-05-27T06:20:17.538962Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# model = Mamba()\n# params = model.init(jax.random.key(42), jnp.ones((1,64,256)))\n# # print(params['other_variables']['hidden_state'].shape, params['other_variables']['hidden_state'].min(), params['other_variables']['hidden_state'].max())\n# # print(model.tabulate(jax.random.key(0), jnp.ones((1,64,256)),\n# #                    compute_flops=True, compute_vjp_flops=True))\n# xs = model.apply(params, jnp.ones((1,64,256)), mutable=['other_variables'])\n# # # print(params['other_variables']['hidden_state'].shape, params['other_variables']['hidden_state'].min(), params['other_variables']['hidden_state'].max())\n# xb.shape, xs[0].shape, xs[1].keys()","metadata":{"id":"wTd3jSQWHFyp","execution":{"iopub.status.busy":"2024-05-27T06:20:17.541127Z","iopub.execute_input":"2024-05-27T06:20:17.541665Z","iopub.status.idle":"2024-05-27T06:20:17.553096Z","shell.execute_reply.started":"2024-05-27T06:20:17.541639Z","shell.execute_reply":"2024-05-27T06:20:17.552267Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# print(xs[1]['other_variables']['hidden_state'].shape, xs[1]['other_variables']['hidden_state'].min(), xs[1]['other_variables']['hidden_state'].max())","metadata":{"execution":{"iopub.status.busy":"2024-05-27T06:20:17.554076Z","iopub.execute_input":"2024-05-27T06:20:17.554642Z","iopub.status.idle":"2024-05-27T06:20:17.564981Z","shell.execute_reply.started":"2024-05-27T06:20:17.554610Z","shell.execute_reply":"2024-05-27T06:20:17.564269Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# xfs = model.apply(params, 2*jnp.ones((1,64,256)), mutable=['other_variables'])\n# print(params['other_variables']['hidden_state'].shape, params['other_variables']['hidden_state'].min(), params['other_variables']['hidden_state'].max())\n# print(xfs[1]['other_variables']['hidden_state'].shape, xfs[1]['other_variables']['hidden_state'].min(), xfs[1]['other_variables']['hidden_state'].max())","metadata":{"execution":{"iopub.status.busy":"2024-05-27T06:20:17.565922Z","iopub.execute_input":"2024-05-27T06:20:17.566219Z","iopub.status.idle":"2024-05-27T06:20:17.573573Z","shell.execute_reply.started":"2024-05-27T06:20:17.566196Z","shell.execute_reply":"2024-05-27T06:20:17.572748Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# test_model = Mamba()\n# test_params = test_model.init(jax.random.key(42), xb)\n# n_params = sum(p.size for p in jax.tree_util.tree_leaves(test_params))\n# print(f\"Total number of parameters: {n_params:_}\")\n# # print(fin_model.tabulate(jax.random.key(42), xb,\n# #                    compute_flops=True, compute_vjp_flops=True))\n# xf = test_model.apply(test_params, xb)\n# xb.shape, xf.shape","metadata":{"id":"cm2a0nepHFyq","execution":{"iopub.status.busy":"2024-05-27T06:20:17.574652Z","iopub.execute_input":"2024-05-27T06:20:17.574990Z","iopub.status.idle":"2024-05-27T06:20:17.582001Z","shell.execute_reply.started":"2024-05-27T06:20:17.574955Z","shell.execute_reply":"2024-05-27T06:20:17.581249Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"class NanoLM(nn.Module):\n    \"\"\"NanoLM model.\"\"\"\n    vocab_size: int = 65\n    num_layers: int = 6\n    num_heads: int = 8\n    head_size: int = 32\n    dropout_rate: float = 0.2\n    embed_size: int = 256\n    block_size: int = 64\n\n    @nn.compact\n    def __call__(self, x, training: bool):\n        x = nn.Embed(self.vocab_size, self.embed_size)(x) + nn.Embed(\n            self.block_size, self.embed_size\n        )(jnp.arange(self.block_size))\n        \n        for i in range(self.num_layers):\n            x_norm = nn.LayerNorm()(x)\n#             x = x + nn.MultiHeadDotProductAttention(\n#               num_heads=self.num_heads,\n#               qkv_features=self.head_size,\n#               out_features=self.head_size * self.num_heads,\n#               dropout_rate=self.dropout_rate,\n#             )(\n#               x_norm,\n#               x_norm,\n#               mask=jnp.tril(jnp.ones((x.shape[-2], x.shape[-2]))),\n#               deterministic=not training,\n#             )\n    \n            x = x + Mamba()(x_norm)\n\n#             x = x + nn.Sequential([\n#               nn.Dense(4 * self.embed_size),\n#               nn.relu,\n#               nn.Dropout(self.dropout_rate, deterministic=not training),\n#               nn.Dense(self.embed_size),\n#             ])(nn.LayerNorm()(x))\n\n        x = nn.LayerNorm()(x)\n        return nn.Dense(self.vocab_size)(x)","metadata":{"id":"zuiaFP6WHFyr","execution":{"iopub.status.busy":"2024-05-27T06:20:17.582989Z","iopub.execute_input":"2024-05-27T06:20:17.583269Z","iopub.status.idle":"2024-05-27T06:20:17.592494Z","shell.execute_reply.started":"2024-05-27T06:20:17.583243Z","shell.execute_reply":"2024-05-27T06:20:17.591729Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# key = jax.random.key(42)\n\n# # fin_model = MambaModel()\n# # fin_params = fin_model.init(key, xb, training=False)\n\n\n# fin_model = NanoLM(\n#     vocab_size=vocab_size,\n#     num_layers=n_layers,\n#     num_heads=8,\n#     head_size=32,\n#     dropout_rate=0.2,\n#     embed_size=n_embd,\n#     block_size=block_size,\n# )\n\n# fin_params = fin_model.init(\n#     {'params': key},\n#     jnp.ones((batch_size, block_size), dtype=jnp.int32),\n#     training=False\n# )\n\n# n_params = sum(p.size for p in jax.tree_util.tree_leaves(fin_params))\n# print(f\"Total number of parameters: {n_params:_}\")\n# # print(fin_model.tabulate(jax.random.key(42), xb,\n# #                    compute_flops=True, compute_vjp_flops=True))\n# xf = fin_model.apply(fin_params, xb, training=False)[0]\n# xb.shape, xf.shape","metadata":{"id":"fnUQPyuvHFys","outputId":"f04ebf31-d67f-4488-dd5d-7fd5b20dd1ea","execution":{"iopub.status.busy":"2024-05-27T06:20:17.593536Z","iopub.execute_input":"2024-05-27T06:20:17.593842Z","iopub.status.idle":"2024-05-27T06:20:17.604834Z","shell.execute_reply.started":"2024-05-27T06:20:17.593795Z","shell.execute_reply":"2024-05-27T06:20:17.604097Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def loss_fun(params, x, y, var_params,dropout_key):\n    logits, updated_variables = model.apply({'params': params, **var_params}, x, training=True, rngs={\"dropout\": dropout_key}, mutable=['other_variables'])\n    accuracy = jnp.mean(jnp.argmax(logits, axis=-1) == y)\n    return optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=y).mean(), (updated_variables, accuracy)\n\n@jax.jit\ndef eval_step(params, x, y, var_params):\n    logits, _ = model.apply({'params': params, **var_params}, x, training=False, mutable=['other_variables'])\n    accuracy = jnp.mean(jnp.argmax(logits, axis=-1) == y)\n    return optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=y).mean(), accuracy","metadata":{"execution":{"iopub.status.busy":"2024-05-27T06:20:17.605798Z","iopub.execute_input":"2024-05-27T06:20:17.606106Z","iopub.status.idle":"2024-05-27T06:20:17.617505Z","shell.execute_reply.started":"2024-05-27T06:20:17.606081Z","shell.execute_reply":"2024-05-27T06:20:17.616615Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"key = jax.random.PRNGKey(42)\nkey, subkey = jax.random.split(key)\n\nmodel = NanoLM(\n    vocab_size=vocab_size,\n    num_layers=n_layers,\n    num_heads=8,\n    head_size=32,\n    dropout_rate=0.2,\n    embed_size=n_embd,\n    block_size=block_size,\n)\n\nvar_params = model.init(\n    key,\n    jnp.ones((batch_size, block_size), dtype=jnp.int32),\n    training=False,\n)\nprint(var_params.keys())\nn_params = sum(p.size for p in jax.tree_util.tree_leaves(var_params))\n\nprint(f\"Total number of parameters: {n_params:_}\")","metadata":{"id":"PKpb3864HFyt","execution":{"iopub.status.busy":"2024-05-27T06:20:17.618644Z","iopub.execute_input":"2024-05-27T06:20:17.618937Z","iopub.status.idle":"2024-05-27T06:20:25.039151Z","shell.execute_reply.started":"2024-05-27T06:20:17.618913Z","shell.execute_reply":"2024-05-27T06:20:25.038175Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"dict_keys(['params', 'other_variables'])\nTotal number of parameters: 1_283_649\n","output_type":"stream"}]},{"cell_type":"code","source":"params = var_params.pop('params')","metadata":{"execution":{"iopub.status.busy":"2024-05-27T06:20:25.040895Z","iopub.execute_input":"2024-05-27T06:20:25.041273Z","iopub.status.idle":"2024-05-27T06:20:25.045290Z","shell.execute_reply.started":"2024-05-27T06:20:25.041232Z","shell.execute_reply":"2024-05-27T06:20:25.044486Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"var_params = jax.tree_map(lambda x: jnp.zeros_like(x), var_params)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T06:20:25.046405Z","iopub.execute_input":"2024-05-27T06:20:25.046680Z","iopub.status.idle":"2024-05-27T06:20:25.060582Z","shell.execute_reply.started":"2024-05-27T06:20:25.046656Z","shell.execute_reply":"2024-05-27T06:20:25.059836Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# decay_rate = 0.96\n# learning_rate_schedule = optax.exponential_decay(learning_rate, decay_rate, max_iters//1000)\nopt = optax.adamw(learning_rate=learning_rate)\n\nopt_state = opt.init(params)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T06:20:25.061545Z","iopub.execute_input":"2024-05-27T06:20:25.061822Z","iopub.status.idle":"2024-05-27T06:20:25.363497Z","shell.execute_reply.started":"2024-05-27T06:20:25.061786Z","shell.execute_reply":"2024-05-27T06:20:25.362734Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"%%time\n\nall_train_losses = []\nall_eval_losses = []\n\nall_train_accuracy =  []\nall_test_accuracy = []\n\n# we define one iteration of the optimizer and JIT this function\n@jax.jit\ndef step(key, params, var_params, opt_state):\n    key, subkey = jax.random.split(key)\n    xb, yb = get_batch(key, train_data)\n    (loss, aux_data), grad = jax.value_and_grad(loss_fun, has_aux=True)(params, xb, yb, var_params, subkey)\n    var_params, train_accuracy = aux_data\n    updates, opt_state = opt.update(grad, opt_state, params)\n    params = optax.apply_updates(params, updates)\n    return params, key, opt_state, loss, var_params, train_accuracy\n\n# for i in tqdm(range(max_iters)):\ncounter = 0\nloss = 10\nwhile counter<max_iters: # and loss > 1.0:\n\n    params, key, opt_state, loss, var_params, train_accuracy = step(key, params, var_params, opt_state)\n    \n\n    # once every N_FREQ_EVAL we compute loss on the validation set\n    if counter % eval_iters == 0:\n        key, subkey = jax.random.split(key)\n        eval_loss, eval_accuracy = eval_step(params, *get_batch(subkey, test_data), var_params)\n        all_train_losses.append(loss)\n        all_eval_losses.append(eval_loss)\n        all_train_accuracy.append(train_accuracy)\n        all_test_accuracy.append(eval_accuracy)\n        print('##########################################################')\n        print(f\"Step: {counter}\\t train loss: {loss}\\t train accuracy: {train_accuracy}\")\n        print(f\"Step: {counter}\\t eval loss: {eval_loss}\\t eval accuracy: {eval_accuracy}\")\n        \n    counter += 1\n        ","metadata":{"execution":{"iopub.status.busy":"2024-05-27T06:20:25.364570Z","iopub.execute_input":"2024-05-27T06:20:25.364878Z","iopub.status.idle":"2024-05-27T06:25:03.151863Z","shell.execute_reply.started":"2024-05-27T06:20:25.364851Z","shell.execute_reply":"2024-05-27T06:25:03.150876Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"##########################################################\nStep: 0\t train loss: 4.702024459838867\t train accuracy: 0.0184326171875\nStep: 0\t eval loss: 4.51285982131958\t eval accuracy: 0.0250244140625\n##########################################################\nStep: 100\t train loss: 1.0302603244781494\t train accuracy: 0.7362060546875\nStep: 100\t eval loss: 1.0054510831832886\t eval accuracy: 0.7379150390625\n##########################################################\nStep: 200\t train loss: 0.2345236837863922\t train accuracy: 0.9476318359375\nStep: 200\t eval loss: 0.24154049158096313\t eval accuracy: 0.9447021484375\n##########################################################\nStep: 300\t train loss: 0.0760331004858017\t train accuracy: 0.9844970703125\nStep: 300\t eval loss: 0.07102090120315552\t eval accuracy: 0.9849853515625\n##########################################################\nStep: 400\t train loss: 0.04816635325551033\t train accuracy: 0.9892578125\nStep: 400\t eval loss: 0.04691700637340546\t eval accuracy: 0.9886474609375\n##########################################################\nStep: 500\t train loss: 0.04408349096775055\t train accuracy: 0.98779296875\nStep: 500\t eval loss: 0.04040125757455826\t eval accuracy: 0.989501953125\n##########################################################\nStep: 600\t train loss: 0.042644135653972626\t train accuracy: 0.9892578125\nStep: 600\t eval loss: 0.041616201400756836\t eval accuracy: 0.9892578125\n##########################################################\nStep: 700\t train loss: 0.03903347998857498\t train accuracy: 0.98974609375\nStep: 700\t eval loss: 0.03915254771709442\t eval accuracy: 0.9893798828125\n##########################################################\nStep: 800\t train loss: 0.037527747452259064\t train accuracy: 0.9898681640625\nStep: 800\t eval loss: 0.03819543868303299\t eval accuracy: 0.98828125\n##########################################################\nStep: 900\t train loss: 0.0413208045065403\t train accuracy: 0.98876953125\nStep: 900\t eval loss: 0.03678717464208603\t eval accuracy: 0.98974609375\n##########################################################\nStep: 1000\t train loss: 0.03460327908396721\t train accuracy: 0.9906005859375\nStep: 1000\t eval loss: 0.038125354796648026\t eval accuracy: 0.9893798828125\n##########################################################\nStep: 1100\t train loss: 0.0374937579035759\t train accuracy: 0.9898681640625\nStep: 1100\t eval loss: 0.034939587116241455\t eval accuracy: 0.9901123046875\n##########################################################\nStep: 1200\t train loss: 0.03653908520936966\t train accuracy: 0.9891357421875\nStep: 1200\t eval loss: 0.03495452553033829\t eval accuracy: 0.990234375\n##########################################################\nStep: 1300\t train loss: 0.03862230107188225\t train accuracy: 0.98876953125\nStep: 1300\t eval loss: 0.036484844982624054\t eval accuracy: 0.99072265625\n##########################################################\nStep: 1400\t train loss: 0.03934645652770996\t train accuracy: 0.989013671875\nStep: 1400\t eval loss: 0.03257589787244797\t eval accuracy: 0.99072265625\n##########################################################\nStep: 1500\t train loss: 0.034219980239868164\t train accuracy: 0.9898681640625\nStep: 1500\t eval loss: 0.035585224628448486\t eval accuracy: 0.9901123046875\n##########################################################\nStep: 1600\t train loss: 0.039250697940588\t train accuracy: 0.989990234375\nStep: 1600\t eval loss: 0.03833217918872833\t eval accuracy: 0.9892578125\n##########################################################\nStep: 1700\t train loss: 0.035890914499759674\t train accuracy: 0.9896240234375\nStep: 1700\t eval loss: 0.03690051659941673\t eval accuracy: 0.989501953125\n##########################################################\nStep: 1800\t train loss: 0.034928202629089355\t train accuracy: 0.9896240234375\nStep: 1800\t eval loss: 0.04283785820007324\t eval accuracy: 0.9886474609375\n##########################################################\nStep: 1900\t train loss: 0.03424760699272156\t train accuracy: 0.9892578125\nStep: 1900\t eval loss: 0.03683929890394211\t eval accuracy: 0.9898681640625\n##########################################################\nStep: 2000\t train loss: 0.03457748889923096\t train accuracy: 0.9893798828125\nStep: 2000\t eval loss: 0.035253576934337616\t eval accuracy: 0.9901123046875\n##########################################################\nStep: 2100\t train loss: 0.03650754690170288\t train accuracy: 0.9892578125\nStep: 2100\t eval loss: 0.037266552448272705\t eval accuracy: 0.9893798828125\n##########################################################\nStep: 2200\t train loss: 0.032801270484924316\t train accuracy: 0.990478515625\nStep: 2200\t eval loss: 0.03388939052820206\t eval accuracy: 0.9901123046875\n##########################################################\nStep: 2300\t train loss: 0.035452358424663544\t train accuracy: 0.9901123046875\nStep: 2300\t eval loss: 0.03413558006286621\t eval accuracy: 0.990234375\n##########################################################\nStep: 2400\t train loss: 0.035306207835674286\t train accuracy: 0.9891357421875\nStep: 2400\t eval loss: 0.03447327762842178\t eval accuracy: 0.99072265625\n##########################################################\nStep: 2500\t train loss: 0.03368120640516281\t train accuracy: 0.9903564453125\nStep: 2500\t eval loss: 0.037516769021749496\t eval accuracy: 0.98876953125\n##########################################################\nStep: 2600\t train loss: 0.03660804405808449\t train accuracy: 0.9898681640625\nStep: 2600\t eval loss: 0.031111018732190132\t eval accuracy: 0.99072265625\n##########################################################\nStep: 2700\t train loss: 0.033600855618715286\t train accuracy: 0.9901123046875\nStep: 2700\t eval loss: 0.03576759248971939\t eval accuracy: 0.9903564453125\n##########################################################\nStep: 2800\t train loss: 0.03545871749520302\t train accuracy: 0.989990234375\nStep: 2800\t eval loss: 0.03633591905236244\t eval accuracy: 0.9898681640625\n##########################################################\nStep: 2900\t train loss: 0.03467781841754913\t train accuracy: 0.9896240234375\nStep: 2900\t eval loss: 0.03955688327550888\t eval accuracy: 0.9881591796875\n##########################################################\nStep: 3000\t train loss: 0.03845144808292389\t train accuracy: 0.98974609375\nStep: 3000\t eval loss: 0.03797789663076401\t eval accuracy: 0.989990234375\n##########################################################\nStep: 3100\t train loss: 0.03000830113887787\t train accuracy: 0.9918212890625\nStep: 3100\t eval loss: 0.039617300033569336\t eval accuracy: 0.989013671875\n##########################################################\nStep: 3200\t train loss: 0.03202752768993378\t train accuracy: 0.989990234375\nStep: 3200\t eval loss: 0.031293585896492004\t eval accuracy: 0.9906005859375\n##########################################################\nStep: 3300\t train loss: 0.03135160729289055\t train accuracy: 0.989990234375\nStep: 3300\t eval loss: 0.03343565762042999\t eval accuracy: 0.98974609375\n##########################################################\nStep: 3400\t train loss: 0.03210081532597542\t train accuracy: 0.990234375\nStep: 3400\t eval loss: 0.03712401166558266\t eval accuracy: 0.99072265625\n##########################################################\nStep: 3500\t train loss: 0.03333387151360512\t train accuracy: 0.990234375\nStep: 3500\t eval loss: 0.03762464597821236\t eval accuracy: 0.98974609375\n##########################################################\nStep: 3600\t train loss: 0.03154546022415161\t train accuracy: 0.9910888671875\nStep: 3600\t eval loss: 0.0371883362531662\t eval accuracy: 0.9884033203125\n##########################################################\nStep: 3700\t train loss: 0.0340552031993866\t train accuracy: 0.9896240234375\nStep: 3700\t eval loss: 0.034794799983501434\t eval accuracy: 0.98974609375\n##########################################################\nStep: 3800\t train loss: 0.03309059143066406\t train accuracy: 0.9908447265625\nStep: 3800\t eval loss: 0.03384236991405487\t eval accuracy: 0.990234375\n##########################################################\nStep: 3900\t train loss: 0.03647809475660324\t train accuracy: 0.990234375\nStep: 3900\t eval loss: 0.035569675266742706\t eval accuracy: 0.9892578125\n##########################################################\nStep: 4000\t train loss: 0.03288315236568451\t train accuracy: 0.9901123046875\nStep: 4000\t eval loss: 0.03422898054122925\t eval accuracy: 0.98974609375\n##########################################################\nStep: 4100\t train loss: 0.032600827515125275\t train accuracy: 0.989990234375\nStep: 4100\t eval loss: 0.03326341509819031\t eval accuracy: 0.9901123046875\n##########################################################\nStep: 4200\t train loss: 0.035934239625930786\t train accuracy: 0.9886474609375\nStep: 4200\t eval loss: 0.03526735305786133\t eval accuracy: 0.9893798828125\n##########################################################\nStep: 4300\t train loss: 0.030401643365621567\t train accuracy: 0.990478515625\nStep: 4300\t eval loss: 0.03683570772409439\t eval accuracy: 0.9898681640625\n##########################################################\nStep: 4400\t train loss: 0.03362543135881424\t train accuracy: 0.989013671875\nStep: 4400\t eval loss: 0.03563333675265312\t eval accuracy: 0.9891357421875\n##########################################################\nStep: 4500\t train loss: 0.03509456664323807\t train accuracy: 0.98876953125\nStep: 4500\t eval loss: 0.03800368309020996\t eval accuracy: 0.9891357421875\n##########################################################\nStep: 4600\t train loss: 0.033829014748334885\t train accuracy: 0.9896240234375\nStep: 4600\t eval loss: 0.041398387402296066\t eval accuracy: 0.9891357421875\n##########################################################\nStep: 4700\t train loss: 0.03071049228310585\t train accuracy: 0.9912109375\nStep: 4700\t eval loss: 0.03582839295268059\t eval accuracy: 0.989990234375\n##########################################################\nStep: 4800\t train loss: 0.029168546199798584\t train accuracy: 0.99169921875\nStep: 4800\t eval loss: 0.03662075102329254\t eval accuracy: 0.9893798828125\n##########################################################\nStep: 4900\t train loss: 0.03392685577273369\t train accuracy: 0.989990234375\nStep: 4900\t eval loss: 0.035212256014347076\t eval accuracy: 0.989501953125\n##########################################################\nStep: 5000\t train loss: 0.03315085172653198\t train accuracy: 0.9896240234375\nStep: 5000\t eval loss: 0.034700341522693634\t eval accuracy: 0.9901123046875\n##########################################################\nStep: 5100\t train loss: 0.034337468445301056\t train accuracy: 0.98974609375\nStep: 5100\t eval loss: 0.032366685569286346\t eval accuracy: 0.989990234375\n##########################################################\nStep: 5200\t train loss: 0.0365213081240654\t train accuracy: 0.9896240234375\nStep: 5200\t eval loss: 0.034437086433172226\t eval accuracy: 0.9901123046875\n##########################################################\nStep: 5300\t train loss: 0.03497280925512314\t train accuracy: 0.9888916015625\nStep: 5300\t eval loss: 0.0351988822221756\t eval accuracy: 0.989501953125\n##########################################################\nStep: 5400\t train loss: 0.029916273429989815\t train accuracy: 0.989990234375\nStep: 5400\t eval loss: 0.036677196621894836\t eval accuracy: 0.9888916015625\n##########################################################\nStep: 5500\t train loss: 0.03500382602214813\t train accuracy: 0.990478515625\nStep: 5500\t eval loss: 0.03420785069465637\t eval accuracy: 0.9893798828125\n##########################################################\nStep: 5600\t train loss: 0.033727921545505524\t train accuracy: 0.9903564453125\nStep: 5600\t eval loss: 0.03548334911465645\t eval accuracy: 0.9893798828125\n##########################################################\nStep: 5700\t train loss: 0.036552511155605316\t train accuracy: 0.988525390625\nStep: 5700\t eval loss: 0.03478957712650299\t eval accuracy: 0.9898681640625\n##########################################################\nStep: 5800\t train loss: 0.03256293386220932\t train accuracy: 0.99072265625\nStep: 5800\t eval loss: 0.03575193136930466\t eval accuracy: 0.9898681640625\n##########################################################\nStep: 5900\t train loss: 0.03201504051685333\t train accuracy: 0.989990234375\nStep: 5900\t eval loss: 0.038006171584129333\t eval accuracy: 0.989501953125\n##########################################################\nStep: 6000\t train loss: 0.03398777171969414\t train accuracy: 0.989501953125\nStep: 6000\t eval loss: 0.038388751447200775\t eval accuracy: 0.988525390625\n##########################################################\nStep: 6100\t train loss: 0.028201624751091003\t train accuracy: 0.9910888671875\nStep: 6100\t eval loss: 0.03430291637778282\t eval accuracy: 0.9898681640625\n##########################################################\nStep: 6200\t train loss: 0.030893102288246155\t train accuracy: 0.9912109375\nStep: 6200\t eval loss: 0.035597510635852814\t eval accuracy: 0.9896240234375\n##########################################################\nStep: 6300\t train loss: 0.03402170538902283\t train accuracy: 0.991455078125\nStep: 6300\t eval loss: 0.03354821354150772\t eval accuracy: 0.990478515625\n##########################################################\nStep: 6400\t train loss: 0.029741298407316208\t train accuracy: 0.9910888671875\nStep: 6400\t eval loss: 0.032326389104127884\t eval accuracy: 0.9908447265625\n##########################################################\nStep: 6500\t train loss: 0.03148931264877319\t train accuracy: 0.990234375\nStep: 6500\t eval loss: 0.03358190879225731\t eval accuracy: 0.9901123046875\n##########################################################\nStep: 6600\t train loss: 0.03457869961857796\t train accuracy: 0.9893798828125\nStep: 6600\t eval loss: 0.034148238599300385\t eval accuracy: 0.98974609375\n##########################################################\nStep: 6700\t train loss: 0.03287986293435097\t train accuracy: 0.9903564453125\nStep: 6700\t eval loss: 0.03627997636795044\t eval accuracy: 0.989013671875\n##########################################################\nStep: 6800\t train loss: 0.0375729501247406\t train accuracy: 0.9888916015625\nStep: 6800\t eval loss: 0.03495504707098007\t eval accuracy: 0.9898681640625\n##########################################################\nStep: 6900\t train loss: 0.035153016448020935\t train accuracy: 0.988525390625\nStep: 6900\t eval loss: 0.03470666706562042\t eval accuracy: 0.989501953125\n##########################################################\nStep: 7000\t train loss: 0.03307158872485161\t train accuracy: 0.9898681640625\nStep: 7000\t eval loss: 0.033524155616760254\t eval accuracy: 0.98974609375\n##########################################################\nStep: 7100\t train loss: 0.03196188434958458\t train accuracy: 0.9908447265625\nStep: 7100\t eval loss: 0.036901623010635376\t eval accuracy: 0.9892578125\n##########################################################\nStep: 7200\t train loss: 0.03256057947874069\t train accuracy: 0.9912109375\nStep: 7200\t eval loss: 0.038787443190813065\t eval accuracy: 0.9881591796875\n##########################################################\nStep: 7300\t train loss: 0.034176602959632874\t train accuracy: 0.9898681640625\nStep: 7300\t eval loss: 0.036156065762043\t eval accuracy: 0.9888916015625\n##########################################################\nStep: 7400\t train loss: 0.035394735634326935\t train accuracy: 0.9893798828125\nStep: 7400\t eval loss: 0.034449879080057144\t eval accuracy: 0.9896240234375\n##########################################################\nStep: 7500\t train loss: 0.031690869480371475\t train accuracy: 0.990966796875\nStep: 7500\t eval loss: 0.0340755358338356\t eval accuracy: 0.9892578125\n##########################################################\nStep: 7600\t train loss: 0.02855151891708374\t train accuracy: 0.991455078125\nStep: 7600\t eval loss: 0.03136909008026123\t eval accuracy: 0.9901123046875\n##########################################################\nStep: 7700\t train loss: 0.036026157438755035\t train accuracy: 0.9893798828125\nStep: 7700\t eval loss: 0.03432531654834747\t eval accuracy: 0.990234375\n##########################################################\nStep: 7800\t train loss: 0.032442208379507065\t train accuracy: 0.9898681640625\nStep: 7800\t eval loss: 0.038551218807697296\t eval accuracy: 0.9888916015625\n##########################################################\nStep: 7900\t train loss: 0.03235592693090439\t train accuracy: 0.989990234375\nStep: 7900\t eval loss: 0.03298547863960266\t eval accuracy: 0.9906005859375\n##########################################################\nStep: 8000\t train loss: 0.03194452449679375\t train accuracy: 0.9912109375\nStep: 8000\t eval loss: 0.030573494732379913\t eval accuracy: 0.990966796875\n##########################################################\nStep: 8100\t train loss: 0.030116260051727295\t train accuracy: 0.99169921875\nStep: 8100\t eval loss: 0.03110361099243164\t eval accuracy: 0.9908447265625\n##########################################################\nStep: 8200\t train loss: 0.03155096620321274\t train accuracy: 0.9908447265625\nStep: 8200\t eval loss: 0.03445056080818176\t eval accuracy: 0.98974609375\n##########################################################\nStep: 8300\t train loss: 0.03721342980861664\t train accuracy: 0.9896240234375\nStep: 8300\t eval loss: 0.03389648720622063\t eval accuracy: 0.989990234375\n##########################################################\nStep: 8400\t train loss: 0.032558269798755646\t train accuracy: 0.9898681640625\nStep: 8400\t eval loss: 0.033567875623703\t eval accuracy: 0.9906005859375\n##########################################################\nStep: 8500\t train loss: 0.03127501532435417\t train accuracy: 0.990966796875\nStep: 8500\t eval loss: 0.0338069424033165\t eval accuracy: 0.990478515625\n##########################################################\nStep: 8600\t train loss: 0.034035809338092804\t train accuracy: 0.9898681640625\nStep: 8600\t eval loss: 0.034688886255025864\t eval accuracy: 0.9901123046875\n##########################################################\nStep: 8700\t train loss: 0.03388906270265579\t train accuracy: 0.9896240234375\nStep: 8700\t eval loss: 0.03197571635246277\t eval accuracy: 0.99072265625\n##########################################################\nStep: 8800\t train loss: 0.039000362157821655\t train accuracy: 0.9891357421875\nStep: 8800\t eval loss: 0.036250751465559006\t eval accuracy: 0.9886474609375\n##########################################################\nStep: 8900\t train loss: 0.0329742506146431\t train accuracy: 0.9901123046875\nStep: 8900\t eval loss: 0.038447268307209015\t eval accuracy: 0.989501953125\n##########################################################\nStep: 9000\t train loss: 0.031592659652233124\t train accuracy: 0.9906005859375\nStep: 9000\t eval loss: 0.03406347334384918\t eval accuracy: 0.9901123046875\n##########################################################\nStep: 9100\t train loss: 0.0345875546336174\t train accuracy: 0.9896240234375\nStep: 9100\t eval loss: 0.03518277779221535\t eval accuracy: 0.9898681640625\n##########################################################\nStep: 9200\t train loss: 0.03389957547187805\t train accuracy: 0.9903564453125\nStep: 9200\t eval loss: 0.034190211445093155\t eval accuracy: 0.990234375\n##########################################################\nStep: 9300\t train loss: 0.03016055002808571\t train accuracy: 0.9910888671875\nStep: 9300\t eval loss: 0.03238127380609512\t eval accuracy: 0.990234375\n##########################################################\nStep: 9400\t train loss: 0.03303212672472\t train accuracy: 0.9898681640625\nStep: 9400\t eval loss: 0.0322338342666626\t eval accuracy: 0.9903564453125\n##########################################################\nStep: 9500\t train loss: 0.033760152757167816\t train accuracy: 0.99072265625\nStep: 9500\t eval loss: 0.03364118933677673\t eval accuracy: 0.989990234375\n##########################################################\nStep: 9600\t train loss: 0.03535584360361099\t train accuracy: 0.98974609375\nStep: 9600\t eval loss: 0.03642676770687103\t eval accuracy: 0.9892578125\n##########################################################\nStep: 9700\t train loss: 0.03501102700829506\t train accuracy: 0.990478515625\nStep: 9700\t eval loss: 0.03604423999786377\t eval accuracy: 0.9893798828125\n##########################################################\nStep: 9800\t train loss: 0.035036418586969376\t train accuracy: 0.989990234375\nStep: 9800\t eval loss: 0.032927997410297394\t eval accuracy: 0.989990234375\n##########################################################\nStep: 9900\t train loss: 0.03244760259985924\t train accuracy: 0.989990234375\nStep: 9900\t eval loss: 0.03266629949212074\t eval accuracy: 0.9906005859375\n##########################################################\nStep: 10000\t train loss: 0.03200899064540863\t train accuracy: 0.990966796875\nStep: 10000\t eval loss: 0.033647019416093826\t eval accuracy: 0.9896240234375\n##########################################################\nStep: 10100\t train loss: 0.03150671347975731\t train accuracy: 0.990966796875\nStep: 10100\t eval loss: 0.03641335666179657\t eval accuracy: 0.9888916015625\n##########################################################\nStep: 10200\t train loss: 0.031164158135652542\t train accuracy: 0.990966796875\nStep: 10200\t eval loss: 0.03393534570932388\t eval accuracy: 0.990966796875\n##########################################################\nStep: 10300\t train loss: 0.03468218073248863\t train accuracy: 0.989501953125\nStep: 10300\t eval loss: 0.03465905040502548\t eval accuracy: 0.989990234375\n##########################################################\nStep: 10400\t train loss: 0.03291841968894005\t train accuracy: 0.9901123046875\nStep: 10400\t eval loss: 0.03524501249194145\t eval accuracy: 0.989501953125\n##########################################################\nStep: 10500\t train loss: 0.03236006945371628\t train accuracy: 0.989013671875\nStep: 10500\t eval loss: 0.03384188562631607\t eval accuracy: 0.990478515625\n##########################################################\nStep: 10600\t train loss: 0.029166553169488907\t train accuracy: 0.9908447265625\nStep: 10600\t eval loss: 0.033707715570926666\t eval accuracy: 0.9893798828125\n##########################################################\nStep: 10700\t train loss: 0.03173323720693588\t train accuracy: 0.9908447265625\nStep: 10700\t eval loss: 0.03243475407361984\t eval accuracy: 0.989990234375\n##########################################################\nStep: 10800\t train loss: 0.028501978144049644\t train accuracy: 0.9908447265625\nStep: 10800\t eval loss: 0.03143943473696709\t eval accuracy: 0.9901123046875\n##########################################################\nStep: 10900\t train loss: 0.03233183175325394\t train accuracy: 0.989990234375\nStep: 10900\t eval loss: 0.03288447856903076\t eval accuracy: 0.98974609375\n##########################################################\nStep: 11000\t train loss: 0.03210793435573578\t train accuracy: 0.990234375\nStep: 11000\t eval loss: 0.033426590263843536\t eval accuracy: 0.990234375\n##########################################################\nStep: 11100\t train loss: 0.033621080219745636\t train accuracy: 0.9901123046875\nStep: 11100\t eval loss: 0.036639027297496796\t eval accuracy: 0.989501953125\n##########################################################\nStep: 11200\t train loss: 0.0284596998244524\t train accuracy: 0.9913330078125\nStep: 11200\t eval loss: 0.035864926874637604\t eval accuracy: 0.9896240234375\n##########################################################\nStep: 11300\t train loss: 0.032576266676187515\t train accuracy: 0.9893798828125\nStep: 11300\t eval loss: 0.03656356409192085\t eval accuracy: 0.9888916015625\n##########################################################\nStep: 11400\t train loss: 0.032516609877347946\t train accuracy: 0.990234375\nStep: 11400\t eval loss: 0.034348007291555405\t eval accuracy: 0.989990234375\n##########################################################\nStep: 11500\t train loss: 0.027722427621483803\t train accuracy: 0.9910888671875\nStep: 11500\t eval loss: 0.03250409662723541\t eval accuracy: 0.989990234375\n##########################################################\nStep: 11600\t train loss: 0.030124252662062645\t train accuracy: 0.9906005859375\nStep: 11600\t eval loss: 0.03343755006790161\t eval accuracy: 0.990478515625\n##########################################################\nStep: 11700\t train loss: 0.03432375192642212\t train accuracy: 0.9901123046875\nStep: 11700\t eval loss: 0.03169576823711395\t eval accuracy: 0.989990234375\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","File \u001b[0;32m<timed exec>:23\u001b[0m\n","File \u001b[0;32m<string>:1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(_cls, count, mu, nu)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt  # Visualization\n\n# Plot loss and accuracy in subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\nax1.set_title('Loss')\nax2.set_title('Accuracy')\n\n\n\nax1.plot(all_train_losses, label='train_loss')\nax1.plot(all_eval_losses, label='eval_loss')\n\nax2.plot(all_train_accuracy, label='train_accuracy')\nax2.plot(all_test_accuracy, label='eval_accuracy')\n\nax1.legend()\nax2.legend()\nplt.show()\nplt.clf()","metadata":{"execution":{"iopub.status.busy":"2024-05-27T06:25:03.153573Z","iopub.execute_input":"2024-05-27T06:25:03.154189Z","iopub.status.idle":"2024-05-27T06:25:03.659702Z","shell.execute_reply.started":"2024-05-27T06:25:03.154150Z","shell.execute_reply":"2024-05-27T06:25:03.658758Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1500x500 with 2 Axes>","image/png":"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"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 0 Axes>"},"metadata":{}}]},{"cell_type":"code","source":"@partial(jax.jit, static_argnames=(\"length\"))\ndef generate_text(rng, params, var_params, length):\n    def _scan_generate(carry, _):\n        random_key, context = carry\n        logits = model.apply({'params': params, **var_params}, context, training=False, mutable=['other_variables'])[0]\n        rng, rng_subkey = jax.random.split(random_key)\n        new_token = jax.random.categorical(\n          rng_subkey, logits[:, -n_tokens, :], axis=-1, shape=(1, 1)\n        )\n        context = jnp.concatenate([context[:, 1:], new_token], axis=1)\n        return (rng, context), new_token\n\n    _, new_tokens = jax.lax.scan(\n    _scan_generate,\n    (rng, jnp.zeros((1, block_size), dtype=jnp.int32)),\n    (),\n    length=length,\n    )\n    return new_tokens","metadata":{"execution":{"iopub.status.busy":"2024-05-27T06:25:03.661089Z","iopub.execute_input":"2024-05-27T06:25:03.661708Z","iopub.status.idle":"2024-05-27T06:25:03.670461Z","shell.execute_reply.started":"2024-05-27T06:25:03.661671Z","shell.execute_reply":"2024-05-27T06:25:03.669489Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"new_tokenz = 1000\nkey, subkey = jax.random.split(jax.random.PRNGKey(156))\n# key, subkey = jax.random.split(key)\n# token_gen = generate_text(jnp.zeros((1,block_size)).astype(jnp.int32), new_tokenz, {'params': state.params})\ntoken_gen = generate_text(key, params, var_params, new_tokenz)[:, 0, 0].tolist()\nprint(token_gen)\nprint(decode(token_gen))","metadata":{"execution":{"iopub.status.busy":"2024-05-27T06:25:03.671860Z","iopub.execute_input":"2024-05-27T06:25:03.672223Z","iopub.status.idle":"2024-05-27T06:25:07.601795Z","shell.execute_reply.started":"2024-05-27T06:25:03.672191Z","shell.execute_reply":"2024-05-27T06:25:07.600943Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"[24, 33, 15, 17, 31, 31, 1, 25, 63, 1, 61, 53, 50, 52, 1, 57, 46, 1, 51, 39, 63, 1, 60, 47, 41, 43, 1, 44, 39, 41, 49, 6, 0, 14, 27, 24, 13, 26, 21, 26, 15, 17, 31, 13, 14, 17, 24, 10, 0, 32, 46, 43, 43, 58, 58, 43, 1, 58, 46, 43, 56, 1, 50, 47, 52, 45, 1, 53, 44, 1, 56, 43, 1, 50, 39, 41, 43, 1, 57, 53, 53, 42, 1, 57, 46, 53, 52, 43, 58, 41, 43, 10, 0, 18, 39, 56, 58, 56, 50, 47, 39, 57, 58, 1, 40, 43, 39, 57, 1, 40, 53, 63, 1, 44, 56, 53, 59, 50, 50, 7, 57, 43, 56, 52, 1, 63, 53, 59, 1, 40, 43, 43, 57, 1, 21, 1, 53, 59, 50, 42, 6, 1, 49, 52, 53, 58, 1, 42, 53, 1, 51, 59, 41, 49, 51, 43, 1, 57, 39, 60, 47, 52, 42, 1, 58, 46, 39, 60, 43, 1, 19, 47, 56, 58, 1, 46, 39, 42, 57, 1, 57, 47, 51, 43, 43, 42, 1, 40, 59, 40, 43, 39, 50, 50, 1, 51, 43, 2, 0, 26, 47, 60, 43, 1, 42, 59, 58, 1, 51, 59, 52, 41, 43, 1, 41, 47, 43, 1, 44, 53, 56, 1, 57, 53, 1, 58, 46, 43, 39, 56, 56, 43, 1, 47, 52, 49, 1, 57, 39, 56, 42, 6, 0, 33, 54, 53, 58, 6, 1, 39, 42, 1, 53, 59, 1, 59, 52, 57, 59, 47, 56, 42, 6, 1, 52, 53, 59, 1, 54, 56, 43, 57, 1, 58, 46, 1, 58, 46, 39, 56, 56, 43, 1, 43, 50, 50, 53, 39, 58, 1, 40, 43, 45, 57, 6, 1, 47, 52, 39, 56, 56, 63, 8, 0, 13, 52, 1, 63, 43, 58, 1, 46, 43, 1, 42, 57, 6, 1, 39, 1, 53, 44, 1, 63, 53, 59, 56, 42, 1, 51, 39, 56, 56, 43, 42, 63, 1, 50, 43, 8, 1, 19, 47, 52, 44, 21, 13, 10, 0, 28, 53, 59, 52, 41, 43, 1, 58, 46, 43, 1, 50, 47, 41, 46, 43, 6, 1, 46, 43, 1, 58, 46, 39, 51, 1, 39, 1, 40, 43, 57, 8, 0, 29, 33, 17, 32, 10, 0, 13, 52, 42, 1, 46, 43, 1, 46, 39, 56, 58, 46, 39, 60, 43, 1, 39, 57, 1, 39, 1, 46, 39, 58, 1, 50, 43, 39, 60, 43, 1, 56, 53, 1, 39, 51, 1, 21, 1, 57, 53, 51, 43, 52, 8, 0, 0, 21, 1, 61, 46, 43, 1, 50, 43, 43, 52, 1, 57, 53, 59, 1, 39, 52, 42, 50, 39, 47, 56, 1, 52, 39, 58, 1, 61, 43, 1, 51, 63, 1, 41, 56, 43, 1, 51, 63, 1, 57, 58, 1, 58, 53, 1, 51, 43, 1, 47, 57, 1, 57, 39, 50, 50, 1, 61, 46, 43, 1, 57, 51, 43, 51, 1, 46, 43, 56, 50, 1, 58, 53, 56, 1, 46, 43, 52, 53, 52, 57, 6, 0, 19, 24, 27, 33, 15, 23, 21, 26, 15, 17, 31, 10, 0, 13, 1, 45, 53, 53, 1, 59, 57, 1, 52, 47, 50, 63, 1, 58, 47, 53, 56, 1, 51, 39, 57, 1, 58, 46, 56, 47, 50, 6, 1, 46, 39, 58, 6, 1, 47, 52, 8, 1, 26, 53, 56, 57, 1, 58, 43, 56, 1, 39, 1, 54, 53, 52, 45, 1, 44, 53, 50, 43, 39, 56, 1, 57, 59, 56, 1, 58, 46, 43, 1, 46, 53, 52, 1, 39, 52, 56, 43, 44, 53, 56, 1, 57, 47, 56, 1, 57, 47, 42, 43, 43, 39, 58, 1, 57, 59, 57, 6, 1, 52, 53, 52, 43, 45, 43, 8, 0, 27, 1, 57, 58, 1, 40, 43, 44, 53, 56, 1, 57, 61, 43, 1, 54, 43, 56, 63, 1, 47, 57, 1, 45, 53, 59, 52, 58, 43, 57, 0, 35, 46, 53, 50, 43, 1, 57, 46, 8, 0, 32, 46, 39, 60, 43, 1, 50, 47, 41, 47, 43, 63, 1, 61, 46, 43, 1, 42, 43, 50, 57, 1, 57, 54, 43, 56, 1, 57, 46, 43, 1, 57, 53, 1, 51, 39, 49, 1, 63, 53, 59, 52, 52, 43, 1, 58, 46, 39, 58, 43, 1, 41, 39, 50, 1, 61, 43, 39, 51, 43, 52, 58, 1, 52, 53, 58, 1, 46, 39, 58, 1, 57, 39, 58, 1, 48, 59, 51, 43, 52, 42, 1, 46, 43, 1, 44, 47, 58, 63, 1, 47, 57, 44, 43, 8, 0, 0, 16, 33, 15, 17, 31, 10, 0, 31, 43, 59, 56, 1, 61, 47, 58, 46, 59, 57, 1, 46, 43, 1, 44, 53, 56, 56, 41, 46, 1, 46, 43, 1, 39, 50, 41, 43, 51, 63, 1, 57, 53, 39, 50, 50, 1, 58, 46, 43, 1, 61, 53, 59, 56, 1, 43, 57, 1, 40, 39, 41, 43, 50, 58, 1, 46, 43, 1, 53, 44, 1, 39, 52, 51, 47, 41, 53, 51, 53, 53, 49, 1, 57, 47, 58, 46, 43, 6, 1, 58, 46, 1, 51, 47, 57, 46, 43, 1, 57, 39, 52, 42, 6, 1, 58, 43, 8, 0, 0, 24, 33, 15, 21, 59, 1, 39, 1, 46, 39, 60, 43, 42, 1, 44, 53, 56, 1, 39, 52, 1, 41, 39, 58, 1, 41, 53, 51, 40, 43, 51, 6, 1, 46, 39, 58, 59, 40, 1, 46, 39, 50, 50, 1, 46, 43, 1, 41, 53, 59, 1, 41, 46, 1, 43, 62, 43, 12, 0, 35, 47, 58, 1, 57, 1, 61, 43, 1, 51, 43, 42, 63, 1, 51, 43, 1, 52, 39, 58, 43, 59, 41, 46, 1, 21, 1, 59, 54, 54, 43, 1, 51, 53, 59, 52, 1, 47, 57, 1, 58, 43, 1, 40, 43, 39, 56, 43, 39, 58, 1, 45, 56, 5, 57, 1, 58]\nLUCESS My woln sh may vice fack,\nBOLANINCESABEL:\nTheette ther ling of re lace sood shonetce:\nFartrliast beas boy froull-sern you bees I ould, knot do muckme savind thave Girt hads simeed bubeall me!\nNive dut munce cie for so thearre ink sard,\nUpot, ad ou unsuird, nou pres th tharre elloat begs, inarry.\nAn yet he ds, a of yourd marredy le. GinfIA:\nPounce the liche, he tham a bes.\nQUET:\nAnd he harthave as a hat leave ro am I somen.\n\nI whe leen sou andlair nat we my cre my st to me is sall whe smem herl tor henons,\nGLOUCKINCES:\nA goo us nily tior mas thril, hat, in. Nors ter a pong folear sur the hon anrefor sir sideeat sus, nonege.\nO st befor swe pery is gountes\nWhole sh.\nThave liciey whe dels sper she so mak younne thate cal weament not hat sat jumend he fity isfe.\n\nDUCES:\nSeur withus he forrch he alcemy soall the wour es bacelt he of anmicomook sithe, th mishe sand, te.\n\nLUCIu a haved for an cat combem, hatub hall he cou ch exe?\nWit s we medy me nateuch I uppe moun is te beareat gr's t\n","output_type":"stream"}]},{"cell_type":"code","source":"dsfsdhfgjdg hfdgjdgjgfjhs'####################","metadata":{"execution":{"iopub.status.busy":"2024-05-27T06:25:07.603231Z","iopub.execute_input":"2024-05-27T06:25:07.603906Z","iopub.status.idle":"2024-05-27T06:25:07.613163Z","shell.execute_reply.started":"2024-05-27T06:25:07.603867Z","shell.execute_reply":"2024-05-27T06:25:07.608342Z"},"trusted":true},"execution_count":35,"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[35], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    dsfsdhfgjdg hfdgjdgjgfjhs'####################\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 1)\n"],"ename":"SyntaxError","evalue":"unterminated string literal (detected at line 1) (2630675753.py, line 1)","output_type":"error"}]},{"cell_type":"code","source":"var_params['other_variables']['Mamba_0']['hidden_state'].shape","metadata":{"execution":{"iopub.status.busy":"2024-05-27T06:25:07.613863Z","iopub.status.idle":"2024-05-27T06:25:07.614197Z","shell.execute_reply.started":"2024-05-27T06:25:07.614031Z","shell.execute_reply":"2024-05-27T06:25:07.614049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params.keys()","metadata":{"execution":{"iopub.status.busy":"2024-05-27T06:25:07.615683Z","iopub.status.idle":"2024-05-27T06:25:07.616032Z","shell.execute_reply.started":"2024-05-27T06:25:07.615867Z","shell.execute_reply":"2024-05-27T06:25:07.615881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params['Dense_12']['kernel'].shape","metadata":{"execution":{"iopub.status.busy":"2024-05-27T06:25:07.617636Z","iopub.status.idle":"2024-05-27T06:25:07.618092Z","shell.execute_reply.started":"2024-05-27T06:25:07.617865Z","shell.execute_reply":"2024-05-27T06:25:07.617884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rngk = jax.random.PRNGKey(389)\nxs, ys = get_batch(rngk, train_data)\nprint(xs[0])\nprint(ys[0])","metadata":{"execution":{"iopub.status.busy":"2024-05-27T06:25:07.619088Z","iopub.status.idle":"2024-05-27T06:25:07.619416Z","shell.execute_reply.started":"2024-05-27T06:25:07.619250Z","shell.execute_reply":"2024-05-27T06:25:07.619263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logits = model.apply({'params': params, **var_params}, xs[0].reshape((1,64)), training=False, mutable=['other_variables'])[0]\nrng, rng_subkey = jax.random.split(rngk)\nfor pso in range(n_tokens):\n    new_token = jax.random.categorical(\n      rng_subkey, logits[:, -1*(n_tokens-pso), :], axis=-1, shape=(1, 1)\n    )\n    print(new_token)","metadata":{"execution":{"iopub.status.busy":"2024-05-27T06:25:07.620947Z","iopub.status.idle":"2024-05-27T06:25:07.621252Z","shell.execute_reply.started":"2024-05-27T06:25:07.621101Z","shell.execute_reply":"2024-05-27T06:25:07.621113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_tok = [51,49,46,46,46,52]\nprint(decode(ys[0].tolist()))","metadata":{"execution":{"iopub.status.busy":"2024-05-27T06:25:07.622375Z","iopub.status.idle":"2024-05-27T06:25:07.622694Z","shell.execute_reply.started":"2024-05-27T06:25:07.622535Z","shell.execute_reply":"2024-05-27T06:25:07.622548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"act_tk = [60, 43, 50, 57,  1, 47]\nprint(decode(act_tk))","metadata":{"execution":{"iopub.status.busy":"2024-05-27T06:25:07.624156Z","iopub.status.idle":"2024-05-27T06:25:07.624463Z","shell.execute_reply.started":"2024-05-27T06:25:07.624304Z","shell.execute_reply":"2024-05-27T06:25:07.624316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jax.nn.standardize(jnp.array([2.0,3.0,4.0]))","metadata":{"id":"Oe_GIDP2HFyt","outputId":"5d3dce16-fcc2-40b9-c49a-00a8c4013ca2","execution":{"iopub.status.busy":"2024-05-27T06:25:07.625658Z","iopub.status.idle":"2024-05-27T06:25:07.626009Z","shell.execute_reply.started":"2024-05-27T06:25:07.625836Z","shell.execute_reply":"2024-05-27T06:25:07.625854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@struct.dataclass\nclass Metrics(metrics.Collection):\n    accuracy: metrics.Accuracy\n    loss: metrics.Average.from_output('loss')","metadata":{"id":"s3nN1jOiHFyu","execution":{"iopub.status.busy":"2024-05-27T06:25:07.627670Z","iopub.status.idle":"2024-05-27T06:25:07.628010Z","shell.execute_reply.started":"2024-05-27T06:25:07.627850Z","shell.execute_reply":"2024-05-27T06:25:07.627864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainState(train_state.TrainState):\n    metrics: Metrics\n\ndef create_train_state(module, rng, learning_rate, train_shape):\n    \"\"\"Creates an initial `TrainState`.\"\"\"\n    params = module.init(rng, jnp.ones(train_shape).astype(jnp.int32), \n                         training=False)['params'] # initialize parameters by passing a template image\n    tx = optax.adamw(learning_rate)\n    return TrainState.create(\n      apply_fn=module.apply, params=params, tx=tx,\n      metrics=Metrics.empty(),\n    )","metadata":{"id":"7LLDTSFQHFyu","execution":{"iopub.status.busy":"2024-05-27T06:25:07.629225Z","iopub.status.idle":"2024-05-27T06:25:07.629531Z","shell.execute_reply.started":"2024-05-27T06:25:07.629381Z","shell.execute_reply":"2024-05-27T06:25:07.629394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TrainState.create(","metadata":{"execution":{"iopub.status.busy":"2024-05-27T06:25:07.630786Z","iopub.status.idle":"2024-05-27T06:25:07.631114Z","shell.execute_reply.started":"2024-05-27T06:25:07.630962Z","shell.execute_reply":"2024-05-27T06:25:07.630976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@jax.jit\ndef train_step(state, inputs, targets):\n    \"\"\"Train for a single step.\"\"\"\n    def loss_fn(params):\n        logits = state.apply_fn({'params': params}, inputs, training=True, \n                                rngs={\"dropout\": key})[0]\n        loss = optax.softmax_cross_entropy_with_integer_labels(\n            logits=logits, labels=targets).mean()\n        return loss\n    grad_fn = jax.grad(loss_fn)\n    grads = grad_fn(state.params)\n    state = state.apply_gradients(grads=grads)\n    return state","metadata":{"id":"zApWXUDaHFyu","execution":{"iopub.status.busy":"2024-05-27T06:25:07.632349Z","iopub.status.idle":"2024-05-27T06:25:07.632679Z","shell.execute_reply.started":"2024-05-27T06:25:07.632521Z","shell.execute_reply":"2024-05-27T06:25:07.632535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@jax.jit\ndef compute_metrics(*, state, inputs, targets):\n    logits = state.apply_fn({'params': state.params}, inputs, training=False)[0]\n    loss = optax.softmax_cross_entropy_with_integer_labels(\n        logits=logits, labels=targets).mean()\n    metric_updates = state.metrics.single_from_model_output(\n    logits=logits, labels=targets, loss=loss)\n    metrics = state.metrics.merge(metric_updates)\n    state = state.replace(metrics=metrics)\n    return state","metadata":{"id":"VzukZ4iEHFyv","execution":{"iopub.status.busy":"2024-05-27T06:25:07.633957Z","iopub.status.idle":"2024-05-27T06:25:07.634282Z","shell.execute_reply.started":"2024-05-27T06:25:07.634123Z","shell.execute_reply":"2024-05-27T06:25:07.634136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 10\nlearning_rate = 0.005\ninit_rng = jax.random.key(0)","metadata":{"id":"ehYvMeuNHFyv","execution":{"iopub.status.busy":"2024-05-27T06:25:07.635875Z","iopub.status.idle":"2024-05-27T06:25:07.636316Z","shell.execute_reply.started":"2024-05-27T06:25:07.636085Z","shell.execute_reply":"2024-05-27T06:25:07.636105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"state = create_train_state(fin_model, init_rng, learning_rate, train_shape)\ndel init_rng  # Must not be used anymore.","metadata":{"id":"D60UHLFHHFyv","execution":{"iopub.status.busy":"2024-05-27T06:25:07.637641Z","iopub.status.idle":"2024-05-27T06:25:07.638098Z","shell.execute_reply.started":"2024-05-27T06:25:07.637866Z","shell.execute_reply":"2024-05-27T06:25:07.637885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics_history = {'train_loss': [],\n                   'train_accuracy': [],\n                   'test_loss': [],\n                   'test_accuracy': []}","metadata":{"id":"Jl-9TlHEHFyv","execution":{"iopub.status.busy":"2024-05-27T06:25:07.639965Z","iopub.status.idle":"2024-05-27T06:25:07.640411Z","shell.execute_reply.started":"2024-05-27T06:25:07.640177Z","shell.execute_reply":"2024-05-27T06:25:07.640196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 442\nkey = jax.random.PRNGKey(SEED)\nloss = 10\ncounter = 0\n# for step in tqdm(range(max_iters)): # increase number of steps for good results...\nwhile counter==max_iters or loss > 1.0:\n\n      # sample a batch of data\n    xb, yb = get_batch(key, train_data)\n    state = train_step(state, xb, yb)\n    state = compute_metrics(state=state, inputs=xb, targets=yb)\n\n    key = (jax.random.split(key)[0])\n\n    if step == 0 or (step+1) % 100 == 0: # one training epoch has passed\n        for metric,value in state.metrics.compute().items(): # compute metrics\n            metrics_history[f'train_{metric}'].append(value) # record metrics\n        state = state.replace(metrics=state.metrics.empty()) # reset train_metrics for next training epoch\n\n        # Compute metrics on the test set after each training epoch\n        test_state = state\n        x_test, y_test = get_batch(key, test_data)\n    #     for test_batch in test_ds.as_numpy_iterator():\n        test_state = compute_metrics(state=test_state, inputs=x_test, targets=y_test)\n\n        for metric,value in test_state.metrics.compute().items():\n            metrics_history[f'test_{metric}'].append(value)\n\n        print(f\"train epoch: {(step+1)}, \"\n              f\"loss: {metrics_history['train_loss'][-1]}, \"\n              f\"accuracy: {metrics_history['train_accuracy'][-1] * 100}\")\n        print(f\"test epoch: {(step+1) }, \"\n          f\"loss: {metrics_history['test_loss'][-1]}, \"\n          f\"accuracy: {metrics_history['test_accuracy'][-1] * 100}\")","metadata":{"id":"CaNt9JazHFyw","outputId":"ba447ddf-9940-44a6-f4b2-d27ed78a88c2","execution":{"iopub.status.busy":"2024-05-27T06:25:07.641650Z","iopub.status.idle":"2024-05-27T06:25:07.642105Z","shell.execute_reply.started":"2024-05-27T06:25:07.641877Z","shell.execute_reply":"2024-05-27T06:25:07.641896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt  # Visualization\n\n# Plot loss and accuracy in subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\nax1.set_title('Loss')\nax2.set_title('Accuracy')\nfor dataset in ('train','test'):\n    ax1.plot(metrics_history[f'{dataset}_loss'], label=f'{dataset}_loss')\n    ax2.plot(metrics_history[f'{dataset}_accuracy'], label=f'{dataset}_accuracy')\nax1.legend()\nax2.legend()\nplt.show()\nplt.clf()","metadata":{"id":"Y40JGx1YHFyw","execution":{"iopub.status.busy":"2024-05-27T06:25:07.643732Z","iopub.status.idle":"2024-05-27T06:25:07.644078Z","shell.execute_reply.started":"2024-05-27T06:25:07.643919Z","shell.execute_reply":"2024-05-27T06:25:07.643933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nlogits = fin_model.apply(fin_params, xb, training=False)[0]\nloss = optax.softmax_cross_entropy_with_integer_labels(\n            logits=logits, labels=yb).mean()\n\nprint(loss)","metadata":{"id":"7pJlFXpVHFyw","execution":{"iopub.status.busy":"2024-05-27T06:25:07.645586Z","iopub.status.idle":"2024-05-27T06:25:07.646039Z","shell.execute_reply.started":"2024-05-27T06:25:07.645792Z","shell.execute_reply":"2024-05-27T06:25:07.645825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def generate_text(idx, max_new_tokens, params):\n# # idx is (B, T) array of indices in the current context\n#     for i in range(max_new_tokens):\n#         # crop idx to the last block_size tokens\n#         idx_cond = idx[:, -block_size:]\n#         # get the predictions\n#         logits = fin_model.apply(params, idx_cond)\n#         # focus only on the last time step\n#         logits = logits[:, -1, :] # becomes (B, C)\n\n#         if i == 0:\n#             rng, rng_subkey = jax.random.split(jax.random.PRNGKey(12))\n#         else:\n#             rng, rng_subkey = jax.random.split(rng)\n\n#         idx_next = jax.random.categorical(rng_subkey, logits, axis=-1, shape=(1, 1)) # (B, 1)\n\n\n#         # append sampled index to the running sequence\n#         idx = jnp.concatenate([idx, idx_next], axis=-1) # (B, T+1)\n\n#     return idx","metadata":{"id":"9d28o-dTHFyx","execution":{"iopub.status.busy":"2024-05-27T06:25:07.647727Z","iopub.status.idle":"2024-05-27T06:25:07.648178Z","shell.execute_reply.started":"2024-05-27T06:25:07.647952Z","shell.execute_reply":"2024-05-27T06:25:07.647971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@partial(jax.jit, static_argnames=(\"self\", \"length\"))\ndef generate_text(rng, params, length):\n    def _scan_generate(carry, _):\n        random_key, context = carry\n        logits = fin_model.apply(params, context, training=False)[0]\n        rng, rng_subkey = jax.random.split(random_key)\n        new_token = jax.random.categorical(\n          rng_subkey, logits[:, -1, :], axis=-1, shape=(1, 1)\n        )\n        context = jnp.concatenate([context[:, 1:], new_token], axis=1)\n        return (rng, context), new_token\n\n    _, new_tokens = jax.lax.scan(\n    _scan_generate,\n    (rng, jnp.zeros((1, block_size), dtype=jnp.int32)),\n    (),\n    length=length,\n    )\n    return new_tokens","metadata":{"id":"WB0og7pAHFyx","execution":{"iopub.status.busy":"2024-05-27T06:25:07.649288Z","iopub.status.idle":"2024-05-27T06:25:07.649621Z","shell.execute_reply.started":"2024-05-27T06:25:07.649461Z","shell.execute_reply":"2024-05-27T06:25:07.649476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_tokenz = 1000\nkey, subkey = jax.random.split(jax.random.PRNGKey(156))\n# key, subkey = jax.random.split(key)\n# token_gen = generate_text(jnp.zeros((1,block_size)).astype(jnp.int32), new_tokenz, {'params': state.params})\ntoken_gen = generate_text(key, {'params': state.params}, new_tokenz)[:, 0, 0].tolist()\nprint(token_gen)\nprint(decode(token_gen))","metadata":{"id":"50Vpg2lEHFyx","execution":{"iopub.status.busy":"2024-05-27T06:25:07.651032Z","iopub.status.idle":"2024-05-27T06:25:07.651332Z","shell.execute_reply.started":"2024-05-27T06:25:07.651183Z","shell.execute_reply":"2024-05-27T06:25:07.651195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sdgh  fs","metadata":{"execution":{"iopub.status.busy":"2024-05-27T06:25:07.652559Z","iopub.status.idle":"2024-05-27T06:25:07.652887Z","shell.execute_reply.started":"2024-05-27T06:25:07.652709Z","shell.execute_reply":"2024-05-27T06:25:07.652721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"state.params","metadata":{"execution":{"iopub.status.busy":"2024-05-27T06:25:07.654008Z","iopub.status.idle":"2024-05-27T06:25:07.654304Z","shell.execute_reply.started":"2024-05-27T06:25:07.654155Z","shell.execute_reply":"2024-05-27T06:25:07.654168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install mamba-ssm","metadata":{"id":"MOw_xjbrHFy0","execution":{"iopub.status.busy":"2024-05-27T06:25:07.655321Z","iopub.status.idle":"2024-05-27T06:25:07.655647Z","shell.execute_reply.started":"2024-05-27T06:25:07.655488Z","shell.execute_reply":"2024-05-27T06:25:07.655502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ones = lambda *size: torch.ones(*size).float().cuda()\nzeros = lambda *size: torch.zeros(*size).float().cuda()\narange = lambda n: torch.arange(n).float().cuda()\nrand = lambda size: torch.rand(*size).abs().float().cuda()\n\ndef create_torch(S = 128, Ba = 2, D = 4, N = 4):\n    x = rand((Ba, 1, D, S))\n    a = -ones((Ba, N, D, 1))\n    b = ones((Ba, N, 1, S)) * 0.1\n    c = rand((Ba, N, 1, S)) * 0.1\n    delta = rand((Ba, 1, D, S)) * 0.1\n    return x, a, b, c, delta","metadata":{"id":"W_PAnYcEOR22","execution":{"iopub.status.busy":"2024-05-27T06:25:07.656995Z","iopub.status.idle":"2024-05-27T06:25:07.657318Z","shell.execute_reply.started":"2024-05-27T06:25:07.657160Z","shell.execute_reply":"2024-05-27T06:25:07.657174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import selective_scan_cuda\n\nxx, aa, bb, cc, ddelta = create_torch()\ny_from_repo = selective_scan_cuda.fwd(xx.squeeze(1), ddelta.squeeze(1), aa[0].squeeze(-1).T, bb.squeeze(-2)[:, None, :, :], cc.squeeze(-2)[:, None, :, :], None, None, None, False)\ny_from_repo","metadata":{"id":"ykh4GTvtOrak","execution":{"iopub.status.busy":"2024-05-27T06:25:07.661913Z","iopub.status.idle":"2024-05-27T06:25:07.662240Z","shell.execute_reply.started":"2024-05-27T06:25:07.662082Z","shell.execute_reply":"2024-05-27T06:25:07.662096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def discretize(a, b, delta):\n    da = delta * a\n    a_ = jnp.exp(da)\n    b_ = b * delta\n    return a_, b_\n\ndef ssm(x, a, b, c, delta):\n    \"Jax Implementation\"\n    y = []\n    h = 0\n    a_, b_ = discretize(a, b, delta)\n    for k in range(x.shape[-1]):\n        h = a_[..., k] * h + b_[..., k] * x[..., k]\n        y.append((c[..., k] * h).sum(1, keepdims=True))\n    return h, jnp.stack(y, -1)\n","metadata":{"id":"NEdG1yPNOtxU","execution":{"iopub.status.busy":"2024-05-27T06:25:07.663701Z","iopub.status.idle":"2024-05-27T06:25:07.664092Z","shell.execute_reply.started":"2024-05-27T06:25:07.663915Z","shell.execute_reply":"2024-05-27T06:25:07.663932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_, y_ = ssm(xx.cpu().numpy(), aa.cpu().numpy(), bb.cpu().numpy(), cc.cpu().numpy(), ddelta.cpu().numpy())","metadata":{"id":"GEjNcZSZPIp_","execution":{"iopub.status.busy":"2024-05-27T06:25:07.665567Z","iopub.status.idle":"2024-05-27T06:25:07.665904Z","shell.execute_reply.started":"2024-05-27T06:25:07.665718Z","shell.execute_reply":"2024-05-27T06:25:07.665731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"tWlqZZOmPnYk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from mamba_ssm import Mamba as Mamba_T\ntorch_mamba = Mamba_T(\n      # This module uses roughly 3 * expand * d_model^2 parameters\n      d_model=n_embd, # Model dimension d_model\n      d_state=16,  # SSM state expansion factor\n      d_conv=4,    # Local convolution width\n      expand=2,    # Block expansion factor\n)","metadata":{"id":"5RHAE_I1Pql9","execution":{"iopub.status.busy":"2024-05-27T06:25:07.667357Z","iopub.status.idle":"2024-05-27T06:25:07.667660Z","shell.execute_reply.started":"2024-05-27T06:25:07.667511Z","shell.execute_reply":"2024-05-27T06:25:07.667524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xm = x = rand((1, 1, n_embd, 32))\nxm.shape","metadata":{"id":"l9zw_M-USrDt","execution":{"iopub.status.busy":"2024-05-27T06:25:07.668762Z","iopub.status.idle":"2024-05-27T06:25:07.669112Z","shell.execute_reply.started":"2024-05-27T06:25:07.668950Z","shell.execute_reply":"2024-05-27T06:25:07.668964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch_mamba(xm.squeeze(1))","metadata":{"id":"gGmA2EWlTCo0","execution":{"iopub.status.busy":"2024-05-27T06:25:07.670311Z","iopub.status.idle":"2024-05-27T06:25:07.670630Z","shell.execute_reply.started":"2024-05-27T06:25:07.670470Z","shell.execute_reply":"2024-05-27T06:25:07.670484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch_mamba.in_proj","metadata":{"id":"73ek9mx9UBBl","execution":{"iopub.status.busy":"2024-05-27T06:25:07.672121Z","iopub.status.idle":"2024-05-27T06:25:07.672423Z","shell.execute_reply.started":"2024-05-27T06:25:07.672274Z","shell.execute_reply":"2024-05-27T06:25:07.672286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import CLIPTokenizer\ntokenizer_1 = CLIPTokenizer.from_pretrained('openai/clip-vit-base-patch32')","metadata":{"id":"P3l_ssIYbiYT","execution":{"iopub.status.busy":"2024-05-27T06:25:07.673435Z","iopub.status.idle":"2024-05-27T06:25:07.673730Z","shell.execute_reply.started":"2024-05-27T06:25:07.673581Z","shell.execute_reply":"2024-05-27T06:25:07.673593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenise_prompts(prompt):\n    inputs = []\n    for tokenizer in [tokenizer_1, tokenizer_2]:\n        text_inputs = tokenizer(\n            positive_prompt,\n            padding=\"max_length\",\n            max_length=tokenizer.model_max_length,\n            truncation=True,\n            return_tensors=\"np\",\n        )\n        inputs.append(text_inputs.input_ids)\n    return jnp.stack(inputs, axis=1)","metadata":{"id":"-X7hXQRMZhl3","execution":{"iopub.status.busy":"2024-05-27T06:25:07.674942Z","iopub.status.idle":"2024-05-27T06:25:07.675240Z","shell.execute_reply.started":"2024-05-27T06:25:07.675090Z","shell.execute_reply":"2024-05-27T06:25:07.675102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xm.squeeze(1).shape","metadata":{"id":"rJhKQ_Oua9Gy","execution":{"iopub.status.busy":"2024-05-27T06:25:07.676319Z","iopub.status.idle":"2024-05-27T06:25:07.676621Z","shell.execute_reply.started":"2024-05-27T06:25:07.676471Z","shell.execute_reply":"2024-05-27T06:25:07.676484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"mzkoYrSVkoJj"},"execution_count":null,"outputs":[]}]}