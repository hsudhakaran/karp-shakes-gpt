{"metadata":{"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8613626,"sourceType":"datasetVersion","datasetId":5155031}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install -q clu","metadata":{"id":"gS6euWNvHFye","outputId":"45b149a7-9450-439c-da67-ab8678a3b0d0","execution":{"iopub.status.busy":"2024-06-26T11:16:43.319847Z","iopub.execute_input":"2024-06-26T11:16:43.320260Z","iopub.status.idle":"2024-06-26T11:16:43.325454Z","shell.execute_reply.started":"2024-06-26T11:16:43.320226Z","shell.execute_reply":"2024-06-26T11:16:43.324566Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# # We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt","metadata":{"id":"7jjCLfuUHFyg","outputId":"dfe048f0-dd44-40ef-edf3-2fa56558672f","execution":{"iopub.status.busy":"2024-06-26T11:16:43.331844Z","iopub.execute_input":"2024-06-26T11:16:43.332183Z","iopub.status.idle":"2024-06-26T11:16:43.336645Z","shell.execute_reply.started":"2024-06-26T11:16:43.332158Z","shell.execute_reply":"2024-06-26T11:16:43.335847Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from functools import partial\nimport jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\nfrom jax.nn.initializers import lecun_normal, normal\nfrom jax.numpy.linalg import eigh, inv, matrix_power\nfrom jax.scipy.signal import convolve\n\nimport tensorflow_datasets as tfds\n\nimport torch\n\nfrom dataclasses import dataclass\n\nfrom typing import Union\n\nimport matplotlib.pyplot as plt\nimport seaborn\n\n# from clu import metrics\nfrom flax.training import train_state  # Useful dataclass to keep train state\nfrom flax import struct                # Flax dataclasses\nimport optax                           # Common loss functions and optimizers\nfrom tqdm import tqdm\n\nimport pdb","metadata":{"id":"YXSCJzupHFyh","execution":{"iopub.status.busy":"2024-06-26T11:16:43.342868Z","iopub.execute_input":"2024-06-26T11:16:43.343150Z","iopub.status.idle":"2024-06-26T11:16:47.394376Z","shell.execute_reply.started":"2024-06-26T11:16:43.343128Z","shell.execute_reply":"2024-06-26T11:16:47.393344Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# read it in to inspect it\n# with open('/kaggle/input/shak-new-input/input.txt', 'r', encoding='utf-8') as f:\n#     text = f.read()","metadata":{"id":"KpJoV3KQHFyh","execution":{"iopub.status.busy":"2024-06-26T11:16:47.396209Z","iopub.execute_input":"2024-06-26T11:16:47.396695Z","iopub.status.idle":"2024-06-26T11:16:47.400940Z","shell.execute_reply.started":"2024-06-26T11:16:47.396668Z","shell.execute_reply":"2024-06-26T11:16:47.399894Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# ds = tfds.load(\"tiny_shakespeare\")\n\n# # combine train and test examples into a single string\n# text_train = \"\"\n# for example in ds[\"train\"].concatenate(ds[\"test\"]).as_numpy_iterator():\n#     text_train += example[\"text\"].decode(\"utf-8\")\n\n# # similarly, create a single string for validation\n# text_validation = \"\"\n# for example in ds[\"validation\"].as_numpy_iterator():\n#     text_validation += example[\"text\"].decode(\"utf-8\")","metadata":{"execution":{"iopub.status.busy":"2024-06-26T11:16:47.402046Z","iopub.execute_input":"2024-06-26T11:16:47.402397Z","iopub.status.idle":"2024-06-26T11:16:47.409427Z","shell.execute_reply.started":"2024-06-26T11:16:47.402366Z","shell.execute_reply":"2024-06-26T11:16:47.408567Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# del ds\n# with open('train.txt', 'w') as file:\n#     file.write(text_train)\n    \n# with open('test.txt', 'w') as file:\n#     file.write(text_validation)    ","metadata":{"execution":{"iopub.status.busy":"2024-06-26T11:16:47.412029Z","iopub.execute_input":"2024-06-26T11:16:47.412896Z","iopub.status.idle":"2024-06-26T11:16:47.416739Z","shell.execute_reply.started":"2024-06-26T11:16:47.412865Z","shell.execute_reply":"2024-06-26T11:16:47.415799Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/input/shak-new-input/train.txt', 'r', encoding='utf-8') as f:\n    text_train = f.read()\n    \nwith open('/kaggle/input/shak-new-input/test.txt', 'r', encoding='utf-8') as f:\n    text_validation = f.read()    ","metadata":{"execution":{"iopub.status.busy":"2024-06-26T11:16:47.417844Z","iopub.execute_input":"2024-06-26T11:16:47.418194Z","iopub.status.idle":"2024-06-26T11:16:47.443811Z","shell.execute_reply.started":"2024-06-26T11:16:47.418161Z","shell.execute_reply":"2024-06-26T11:16:47.443111Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# here are all the unique characters that occur in this text\nchars = sorted(list(set(text_train+text_validation)))\n# chars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)","metadata":{"id":"PsWxZqyRHFyi","outputId":"b1730724-647e-45cd-edfa-97af24995830","execution":{"iopub.status.busy":"2024-06-26T11:16:47.444858Z","iopub.execute_input":"2024-06-26T11:16:47.445217Z","iopub.status.idle":"2024-06-26T11:16:47.468692Z","shell.execute_reply.started":"2024-06-26T11:16:47.445185Z","shell.execute_reply":"2024-06-26T11:16:47.467729Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n65\n","output_type":"stream"}]},{"cell_type":"code","source":"# from transformers import AutoTokenizer\n\n# tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Phi-3-mini-4k-instruct\", padding_side=\"left\")","metadata":{"execution":{"iopub.status.busy":"2024-06-26T11:16:47.469805Z","iopub.execute_input":"2024-06-26T11:16:47.470052Z","iopub.status.idle":"2024-06-26T11:16:47.475242Z","shell.execute_reply.started":"2024-06-26T11:16:47.470023Z","shell.execute_reply":"2024-06-26T11:16:47.474434Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# text_inputs = tokenizer(text, return_tensors=\"np\")\n# data = jnp.array(text_inputs['input_ids'][0])","metadata":{"execution":{"iopub.status.busy":"2024-06-26T11:16:47.476316Z","iopub.execute_input":"2024-06-26T11:16:47.476585Z","iopub.status.idle":"2024-06-26T11:16:47.483305Z","shell.execute_reply.started":"2024-06-26T11:16:47.476563Z","shell.execute_reply":"2024-06-26T11:16:47.482280Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# vocab_size = tokenizer.vocab_size\n# print(vocab_size)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T11:16:47.484544Z","iopub.execute_input":"2024-06-26T11:16:47.484971Z","iopub.status.idle":"2024-06-26T11:16:47.492963Z","shell.execute_reply.started":"2024-06-26T11:16:47.484941Z","shell.execute_reply":"2024-06-26T11:16:47.492143Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# print(tokenizer.decode((text_inputs['input_ids'][0][0:100]).tolist()))","metadata":{"execution":{"iopub.status.busy":"2024-06-26T11:16:47.496350Z","iopub.execute_input":"2024-06-26T11:16:47.496871Z","iopub.status.idle":"2024-06-26T11:16:47.501134Z","shell.execute_reply.started":"2024-06-26T11:16:47.496838Z","shell.execute_reply":"2024-06-26T11:16:47.500292Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# create a mapping from characters to integers\nstoi = { ch: i for i,ch in enumerate(chars) }\nitos = { i: ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))","metadata":{"id":"S-mzLOk1HFyi","outputId":"f56e2f85-5a1c-4099-87df-436ba39f4363","execution":{"iopub.status.busy":"2024-06-26T11:16:47.502354Z","iopub.execute_input":"2024-06-26T11:16:47.502960Z","iopub.status.idle":"2024-06-26T11:16:47.509853Z","shell.execute_reply.started":"2024-06-26T11:16:47.502929Z","shell.execute_reply":"2024-06-26T11:16:47.508937Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"[46, 47, 47, 1, 58, 46, 43, 56, 43]\nhii there\n","output_type":"stream"}]},{"cell_type":"code","source":"# data = jnp.array(encode(text), dtype=jnp.int32)\n# print(data.shape, data.dtype)\n# print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this","metadata":{"id":"HImuqDd8HFyj","outputId":"91dcd15f-f068-4551-ad29-e6e41e52fd91","execution":{"iopub.status.busy":"2024-06-26T11:16:47.510961Z","iopub.execute_input":"2024-06-26T11:16:47.511347Z","iopub.status.idle":"2024-06-26T11:16:47.518523Z","shell.execute_reply.started":"2024-06-26T11:16:47.511317Z","shell.execute_reply":"2024-06-26T11:16:47.517668Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# train_test_split = 0.9\n# n = int(train_test_split*len(data))\n# train_data = data[:n]\n# test_data = data[n:]\n\ntrain_data = jnp.array(encode(text_train), dtype=jnp.int32)\ntest_data = jnp.array(encode(text_validation), dtype=jnp.int32)","metadata":{"id":"pXrAqMxRHFyj","execution":{"iopub.status.busy":"2024-06-26T11:16:47.519586Z","iopub.execute_input":"2024-06-26T11:16:47.520319Z","iopub.status.idle":"2024-06-26T11:16:49.801550Z","shell.execute_reply.started":"2024-06-26T11:16:47.520294Z","shell.execute_reply":"2024-06-26T11:16:49.800526Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"block_size = 8\ntrain_data[:block_size+1]","metadata":{"id":"ahhKyiAzHFyj","outputId":"98306c96-5082-4dfa-ba66-915051831fc8","execution":{"iopub.status.busy":"2024-06-26T11:16:49.802837Z","iopub.execute_input":"2024-06-26T11:16:49.803156Z","iopub.status.idle":"2024-06-26T11:16:49.883294Z","shell.execute_reply.started":"2024-06-26T11:16:49.803129Z","shell.execute_reply":"2024-06-26T11:16:49.882357Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"Array([18, 47, 56, 57, 58,  1, 15, 47, 58], dtype=int32)"},"metadata":{}}]},{"cell_type":"code","source":"x = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")","metadata":{"id":"HIpsznQmHFyk","outputId":"be9d197b-0b79-43ed-f3a9-e74295d51c79","execution":{"iopub.status.busy":"2024-06-26T11:16:49.884409Z","iopub.execute_input":"2024-06-26T11:16:49.884728Z","iopub.status.idle":"2024-06-26T11:16:50.483829Z","shell.execute_reply.started":"2024-06-26T11:16:49.884689Z","shell.execute_reply":"2024-06-26T11:16:50.482914Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"when input is [18] the target: 47\nwhen input is [18 47] the target: 56\nwhen input is [18 47 56] the target: 57\nwhen input is [18 47 56 57] the target: 58\nwhen input is [18 47 56 57 58] the target: 1\nwhen input is [18 47 56 57 58  1] the target: 15\nwhen input is [18 47 56 57 58  1 15] the target: 47\nwhen input is [18 47 56 57 58  1 15 47] the target: 58\n","output_type":"stream"}]},{"cell_type":"code","source":"batch_size = 128 # how many independent sequences will we process in parallel?\nblock_size = 64 # what is the maximum context length for predictions?\nmax_iters = 50000\nlearning_rate = 5e-4\n# device = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 100\nn_embd = 256\nexpans = 2\nn_heads = 1\nchannel_size = n_embd // n_heads\nn_layers = 6\ndropout = 0.2\nconv_k_size = 3\nn_latent_dim = 16\nn_tokens = 1\n\nrng_key = jax.random.PRNGKey(1564)\n\ndynamic_slice_vmap = jax.vmap(jax.lax.dynamic_slice, in_axes=(None, 0, None))\n\n@jax.jit\ndef get_batch(random_key, data):\n    \"\"\"Prepares a random batch of training data.\n\n    Args:\n      random_key: A random seed for sampling a batch.\n      data: The complete training dataset.\n\n    Returns:\n      x: Input sequences.\n      y: Target sequences (shifted inputs).\n    \"\"\"\n    ix = jax.random.randint(\n      random_key, shape=(batch_size, 1), minval=0, maxval=len(data) - block_size\n    )\n    x = dynamic_slice_vmap(data, ix, (block_size,))\n    y = dynamic_slice_vmap(data, ix + n_tokens, (block_size,))\n    return x, y\n\nxb, yb = get_batch(rng_key, train_data)\ntrain_shape = xb.shape\nprint('inputs:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\n# print('----')\n\n# for b in range(batch_size): # batch dimension\n#     for t in range(block_size): # time dimension\n#         context = xb[b, :t+1]\n#         target = yb[b,t]\n#         print(f\"when input is {context} the target: {target}\")","metadata":{"id":"UuAjtqPeHFyk","outputId":"6a88fb2b-b798-4ee9-9f4f-f38ce898d576","execution":{"iopub.status.busy":"2024-06-26T11:16:50.484997Z","iopub.execute_input":"2024-06-26T11:16:50.485355Z","iopub.status.idle":"2024-06-26T11:16:50.808272Z","shell.execute_reply.started":"2024-06-26T11:16:50.485327Z","shell.execute_reply":"2024-06-26T11:16:50.807296Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"inputs:\n(128, 64)\n[[12  0  0 ... 53 42  1]\n [ 1 44 39 ... 56  1 45]\n [56 57  8 ... 10  0 17]\n ...\n [54 53 53 ... 17 17 26]\n [59 58 58 ... 16 21 33]\n [50  1 57 ... 47 58  1]]\ntargets:\n(128, 64)\n[[ 0  0 15 ... 42  1 40]\n [44 39 56 ...  1 45 56]\n [57  8  0 ...  0 17 47]\n ...\n [53 53 56 ... 17 26  1]\n [58 58 43 ... 21 33 31]\n [ 1 57 53 ... 58  1 40]]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(xb[0])\nprint(yb[0])","metadata":{"execution":{"iopub.status.busy":"2024-06-26T11:16:50.809319Z","iopub.execute_input":"2024-06-26T11:16:50.809586Z","iopub.status.idle":"2024-06-26T11:16:50.909315Z","shell.execute_reply.started":"2024-06-26T11:16:50.809564Z","shell.execute_reply":"2024-06-26T11:16:50.908330Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"[12  0  0 15 27 25 21 26 21 33 31 10  0  5 32 61 39 57  1 44 56 53 51  1\n 58 46 43  1 41 39 52 53 52  8  0  0 15 27 30 21 27 24 13 26 33 31 10  0\n  5 31 46 39 50 50  5  2  0 27  1 45 53 53 42  1]\n[ 0  0 15 27 25 21 26 21 33 31 10  0  5 32 61 39 57  1 44 56 53 51  1 58\n 46 43  1 41 39 52 53 52  8  0  0 15 27 30 21 27 24 13 26 33 31 10  0  5\n 31 46 39 50 50  5  2  0 27  1 45 53 53 42  1 40]\n","output_type":"stream"}]},{"cell_type":"code","source":"# hidden_state = [jnp.zeros((1,n_latent_dim, n_embd * expans)) for _ in range(n_layers)]\n# hidden_state[0].shape","metadata":{"execution":{"iopub.status.busy":"2024-06-26T11:16:50.910363Z","iopub.execute_input":"2024-06-26T11:16:50.910635Z","iopub.status.idle":"2024-06-26T11:16:50.914876Z","shell.execute_reply.started":"2024-06-26T11:16:50.910613Z","shell.execute_reply":"2024-06-26T11:16:50.913840Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# Mamba Block\nDense --> Conv1D --> Silu --> SSM --> Silu -->","metadata":{"id":"yOccqzJlHFym"}},{"cell_type":"code","source":"class Mamba(nn.Module):\n\n    def setup(self):\n        emb_features = n_embd * expans\n        self.in_proj1 = nn.Conv(features=n_embd, kernel_size=conv_k_size,padding=1) #nn.Dense(features=emb_features)\n        self.in_proj2 = nn.Dense(features=emb_features)\n\n        # Adjusted for Flax. Flax does not have nn.Conv1d, so you might need to reshape or use a different approach\n        self.conv1d = nn.Conv(features=emb_features,\n                              kernel_size=conv_k_size,\n                              padding=1,\n                              )\n\n        self.A = -1*self.param('A', nn.initializers.ones, (1, n_latent_dim, emb_features, 1))\n        self.B = 0.1*self.param('B', nn.initializers.ones, (1, n_latent_dim, 1, block_size))\n        self.C = 0.09*self.param('C', jax.random.normal, (1, n_latent_dim, 1, block_size))\n        self.D = 0.1*self.param('D', jax.random.normal, (1, 1,emb_features, block_size))\n        self.delta = 0.05*self.param('delta', jax.random.normal, (1, 1,emb_features, block_size))\n\n        self.out_proj = nn.Dense(n_embd // n_heads)\n        \n        self.hidden_state = self.variable('other_variables','hidden_state', \n                                          jnp.zeros, \n                                          (1,n_latent_dim, emb_features))\n#         self.rms_norm = nn.RMSNorm()\n\n    def __call__(self, embeds):\n        x = self.in_proj1(embeds)\n        x = jax.nn.silu(x) #new\n        x = self.conv1d(x)\n        x = jax.nn.silu(x)\n#         pdb.set_trace()\n\n        x = jnp.expand_dims(jnp.transpose(x,(0,2,1)), axis=1)\n#         x = x.reshape((x.shape[0],1,x.shape[2],x.shape[1]))\n        x = self.ssm(x)\n#         x = x.reshape((x.shape[0],x.shape[3],x.shape[2]))\n        x = jnp.transpose(x[:,0,:,:],(0,2,1))\n        x = x*jnp.concatenate([jax.nn.silu(self.in_proj2(embeds))[:,1:,:],jnp.ones((x.shape[0],1,x.shape[-1]))], axis=1)\n\n        x = self.out_proj(x)\n\n#         x = self.rms_norm(x)\n\n        return x\n    def discretize(self):\n        da = self.delta * self.A\n        a_ = jnp.exp(da)\n        b_ = self.B * self.delta\n        return a_, b_\n\n    def ssm(self, x):\n        a_, b_ = self.discretize()\n        h = 0\n        for k in range(x.shape[-1]):\n            h = a_[..., k] * h + b_[..., k] * x[..., k]\n#         _, N, D, S = a_.shape\n#         indices = jnp.tril(jnp.ones((S-1,S-1))) \n#         indices = jnp.expand_dims(a_[...,1:],axis=4)*jnp.expand_dims(indices, axis=(0,1,2)) + jnp.expand_dims(jnp.triu(jnp.ones((S-1,S-1)),1), axis=(0,1,2))\n#         indices = (jnp.concatenate((indices, jnp.ones((1,N,D,S-1,1))), axis=-1)).prod(axis=-2)\n#         h = (indices*(b_*x)).sum(axis=-1)\n\n        y = ((self.C * jax.lax.expand_dims(h,[3])).sum(1, keepdims=True) + self.D*x)\n        \n#         self.hidden_state.value = jax.nn.standardize(h.mean(0, keepdims=True))\n        return y","metadata":{"id":"4qOdblU5HFyo","execution":{"iopub.status.busy":"2024-06-26T11:16:50.916308Z","iopub.execute_input":"2024-06-26T11:16:50.916616Z","iopub.status.idle":"2024-06-26T11:16:50.934967Z","shell.execute_reply.started":"2024-06-26T11:16:50.916586Z","shell.execute_reply":"2024-06-26T11:16:50.934065Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# class MultiHeadMamba(nn.Module):\n#     def setup(self):\n#         self.heads = [Mamba() for _ in range(n_heads)]\n#         self.rms_norm = nn.RMSNorm()\n\n#     def __call__(self, x):\n#         out = jnp.concatenate([h(x) for h in self.heads], axis=-1)\n#         x = self.rms_norm(out)\n#         return x","metadata":{"id":"0bH9vlLZHFyq","execution":{"iopub.status.busy":"2024-06-26T11:16:50.936181Z","iopub.execute_input":"2024-06-26T11:16:50.936519Z","iopub.status.idle":"2024-06-26T11:16:50.945476Z","shell.execute_reply.started":"2024-06-26T11:16:50.936488Z","shell.execute_reply":"2024-06-26T11:16:50.944647Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# class FeedForward(nn.Module):\n#     def setup(self):\n#         self.ffn = nn.Sequential([\n#             nn.Dense(4 * n_embd),\n#             nn.relu,\n#             nn.Dense(n_embd)]\n#         )\n#     def __call__(self, x):\n#         return self.ffn(x)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T11:16:50.946478Z","iopub.execute_input":"2024-06-26T11:16:50.946775Z","iopub.status.idle":"2024-06-26T11:16:50.953999Z","shell.execute_reply.started":"2024-06-26T11:16:50.946751Z","shell.execute_reply":"2024-06-26T11:16:50.953127Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# class MambaBlock(nn.Module):\n#     def setup(self):\n#         self.mamba_block = Mamba()\n#         self.ln1 = nn.RMSNorm()\n#         self.ffn = FeedForward()\n#         self.ln2 = nn.LayerNorm()\n\n#     def __call__(self, x):\n#         x = x + self.mamba_block(self.ln2(x))\n#         x = x + self.ffn(self.ln1(x))\n#         return x\n","metadata":{"id":"UiCxIjoEp2QA","execution":{"iopub.status.busy":"2024-06-26T11:16:50.955258Z","iopub.execute_input":"2024-06-26T11:16:50.955825Z","iopub.status.idle":"2024-06-26T11:16:50.961986Z","shell.execute_reply.started":"2024-06-26T11:16:50.955801Z","shell.execute_reply":"2024-06-26T11:16:50.961190Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# class MambaModel(nn.Module):\n\n#     def setup(self):\n#         self.tok_embeddings = nn.Embed(vocab_size, n_embd)\n#         self.pos_embeddings = nn.Embed(block_size, n_embd)\n#         self.ln = nn.LayerNorm()\n#         self.mamba_layers = [MambaBlock() for _ in range(n_layers)]\n#         self.preds_out = nn.Dense(vocab_size)\n\n#     def __call__(self, x, training: bool):\n#         x = self.tok_embeddings(x) + self.pos_embeddings(jnp.arange(block_size))\n# #         x = self.ln(x)\n#         for layer in self.mamba_layers:\n#             x = layer(x)\n            \n#         return self.preds_out(x)\n\n#     @jax.jit\n#     def generate(self, idx, max_new_tokens, params):\n#     # idx is (B, T) array of indices in the current context\n#         for _ in range(max_new_tokens):\n#             # crop idx to the last block_size tokens\n#             idx_cond = idx[:, -block_size:]\n#             # get the predictions\n#             logits = self.apply(params, idx_cond)\n#             # focus only on the last time step\n#             logits = logits[:, -1, :] # becomes (B, C)\n#             # apply softmax to get probabilities\n#             ##probs = tf.keras.activations.softmax(logits, dim=-1) # (B, C)\n#             # sample from the distribution\n#             idx_next = jax.random.categorical(jax.random.PRNGKey(52), logits) # (B, 1)\n#             # append sampled index to the running sequence\n#             idx = jax.numpy.expand_dims(jnp.concatenate([idx[0], idx_next], axis=0), 0) # (B, T+1)\n#     #         print(idx_next)\n#     #         print(idx)\n\n#         return idx","metadata":{"id":"y4C7OWL8HFyq","execution":{"iopub.status.busy":"2024-06-26T11:16:50.963189Z","iopub.execute_input":"2024-06-26T11:16:50.964058Z","iopub.status.idle":"2024-06-26T11:16:50.970809Z","shell.execute_reply.started":"2024-06-26T11:16:50.964026Z","shell.execute_reply":"2024-06-26T11:16:50.970125Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# model = Mamba()\n# params = model.init(jax.random.key(42), jnp.ones((1,64,256)))\n# # print(params['other_variables']['hidden_state'].shape, params['other_variables']['hidden_state'].min(), params['other_variables']['hidden_state'].max())\n# # print(model.tabulate(jax.random.key(0), jnp.ones((1,64,256)),\n# #                    compute_flops=True, compute_vjp_flops=True))\n# xs = model.apply(params, jnp.ones((1,64,256)), mutable=['other_variables'])\n# # # print(params['other_variables']['hidden_state'].shape, params['other_variables']['hidden_state'].min(), params['other_variables']['hidden_state'].max())\n# xb.shape, xs[0].shape, xs[1].keys()","metadata":{"id":"wTd3jSQWHFyp","execution":{"iopub.status.busy":"2024-06-26T11:16:50.971959Z","iopub.execute_input":"2024-06-26T11:16:50.972310Z","iopub.status.idle":"2024-06-26T11:16:50.981391Z","shell.execute_reply.started":"2024-06-26T11:16:50.972279Z","shell.execute_reply":"2024-06-26T11:16:50.980593Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# print(xs[1]['other_variables']['hidden_state'].shape, xs[1]['other_variables']['hidden_state'].min(), xs[1]['other_variables']['hidden_state'].max())","metadata":{"execution":{"iopub.status.busy":"2024-06-26T11:16:50.982435Z","iopub.execute_input":"2024-06-26T11:16:50.982956Z","iopub.status.idle":"2024-06-26T11:16:50.992692Z","shell.execute_reply.started":"2024-06-26T11:16:50.982926Z","shell.execute_reply":"2024-06-26T11:16:50.991883Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# xfs = model.apply(params, 2*jnp.ones((1,64,256)), mutable=['other_variables'])\n# print(params['other_variables']['hidden_state'].shape, params['other_variables']['hidden_state'].min(), params['other_variables']['hidden_state'].max())\n# print(xfs[1]['other_variables']['hidden_state'].shape, xfs[1]['other_variables']['hidden_state'].min(), xfs[1]['other_variables']['hidden_state'].max())","metadata":{"execution":{"iopub.status.busy":"2024-06-26T11:16:50.993584Z","iopub.execute_input":"2024-06-26T11:16:50.993813Z","iopub.status.idle":"2024-06-26T11:16:50.999733Z","shell.execute_reply.started":"2024-06-26T11:16:50.993793Z","shell.execute_reply":"2024-06-26T11:16:50.998835Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# test_model = Mamba()\n# test_params = test_model.init(jax.random.key(42), xb)\n# n_params = sum(p.size for p in jax.tree_util.tree_leaves(test_params))\n# print(f\"Total number of parameters: {n_params:_}\")\n# # print(fin_model.tabulate(jax.random.key(42), xb,\n# #                    compute_flops=True, compute_vjp_flops=True))\n# xf = test_model.apply(test_params, xb)\n# xb.shape, xf.shape","metadata":{"id":"cm2a0nepHFyq","execution":{"iopub.status.busy":"2024-06-26T11:16:51.000901Z","iopub.execute_input":"2024-06-26T11:16:51.001251Z","iopub.status.idle":"2024-06-26T11:16:51.008238Z","shell.execute_reply.started":"2024-06-26T11:16:51.001220Z","shell.execute_reply":"2024-06-26T11:16:51.007476Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"class NanoLM(nn.Module):\n    \"\"\"NanoLM model.\"\"\"\n    vocab_size: int = 65\n    num_layers: int = 6\n    num_heads: int = 8\n    head_size: int = 32\n    dropout_rate: float = 0.2\n    embed_size: int = 256\n    block_size: int = 64\n\n    @nn.compact\n    def __call__(self, x, training: bool):\n        breakpoint()\n        x = nn.Embed(self.vocab_size, self.embed_size)(x) + nn.Embed(\n            self.block_size, self.embed_size\n        )(jnp.arange(self.block_size))\n        \n        for i in range(self.num_layers):\n#             x = x + nn.MultiHeadDotProductAttention(\n#               num_heads=self.num_heads,\n#               qkv_features=self.head_size,\n#               out_features=self.head_size * self.num_heads,\n#               dropout_rate=self.dropout_rate,\n#             )(\n#               x_norm,\n#               x_norm,\n#               mask=jnp.tril(jnp.ones((x.shape[-2], x.shape[-2]))),\n#               deterministic=not training,\n#             )\n    \n            x = Mamba()(nn.RMSNorm()(x)) * jnp.concatenate([x[:,1:,:],jnp.ones((x.shape[0],1,x.shape[-1]))], axis=1)\n#             x = x + nn.Sequential([\n#               nn.Dense(4 * self.embed_size),\n#               nn.relu,\n#               nn.Dropout(self.dropout_rate, deterministic=not training),\n#               nn.Dense(self.embed_size),\n#             ])(nn.RMSNorm()(x))\n\n        x = nn.Dense(self.vocab_size)(nn.RMSNorm()(x))\n        return x","metadata":{"id":"zuiaFP6WHFyr","execution":{"iopub.status.busy":"2024-06-26T11:16:51.015699Z","iopub.execute_input":"2024-06-26T11:16:51.015960Z","iopub.status.idle":"2024-06-26T11:16:51.025686Z","shell.execute_reply.started":"2024-06-26T11:16:51.015938Z","shell.execute_reply":"2024-06-26T11:16:51.024836Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# key = jax.random.key(42)\n\n# # fin_model = MambaModel()\n# # fin_params = fin_model.init(key, xb, training=False)\n\n\n# fin_model = NanoLM(\n#     vocab_size=vocab_size,\n#     num_layers=n_layers,\n#     num_heads=8,\n#     head_size=32,\n#     dropout_rate=0.2,\n#     embed_size=n_embd,\n#     block_size=block_size,\n# )\n\n# fin_params = fin_model.init(\n#     {'params': key},\n#     jnp.ones((batch_size, block_size), dtype=jnp.int32),\n#     training=False\n# )\n\n# n_params = sum(p.size for p in jax.tree_util.tree_leaves(fin_params))\n# print(f\"Total number of parameters: {n_params:_}\")\n# # print(fin_model.tabulate(jax.random.key(42), xb,\n# #                    compute_flops=True, compute_vjp_flops=True))\n# xf = fin_model.apply(fin_params, xb, training=False)[0]\n# xb.shape, xf.shape","metadata":{"id":"fnUQPyuvHFys","outputId":"f04ebf31-d67f-4488-dd5d-7fd5b20dd1ea","execution":{"iopub.status.busy":"2024-06-26T11:16:51.026891Z","iopub.execute_input":"2024-06-26T11:16:51.027225Z","iopub.status.idle":"2024-06-26T11:16:51.036062Z","shell.execute_reply.started":"2024-06-26T11:16:51.027200Z","shell.execute_reply":"2024-06-26T11:16:51.035288Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"def loss_fun(params, x, y, var_params,dropout_key):\n    logits, updated_variables = model.apply({'params': params, **var_params}, x, training=True, rngs={\"dropout\": dropout_key}, mutable=['other_variables'])\n    accuracy = jnp.mean(jnp.argmax(logits, axis=-1) == y)\n    return optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=y).mean(), (updated_variables, accuracy)\n\n@jax.jit\ndef eval_step(params, x, y, var_params):\n    logits, _ = model.apply({'params': params, **var_params}, x, training=False, mutable=['other_variables'])\n    accuracy = jnp.mean(jnp.argmax(logits, axis=-1) == y)\n    return optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=y).mean(), accuracy","metadata":{"execution":{"iopub.status.busy":"2024-06-26T11:16:51.037144Z","iopub.execute_input":"2024-06-26T11:16:51.037477Z","iopub.status.idle":"2024-06-26T11:16:51.048544Z","shell.execute_reply.started":"2024-06-26T11:16:51.037446Z","shell.execute_reply":"2024-06-26T11:16:51.047698Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"key = jax.random.PRNGKey(42)\nkey, subkey = jax.random.split(key)\n\nmodel = NanoLM(\n    vocab_size=vocab_size,\n    num_layers=n_layers,\n    num_heads=8,\n    head_size=32,\n    dropout_rate=0.2,\n    embed_size=n_embd,\n    block_size=block_size,\n)\n\nvar_params = model.init(\n    key,\n    jnp.ones((batch_size, block_size), dtype=jnp.int32),\n    training=False,\n)\nprint(var_params.keys())\nn_params = sum(p.size for p in jax.tree_util.tree_leaves(var_params))\n\nprint(f\"Total number of parameters: {n_params:_}\")","metadata":{"id":"PKpb3864HFyt","execution":{"iopub.status.busy":"2024-06-26T11:16:51.050827Z","iopub.execute_input":"2024-06-26T11:16:51.051204Z","iopub.status.idle":"2024-06-26T11:17:00.816320Z","shell.execute_reply.started":"2024-06-26T11:16:51.051165Z","shell.execute_reply":"2024-06-26T11:17:00.815343Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"dict_keys(['params', 'other_variables'])\nTotal number of parameters: 5_676_353\n","output_type":"stream"}]},{"cell_type":"code","source":"var_params['params']['Embed_0']['embedding'].shape","metadata":{"execution":{"iopub.status.busy":"2024-06-26T11:17:00.817644Z","iopub.execute_input":"2024-06-26T11:17:00.817941Z","iopub.status.idle":"2024-06-26T11:17:00.823977Z","shell.execute_reply.started":"2024-06-26T11:17:00.817916Z","shell.execute_reply":"2024-06-26T11:17:00.823010Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"(65, 256)"},"metadata":{}}]},{"cell_type":"code","source":"params = var_params.pop('params')","metadata":{"execution":{"iopub.status.busy":"2024-06-26T11:17:00.825030Z","iopub.execute_input":"2024-06-26T11:17:00.825360Z","iopub.status.idle":"2024-06-26T11:17:00.835063Z","shell.execute_reply.started":"2024-06-26T11:17:00.825332Z","shell.execute_reply":"2024-06-26T11:17:00.834267Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"var_params = jax.tree_map(lambda x: jnp.zeros_like(x), var_params)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T11:17:00.836458Z","iopub.execute_input":"2024-06-26T11:17:00.836752Z","iopub.status.idle":"2024-06-26T11:17:00.847239Z","shell.execute_reply.started":"2024-06-26T11:17:00.836729Z","shell.execute_reply":"2024-06-26T11:17:00.846377Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# decay_rate = 0.96\n# learning_rate_schedule = optax.exponential_decay(learning_rate, decay_rate, max_iters//1000)\nopt = optax.adamw(learning_rate=learning_rate)\n\nopt_state = opt.init(params)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T11:17:00.848382Z","iopub.execute_input":"2024-06-26T11:17:00.848647Z","iopub.status.idle":"2024-06-26T11:17:01.236406Z","shell.execute_reply.started":"2024-06-26T11:17:00.848625Z","shell.execute_reply":"2024-06-26T11:17:01.235590Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"%%time\n\nall_train_losses = []\nall_eval_losses = []\n\nall_train_accuracy =  []\nall_test_accuracy = []\n\n# we define one iteration of the optimizer and JIT this function\n@jax.jit\ndef step(key, params, var_params, opt_state):\n    key, subkey = jax.random.split(key)\n    xb, yb = get_batch(key, train_data)\n    (loss, aux_data), grad = jax.value_and_grad(loss_fun, has_aux=True)(params, xb, yb, var_params, subkey)\n    var_params, train_accuracy = aux_data\n    updates, opt_state = opt.update(grad, opt_state, params)\n    params = optax.apply_updates(params, updates)\n    return params, key, opt_state, loss, var_params, train_accuracy\n\n# for i in tqdm(range(max_iters)):\ncounter = 0\nloss = 10\nwhile counter<max_iters: # and loss > 1.0:\n\n    params, key, opt_state, loss, var_params, train_accuracy = step(key, params, var_params, opt_state)\n    \n\n    # once every N_FREQ_EVAL we compute loss on the validation set\n    if counter % eval_iters == 0:\n        key, subkey = jax.random.split(key)\n        eval_loss, eval_accuracy = eval_step(params, *get_batch(subkey, test_data), var_params)\n        all_train_losses.append(loss)\n        all_eval_losses.append(eval_loss)\n        all_train_accuracy.append(train_accuracy)\n        all_test_accuracy.append(eval_accuracy)\n        print('##########################################################')\n        print(\"Step: \", counter,\"\\t Train Loss: \", loss,\"\\t Train Accuracy: \", format(train_accuracy, \".2%\"))\n        print(\"Step: \", counter,\"\\t Eval Loss: \", eval_loss,\"\\t Eval Accuracy: \", format(eval_accuracy, \".2%\"))\n        \n    counter += 1\n        ","metadata":{"execution":{"iopub.status.busy":"2024-06-26T11:17:01.237676Z","iopub.execute_input":"2024-06-26T11:17:01.238337Z","iopub.status.idle":"2024-06-26T13:46:47.504137Z","shell.execute_reply.started":"2024-06-26T11:17:01.238303Z","shell.execute_reply":"2024-06-26T13:46:47.503110Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"##########################################################\nStep:  0 \t Train Loss:  4.182764 \t Train Accuracy:  3.19%\nStep:  0 \t Eval Loss:  4.1668806 \t Eval Accuracy:  14.90%\n##########################################################\nStep:  100 \t Train Loss:  3.977272 \t Train Accuracy:  17.92%\nStep:  100 \t Eval Loss:  3.975817 \t Eval Accuracy:  17.71%\n##########################################################\nStep:  200 \t Train Loss:  3.9194193 \t Train Accuracy:  18.13%\nStep:  200 \t Eval Loss:  3.920799 \t Eval Accuracy:  18.42%\n##########################################################\nStep:  300 \t Train Loss:  3.8276703 \t Train Accuracy:  19.01%\nStep:  300 \t Eval Loss:  3.8364005 \t Eval Accuracy:  19.08%\n##########################################################\nStep:  400 \t Train Loss:  3.77316 \t Train Accuracy:  19.74%\nStep:  400 \t Eval Loss:  3.7727327 \t Eval Accuracy:  19.52%\n##########################################################\nStep:  500 \t Train Loss:  3.7241933 \t Train Accuracy:  19.37%\nStep:  500 \t Eval Loss:  3.7349508 \t Eval Accuracy:  19.40%\n##########################################################\nStep:  600 \t Train Loss:  3.6872444 \t Train Accuracy:  19.81%\nStep:  600 \t Eval Loss:  3.6929126 \t Eval Accuracy:  19.69%\n##########################################################\nStep:  700 \t Train Loss:  3.6638246 \t Train Accuracy:  19.32%\nStep:  700 \t Eval Loss:  3.6609735 \t Eval Accuracy:  19.65%\n##########################################################\nStep:  800 \t Train Loss:  3.6302803 \t Train Accuracy:  19.07%\nStep:  800 \t Eval Loss:  3.6325989 \t Eval Accuracy:  19.47%\n##########################################################\nStep:  900 \t Train Loss:  3.6090806 \t Train Accuracy:  19.14%\nStep:  900 \t Eval Loss:  3.591358 \t Eval Accuracy:  19.63%\n##########################################################\nStep:  1000 \t Train Loss:  3.5406508 \t Train Accuracy:  19.85%\nStep:  1000 \t Eval Loss:  3.5679235 \t Eval Accuracy:  19.21%\n##########################################################\nStep:  1100 \t Train Loss:  3.4893615 \t Train Accuracy:  20.74%\nStep:  1100 \t Eval Loss:  3.5046015 \t Eval Accuracy:  20.52%\n##########################################################\nStep:  1200 \t Train Loss:  3.446525 \t Train Accuracy:  20.70%\nStep:  1200 \t Eval Loss:  3.4611907 \t Eval Accuracy:  20.89%\n##########################################################\nStep:  1300 \t Train Loss:  3.430615 \t Train Accuracy:  20.80%\nStep:  1300 \t Eval Loss:  3.421093 \t Eval Accuracy:  21.13%\n##########################################################\nStep:  1400 \t Train Loss:  3.3911395 \t Train Accuracy:  21.09%\nStep:  1400 \t Eval Loss:  3.410997 \t Eval Accuracy:  21.01%\n##########################################################\nStep:  1500 \t Train Loss:  3.376534 \t Train Accuracy:  21.35%\nStep:  1500 \t Eval Loss:  3.40015 \t Eval Accuracy:  20.80%\n##########################################################\nStep:  1600 \t Train Loss:  3.3530686 \t Train Accuracy:  21.22%\nStep:  1600 \t Eval Loss:  3.3746667 \t Eval Accuracy:  21.45%\n##########################################################\nStep:  1700 \t Train Loss:  3.335843 \t Train Accuracy:  21.09%\nStep:  1700 \t Eval Loss:  3.3604655 \t Eval Accuracy:  20.96%\n##########################################################\nStep:  1800 \t Train Loss:  3.32296 \t Train Accuracy:  21.13%\nStep:  1800 \t Eval Loss:  3.35837 \t Eval Accuracy:  20.68%\n##########################################################\nStep:  1900 \t Train Loss:  3.3233132 \t Train Accuracy:  21.14%\nStep:  1900 \t Eval Loss:  3.3202405 \t Eval Accuracy:  21.02%\n##########################################################\nStep:  2000 \t Train Loss:  3.310266 \t Train Accuracy:  20.76%\nStep:  2000 \t Eval Loss:  3.3007617 \t Eval Accuracy:  20.95%\n##########################################################\nStep:  2100 \t Train Loss:  3.309504 \t Train Accuracy:  20.47%\nStep:  2100 \t Eval Loss:  3.2932243 \t Eval Accuracy:  21.23%\n##########################################################\nStep:  2200 \t Train Loss:  3.2687464 \t Train Accuracy:  21.29%\nStep:  2200 \t Eval Loss:  3.262181 \t Eval Accuracy:  21.41%\n##########################################################\nStep:  2300 \t Train Loss:  3.2718427 \t Train Accuracy:  20.68%\nStep:  2300 \t Eval Loss:  3.2724998 \t Eval Accuracy:  20.95%\n##########################################################\nStep:  2400 \t Train Loss:  3.2345662 \t Train Accuracy:  21.31%\nStep:  2400 \t Eval Loss:  3.281128 \t Eval Accuracy:  20.37%\n##########################################################\nStep:  2500 \t Train Loss:  3.242529 \t Train Accuracy:  20.81%\nStep:  2500 \t Eval Loss:  3.272018 \t Eval Accuracy:  20.56%\n##########################################################\nStep:  2600 \t Train Loss:  3.2387118 \t Train Accuracy:  21.08%\nStep:  2600 \t Eval Loss:  3.2520719 \t Eval Accuracy:  20.62%\n##########################################################\nStep:  2700 \t Train Loss:  3.2190576 \t Train Accuracy:  21.33%\nStep:  2700 \t Eval Loss:  3.2253504 \t Eval Accuracy:  21.04%\n##########################################################\nStep:  2800 \t Train Loss:  3.2233193 \t Train Accuracy:  20.83%\nStep:  2800 \t Eval Loss:  3.2269783 \t Eval Accuracy:  20.72%\n##########################################################\nStep:  2900 \t Train Loss:  3.1992111 \t Train Accuracy:  21.09%\nStep:  2900 \t Eval Loss:  3.204938 \t Eval Accuracy:  21.37%\n##########################################################\nStep:  3000 \t Train Loss:  3.189901 \t Train Accuracy:  20.72%\nStep:  3000 \t Eval Loss:  3.1908073 \t Eval Accuracy:  21.23%\n##########################################################\nStep:  3100 \t Train Loss:  3.1532779 \t Train Accuracy:  21.13%\nStep:  3100 \t Eval Loss:  3.1612146 \t Eval Accuracy:  21.56%\n##########################################################\nStep:  3200 \t Train Loss:  3.135346 \t Train Accuracy:  22.09%\nStep:  3200 \t Eval Loss:  3.1697574 \t Eval Accuracy:  21.80%\n##########################################################\nStep:  3300 \t Train Loss:  3.1239712 \t Train Accuracy:  22.23%\nStep:  3300 \t Eval Loss:  3.1361983 \t Eval Accuracy:  21.84%\n##########################################################\nStep:  3400 \t Train Loss:  3.0731943 \t Train Accuracy:  22.44%\nStep:  3400 \t Eval Loss:  3.0961962 \t Eval Accuracy:  22.41%\n##########################################################\nStep:  3500 \t Train Loss:  3.0683022 \t Train Accuracy:  23.29%\nStep:  3500 \t Eval Loss:  3.0826855 \t Eval Accuracy:  23.01%\n##########################################################\nStep:  3600 \t Train Loss:  2.9858348 \t Train Accuracy:  24.35%\nStep:  3600 \t Eval Loss:  3.022245 \t Eval Accuracy:  24.39%\n##########################################################\nStep:  3700 \t Train Loss:  2.9789886 \t Train Accuracy:  24.87%\nStep:  3700 \t Eval Loss:  2.9545941 \t Eval Accuracy:  25.24%\n##########################################################\nStep:  3800 \t Train Loss:  2.8658857 \t Train Accuracy:  27.10%\nStep:  3800 \t Eval Loss:  2.8902233 \t Eval Accuracy:  27.15%\n##########################################################\nStep:  3900 \t Train Loss:  2.7875998 \t Train Accuracy:  28.32%\nStep:  3900 \t Eval Loss:  2.824016 \t Eval Accuracy:  28.17%\n##########################################################\nStep:  4000 \t Train Loss:  2.6473713 \t Train Accuracy:  31.53%\nStep:  4000 \t Eval Loss:  2.6304374 \t Eval Accuracy:  32.42%\n##########################################################\nStep:  4100 \t Train Loss:  2.3781419 \t Train Accuracy:  37.11%\nStep:  4100 \t Eval Loss:  2.4001286 \t Eval Accuracy:  36.45%\n##########################################################\nStep:  4200 \t Train Loss:  1.9914522 \t Train Accuracy:  45.24%\nStep:  4200 \t Eval Loss:  1.9977872 \t Eval Accuracy:  45.36%\n##########################################################\nStep:  4300 \t Train Loss:  1.3726728 \t Train Accuracy:  60.82%\nStep:  4300 \t Eval Loss:  1.3898683 \t Eval Accuracy:  60.10%\n##########################################################\nStep:  4400 \t Train Loss:  0.8382372 \t Train Accuracy:  76.23%\nStep:  4400 \t Eval Loss:  0.8404218 \t Eval Accuracy:  76.03%\n##########################################################\nStep:  4500 \t Train Loss:  0.53608364 \t Train Accuracy:  84.23%\nStep:  4500 \t Eval Loss:  0.5263783 \t Eval Accuracy:  83.89%\n##########################################################\nStep:  4600 \t Train Loss:  0.30516616 \t Train Accuracy:  91.47%\nStep:  4600 \t Eval Loss:  0.35516757 \t Eval Accuracy:  89.81%\n##########################################################\nStep:  4700 \t Train Loss:  0.23189387 \t Train Accuracy:  93.47%\nStep:  4700 \t Eval Loss:  0.23365971 \t Eval Accuracy:  93.58%\n##########################################################\nStep:  4800 \t Train Loss:  0.16759309 \t Train Accuracy:  95.63%\nStep:  4800 \t Eval Loss:  0.17128801 \t Eval Accuracy:  95.46%\n##########################################################\nStep:  4900 \t Train Loss:  0.14388311 \t Train Accuracy:  96.03%\nStep:  4900 \t Eval Loss:  0.1467569 \t Eval Accuracy:  96.18%\n##########################################################\nStep:  5000 \t Train Loss:  0.11225142 \t Train Accuracy:  97.31%\nStep:  5000 \t Eval Loss:  0.096874066 \t Eval Accuracy:  97.56%\n##########################################################\nStep:  5100 \t Train Loss:  0.09447164 \t Train Accuracy:  97.58%\nStep:  5100 \t Eval Loss:  0.10477353 \t Eval Accuracy:  97.09%\n##########################################################\nStep:  5200 \t Train Loss:  0.070091985 \t Train Accuracy:  98.28%\nStep:  5200 \t Eval Loss:  0.0847218 \t Eval Accuracy:  97.63%\n##########################################################\nStep:  5300 \t Train Loss:  0.06218889 \t Train Accuracy:  98.47%\nStep:  5300 \t Eval Loss:  0.07117006 \t Eval Accuracy:  98.08%\n##########################################################\nStep:  5400 \t Train Loss:  0.05552965 \t Train Accuracy:  98.54%\nStep:  5400 \t Eval Loss:  0.064852655 \t Eval Accuracy:  98.22%\n##########################################################\nStep:  5500 \t Train Loss:  0.0536858 \t Train Accuracy:  98.68%\nStep:  5500 \t Eval Loss:  0.06478214 \t Eval Accuracy:  98.46%\n##########################################################\nStep:  5600 \t Train Loss:  0.050906867 \t Train Accuracy:  98.75%\nStep:  5600 \t Eval Loss:  0.058778957 \t Eval Accuracy:  98.43%\n##########################################################\nStep:  5700 \t Train Loss:  0.046325397 \t Train Accuracy:  98.83%\nStep:  5700 \t Eval Loss:  0.05972642 \t Eval Accuracy:  98.38%\n##########################################################\nStep:  5800 \t Train Loss:  0.04410039 \t Train Accuracy:  98.97%\nStep:  5800 \t Eval Loss:  0.041460708 \t Eval Accuracy:  98.83%\n##########################################################\nStep:  5900 \t Train Loss:  0.048434723 \t Train Accuracy:  98.68%\nStep:  5900 \t Eval Loss:  0.04788415 \t Eval Accuracy:  98.73%\n##########################################################\nStep:  6000 \t Train Loss:  0.042583242 \t Train Accuracy:  98.94%\nStep:  6000 \t Eval Loss:  0.043024186 \t Eval Accuracy:  98.95%\n##########################################################\nStep:  6100 \t Train Loss:  0.04658038 \t Train Accuracy:  98.69%\nStep:  6100 \t Eval Loss:  0.04158409 \t Eval Accuracy:  98.85%\n##########################################################\nStep:  6200 \t Train Loss:  0.0445648 \t Train Accuracy:  98.80%\nStep:  6200 \t Eval Loss:  0.047532305 \t Eval Accuracy:  98.61%\n##########################################################\nStep:  6300 \t Train Loss:  0.03599543 \t Train Accuracy:  99.00%\nStep:  6300 \t Eval Loss:  0.04029745 \t Eval Accuracy:  98.82%\n##########################################################\nStep:  6400 \t Train Loss:  0.034039795 \t Train Accuracy:  99.12%\nStep:  6400 \t Eval Loss:  0.036363825 \t Eval Accuracy:  98.94%\n##########################################################\nStep:  6500 \t Train Loss:  0.0484203 \t Train Accuracy:  98.75%\nStep:  6500 \t Eval Loss:  0.046088807 \t Eval Accuracy:  98.71%\n##########################################################\nStep:  6600 \t Train Loss:  0.039416056 \t Train Accuracy:  98.96%\nStep:  6600 \t Eval Loss:  0.035335317 \t Eval Accuracy:  99.05%\n##########################################################\nStep:  6700 \t Train Loss:  0.029571656 \t Train Accuracy:  99.11%\nStep:  6700 \t Eval Loss:  0.028993199 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  6800 \t Train Loss:  0.037410744 \t Train Accuracy:  98.99%\nStep:  6800 \t Eval Loss:  0.037859734 \t Eval Accuracy:  98.96%\n##########################################################\nStep:  6900 \t Train Loss:  0.0347222 \t Train Accuracy:  98.97%\nStep:  6900 \t Eval Loss:  0.03750933 \t Eval Accuracy:  98.94%\n##########################################################\nStep:  7000 \t Train Loss:  0.0330755 \t Train Accuracy:  99.01%\nStep:  7000 \t Eval Loss:  0.03801887 \t Eval Accuracy:  98.75%\n##########################################################\nStep:  7100 \t Train Loss:  0.031680178 \t Train Accuracy:  99.05%\nStep:  7100 \t Eval Loss:  0.035451215 \t Eval Accuracy:  98.96%\n##########################################################\nStep:  7200 \t Train Loss:  0.03079737 \t Train Accuracy:  99.15%\nStep:  7200 \t Eval Loss:  0.03406627 \t Eval Accuracy:  99.05%\n##########################################################\nStep:  7300 \t Train Loss:  0.030759564 \t Train Accuracy:  99.08%\nStep:  7300 \t Eval Loss:  0.038983002 \t Eval Accuracy:  98.88%\n##########################################################\nStep:  7400 \t Train Loss:  0.037092343 \t Train Accuracy:  98.99%\nStep:  7400 \t Eval Loss:  0.040490683 \t Eval Accuracy:  98.88%\n##########################################################\nStep:  7500 \t Train Loss:  0.026974658 \t Train Accuracy:  99.28%\nStep:  7500 \t Eval Loss:  0.036069985 \t Eval Accuracy:  98.95%\n##########################################################\nStep:  7600 \t Train Loss:  0.035733767 \t Train Accuracy:  98.97%\nStep:  7600 \t Eval Loss:  0.04168623 \t Eval Accuracy:  98.85%\n##########################################################\nStep:  7700 \t Train Loss:  0.046862334 \t Train Accuracy:  98.55%\nStep:  7700 \t Eval Loss:  0.047454905 \t Eval Accuracy:  98.66%\n##########################################################\nStep:  7800 \t Train Loss:  0.031663753 \t Train Accuracy:  99.24%\nStep:  7800 \t Eval Loss:  0.032728657 \t Eval Accuracy:  99.15%\n##########################################################\nStep:  7900 \t Train Loss:  0.0299939 \t Train Accuracy:  99.24%\nStep:  7900 \t Eval Loss:  0.032347463 \t Eval Accuracy:  99.00%\n##########################################################\nStep:  8000 \t Train Loss:  0.028219812 \t Train Accuracy:  99.18%\nStep:  8000 \t Eval Loss:  0.028708577 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  8100 \t Train Loss:  0.028092705 \t Train Accuracy:  99.22%\nStep:  8100 \t Eval Loss:  0.029355263 \t Eval Accuracy:  99.11%\n##########################################################\nStep:  8200 \t Train Loss:  0.02664372 \t Train Accuracy:  99.26%\nStep:  8200 \t Eval Loss:  0.02937988 \t Eval Accuracy:  99.08%\n##########################################################\nStep:  8300 \t Train Loss:  0.036768615 \t Train Accuracy:  98.94%\nStep:  8300 \t Eval Loss:  0.03629815 \t Eval Accuracy:  99.06%\n##########################################################\nStep:  8400 \t Train Loss:  0.0314747 \t Train Accuracy:  99.10%\nStep:  8400 \t Eval Loss:  0.03467478 \t Eval Accuracy:  99.10%\n##########################################################\nStep:  8500 \t Train Loss:  0.027387638 \t Train Accuracy:  99.15%\nStep:  8500 \t Eval Loss:  0.03574162 \t Eval Accuracy:  98.90%\n##########################################################\nStep:  8600 \t Train Loss:  0.029794153 \t Train Accuracy:  99.12%\nStep:  8600 \t Eval Loss:  0.032031827 \t Eval Accuracy:  99.10%\n##########################################################\nStep:  8700 \t Train Loss:  0.028638415 \t Train Accuracy:  99.21%\nStep:  8700 \t Eval Loss:  0.030446935 \t Eval Accuracy:  99.05%\n##########################################################\nStep:  8800 \t Train Loss:  0.066263236 \t Train Accuracy:  98.11%\nStep:  8800 \t Eval Loss:  0.06304605 \t Eval Accuracy:  98.16%\n##########################################################\nStep:  8900 \t Train Loss:  0.03734104 \t Train Accuracy:  98.99%\nStep:  8900 \t Eval Loss:  0.034848835 \t Eval Accuracy:  98.97%\n##########################################################\nStep:  9000 \t Train Loss:  0.033681832 \t Train Accuracy:  98.94%\nStep:  9000 \t Eval Loss:  0.03240036 \t Eval Accuracy:  99.16%\n##########################################################\nStep:  9100 \t Train Loss:  0.023322012 \t Train Accuracy:  99.41%\nStep:  9100 \t Eval Loss:  0.032693155 \t Eval Accuracy:  99.13%\n##########################################################\nStep:  9200 \t Train Loss:  0.030188046 \t Train Accuracy:  99.17%\nStep:  9200 \t Eval Loss:  0.028717937 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  9300 \t Train Loss:  0.022802953 \t Train Accuracy:  99.35%\nStep:  9300 \t Eval Loss:  0.034120232 \t Eval Accuracy:  99.08%\n##########################################################\nStep:  9400 \t Train Loss:  0.03057538 \t Train Accuracy:  99.07%\nStep:  9400 \t Eval Loss:  0.02519574 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  9500 \t Train Loss:  0.026887652 \t Train Accuracy:  99.23%\nStep:  9500 \t Eval Loss:  0.027046204 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  9600 \t Train Loss:  0.024231795 \t Train Accuracy:  99.30%\nStep:  9600 \t Eval Loss:  0.03136181 \t Eval Accuracy:  99.06%\n##########################################################\nStep:  9700 \t Train Loss:  0.029780317 \t Train Accuracy:  99.13%\nStep:  9700 \t Eval Loss:  0.034515887 \t Eval Accuracy:  99.05%\n##########################################################\nStep:  9800 \t Train Loss:  0.027819972 \t Train Accuracy:  99.24%\nStep:  9800 \t Eval Loss:  0.0319338 \t Eval Accuracy:  99.06%\n##########################################################\nStep:  9900 \t Train Loss:  0.042169973 \t Train Accuracy:  98.71%\nStep:  9900 \t Eval Loss:  0.03826761 \t Eval Accuracy:  98.99%\n##########################################################\nStep:  10000 \t Train Loss:  0.029487379 \t Train Accuracy:  99.07%\nStep:  10000 \t Eval Loss:  0.03343952 \t Eval Accuracy:  99.00%\n##########################################################\nStep:  10100 \t Train Loss:  0.027018778 \t Train Accuracy:  99.16%\nStep:  10100 \t Eval Loss:  0.02826786 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  10200 \t Train Loss:  0.02886826 \t Train Accuracy:  99.22%\nStep:  10200 \t Eval Loss:  0.029593524 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  10300 \t Train Loss:  0.027369892 \t Train Accuracy:  99.23%\nStep:  10300 \t Eval Loss:  0.031006647 \t Eval Accuracy:  99.11%\n##########################################################\nStep:  10400 \t Train Loss:  0.02221781 \t Train Accuracy:  99.38%\nStep:  10400 \t Eval Loss:  0.029354809 \t Eval Accuracy:  99.12%\n##########################################################\nStep:  10500 \t Train Loss:  0.02867863 \t Train Accuracy:  99.16%\nStep:  10500 \t Eval Loss:  0.036854956 \t Eval Accuracy:  98.97%\n##########################################################\nStep:  10600 \t Train Loss:  0.02672197 \t Train Accuracy:  99.13%\nStep:  10600 \t Eval Loss:  0.029605582 \t Eval Accuracy:  99.17%\n##########################################################\nStep:  10700 \t Train Loss:  0.030527363 \t Train Accuracy:  99.08%\nStep:  10700 \t Eval Loss:  0.0349552 \t Eval Accuracy:  98.94%\n##########################################################\nStep:  10800 \t Train Loss:  0.026236525 \t Train Accuracy:  99.22%\nStep:  10800 \t Eval Loss:  0.029543804 \t Eval Accuracy:  99.08%\n##########################################################\nStep:  10900 \t Train Loss:  0.0235526 \t Train Accuracy:  99.29%\nStep:  10900 \t Eval Loss:  0.025991427 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  11000 \t Train Loss:  0.02825604 \t Train Accuracy:  99.16%\nStep:  11000 \t Eval Loss:  0.031836703 \t Eval Accuracy:  99.08%\n##########################################################\nStep:  11100 \t Train Loss:  0.034560688 \t Train Accuracy:  98.99%\nStep:  11100 \t Eval Loss:  0.035005853 \t Eval Accuracy:  98.97%\n##########################################################\nStep:  11200 \t Train Loss:  0.029718399 \t Train Accuracy:  99.24%\nStep:  11200 \t Eval Loss:  0.033174917 \t Eval Accuracy:  99.04%\n##########################################################\nStep:  11300 \t Train Loss:  0.025278214 \t Train Accuracy:  99.32%\nStep:  11300 \t Eval Loss:  0.02953637 \t Eval Accuracy:  99.10%\n##########################################################\nStep:  11400 \t Train Loss:  0.027228191 \t Train Accuracy:  99.19%\nStep:  11400 \t Eval Loss:  0.027192913 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  11500 \t Train Loss:  0.025386935 \t Train Accuracy:  99.17%\nStep:  11500 \t Eval Loss:  0.027222041 \t Eval Accuracy:  99.12%\n##########################################################\nStep:  11600 \t Train Loss:  0.025499238 \t Train Accuracy:  99.22%\nStep:  11600 \t Eval Loss:  0.025882425 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  11700 \t Train Loss:  0.025264286 \t Train Accuracy:  99.24%\nStep:  11700 \t Eval Loss:  0.027742416 \t Eval Accuracy:  99.13%\n##########################################################\nStep:  11800 \t Train Loss:  0.026421998 \t Train Accuracy:  99.24%\nStep:  11800 \t Eval Loss:  0.032721102 \t Eval Accuracy:  99.02%\n##########################################################\nStep:  11900 \t Train Loss:  0.022805627 \t Train Accuracy:  99.26%\nStep:  11900 \t Eval Loss:  0.024738446 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  12000 \t Train Loss:  0.031626035 \t Train Accuracy:  99.02%\nStep:  12000 \t Eval Loss:  0.030425075 \t Eval Accuracy:  99.15%\n##########################################################\nStep:  12100 \t Train Loss:  0.03815905 \t Train Accuracy:  98.86%\nStep:  12100 \t Eval Loss:  0.035796493 \t Eval Accuracy:  98.85%\n##########################################################\nStep:  12200 \t Train Loss:  0.02416282 \t Train Accuracy:  99.29%\nStep:  12200 \t Eval Loss:  0.029265132 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  12300 \t Train Loss:  0.024773873 \t Train Accuracy:  99.27%\nStep:  12300 \t Eval Loss:  0.02712523 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  12400 \t Train Loss:  0.03539668 \t Train Accuracy:  98.90%\nStep:  12400 \t Eval Loss:  0.028411623 \t Eval Accuracy:  99.15%\n##########################################################\nStep:  12500 \t Train Loss:  0.030866437 \t Train Accuracy:  99.18%\nStep:  12500 \t Eval Loss:  0.03216686 \t Eval Accuracy:  99.13%\n##########################################################\nStep:  12600 \t Train Loss:  0.024542946 \t Train Accuracy:  99.33%\nStep:  12600 \t Eval Loss:  0.0267578 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  12700 \t Train Loss:  0.025547937 \t Train Accuracy:  99.18%\nStep:  12700 \t Eval Loss:  0.025053266 \t Eval Accuracy:  99.32%\n##########################################################\nStep:  12800 \t Train Loss:  0.02411567 \t Train Accuracy:  99.29%\nStep:  12800 \t Eval Loss:  0.027943004 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  12900 \t Train Loss:  0.02395698 \t Train Accuracy:  99.17%\nStep:  12900 \t Eval Loss:  0.029434513 \t Eval Accuracy:  99.11%\n##########################################################\nStep:  13000 \t Train Loss:  0.026369976 \t Train Accuracy:  99.24%\nStep:  13000 \t Eval Loss:  0.026276197 \t Eval Accuracy:  99.16%\n##########################################################\nStep:  13100 \t Train Loss:  0.027286377 \t Train Accuracy:  99.19%\nStep:  13100 \t Eval Loss:  0.02373477 \t Eval Accuracy:  99.30%\n##########################################################\nStep:  13200 \t Train Loss:  0.028012663 \t Train Accuracy:  99.16%\nStep:  13200 \t Eval Loss:  0.031476814 \t Eval Accuracy:  99.10%\n##########################################################\nStep:  13300 \t Train Loss:  0.026538206 \t Train Accuracy:  99.19%\nStep:  13300 \t Eval Loss:  0.027938642 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  13400 \t Train Loss:  0.040768895 \t Train Accuracy:  98.73%\nStep:  13400 \t Eval Loss:  0.039800793 \t Eval Accuracy:  98.72%\n##########################################################\nStep:  13500 \t Train Loss:  0.02672138 \t Train Accuracy:  99.23%\nStep:  13500 \t Eval Loss:  0.030695647 \t Eval Accuracy:  99.10%\n##########################################################\nStep:  13600 \t Train Loss:  0.030695688 \t Train Accuracy:  99.22%\nStep:  13600 \t Eval Loss:  0.02666948 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  13700 \t Train Loss:  0.028325222 \t Train Accuracy:  99.13%\nStep:  13700 \t Eval Loss:  0.024889441 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  13800 \t Train Loss:  0.028318768 \t Train Accuracy:  99.26%\nStep:  13800 \t Eval Loss:  0.031943735 \t Eval Accuracy:  99.06%\n##########################################################\nStep:  13900 \t Train Loss:  0.027852062 \t Train Accuracy:  99.17%\nStep:  13900 \t Eval Loss:  0.029298412 \t Eval Accuracy:  99.12%\n##########################################################\nStep:  14000 \t Train Loss:  0.026163334 \t Train Accuracy:  99.24%\nStep:  14000 \t Eval Loss:  0.025139064 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  14100 \t Train Loss:  0.022734039 \t Train Accuracy:  99.28%\nStep:  14100 \t Eval Loss:  0.023068659 \t Eval Accuracy:  99.34%\n##########################################################\nStep:  14200 \t Train Loss:  0.020378929 \t Train Accuracy:  99.44%\nStep:  14200 \t Eval Loss:  0.024522409 \t Eval Accuracy:  99.32%\n##########################################################\nStep:  14300 \t Train Loss:  0.02408027 \t Train Accuracy:  99.28%\nStep:  14300 \t Eval Loss:  0.0257279 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  14400 \t Train Loss:  0.023659531 \t Train Accuracy:  99.28%\nStep:  14400 \t Eval Loss:  0.025841182 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  14500 \t Train Loss:  0.023218095 \t Train Accuracy:  99.23%\nStep:  14500 \t Eval Loss:  0.027671032 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  14600 \t Train Loss:  0.025062077 \t Train Accuracy:  99.26%\nStep:  14600 \t Eval Loss:  0.026310518 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  14700 \t Train Loss:  0.02616848 \t Train Accuracy:  99.29%\nStep:  14700 \t Eval Loss:  0.031346977 \t Eval Accuracy:  99.11%\n##########################################################\nStep:  14800 \t Train Loss:  0.028414026 \t Train Accuracy:  99.18%\nStep:  14800 \t Eval Loss:  0.025528599 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  14900 \t Train Loss:  0.024424352 \t Train Accuracy:  99.26%\nStep:  14900 \t Eval Loss:  0.025518673 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  15000 \t Train Loss:  0.027000086 \t Train Accuracy:  99.18%\nStep:  15000 \t Eval Loss:  0.024424903 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  15100 \t Train Loss:  0.02476579 \t Train Accuracy:  99.29%\nStep:  15100 \t Eval Loss:  0.028485391 \t Eval Accuracy:  99.08%\n##########################################################\nStep:  15200 \t Train Loss:  0.025509138 \t Train Accuracy:  99.26%\nStep:  15200 \t Eval Loss:  0.031084165 \t Eval Accuracy:  99.06%\n##########################################################\nStep:  15300 \t Train Loss:  0.02433718 \t Train Accuracy:  99.27%\nStep:  15300 \t Eval Loss:  0.026894439 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  15400 \t Train Loss:  0.023619074 \t Train Accuracy:  99.33%\nStep:  15400 \t Eval Loss:  0.023228595 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  15500 \t Train Loss:  0.024039786 \t Train Accuracy:  99.24%\nStep:  15500 \t Eval Loss:  0.023828128 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  15600 \t Train Loss:  0.022282714 \t Train Accuracy:  99.33%\nStep:  15600 \t Eval Loss:  0.025256522 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  15700 \t Train Loss:  0.02577924 \t Train Accuracy:  99.27%\nStep:  15700 \t Eval Loss:  0.03185953 \t Eval Accuracy:  99.08%\n##########################################################\nStep:  15800 \t Train Loss:  0.02310444 \t Train Accuracy:  99.35%\nStep:  15800 \t Eval Loss:  0.028838966 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  15900 \t Train Loss:  0.033686973 \t Train Accuracy:  98.86%\nStep:  15900 \t Eval Loss:  0.0368793 \t Eval Accuracy:  98.83%\n##########################################################\nStep:  16000 \t Train Loss:  0.028862173 \t Train Accuracy:  99.10%\nStep:  16000 \t Eval Loss:  0.030969452 \t Eval Accuracy:  99.13%\n##########################################################\nStep:  16100 \t Train Loss:  0.021214858 \t Train Accuracy:  99.38%\nStep:  16100 \t Eval Loss:  0.028387897 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  16200 \t Train Loss:  0.026571507 \t Train Accuracy:  99.08%\nStep:  16200 \t Eval Loss:  0.02827045 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  16300 \t Train Loss:  0.028265737 \t Train Accuracy:  99.19%\nStep:  16300 \t Eval Loss:  0.027137622 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  16400 \t Train Loss:  0.021598196 \t Train Accuracy:  99.34%\nStep:  16400 \t Eval Loss:  0.023896713 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  16500 \t Train Loss:  0.020083468 \t Train Accuracy:  99.37%\nStep:  16500 \t Eval Loss:  0.024203083 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  16600 \t Train Loss:  0.025258733 \t Train Accuracy:  99.19%\nStep:  16600 \t Eval Loss:  0.023726432 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  16700 \t Train Loss:  0.020681627 \t Train Accuracy:  99.40%\nStep:  16700 \t Eval Loss:  0.024059415 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  16800 \t Train Loss:  0.024832092 \t Train Accuracy:  99.26%\nStep:  16800 \t Eval Loss:  0.022140356 \t Eval Accuracy:  99.33%\n##########################################################\nStep:  16900 \t Train Loss:  0.024381323 \t Train Accuracy:  99.26%\nStep:  16900 \t Eval Loss:  0.024018735 \t Eval Accuracy:  99.37%\n##########################################################\nStep:  17000 \t Train Loss:  0.02126164 \t Train Accuracy:  99.34%\nStep:  17000 \t Eval Loss:  0.024868589 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  17100 \t Train Loss:  0.03851465 \t Train Accuracy:  98.91%\nStep:  17100 \t Eval Loss:  0.037047066 \t Eval Accuracy:  98.99%\n##########################################################\nStep:  17200 \t Train Loss:  0.030700315 \t Train Accuracy:  99.06%\nStep:  17200 \t Eval Loss:  0.035030045 \t Eval Accuracy:  98.91%\n##########################################################\nStep:  17300 \t Train Loss:  0.026497122 \t Train Accuracy:  99.18%\nStep:  17300 \t Eval Loss:  0.03020255 \t Eval Accuracy:  99.08%\n##########################################################\nStep:  17400 \t Train Loss:  0.026285999 \t Train Accuracy:  99.19%\nStep:  17400 \t Eval Loss:  0.022423085 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  17500 \t Train Loss:  0.021317799 \t Train Accuracy:  99.38%\nStep:  17500 \t Eval Loss:  0.029347988 \t Eval Accuracy:  99.15%\n##########################################################\nStep:  17600 \t Train Loss:  0.028207652 \t Train Accuracy:  99.12%\nStep:  17600 \t Eval Loss:  0.030378934 \t Eval Accuracy:  99.04%\n##########################################################\nStep:  17700 \t Train Loss:  0.028776407 \t Train Accuracy:  99.12%\nStep:  17700 \t Eval Loss:  0.029233083 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  17800 \t Train Loss:  0.021029733 \t Train Accuracy:  99.35%\nStep:  17800 \t Eval Loss:  0.027214622 \t Eval Accuracy:  99.35%\n##########################################################\nStep:  17900 \t Train Loss:  0.023048501 \t Train Accuracy:  99.33%\nStep:  17900 \t Eval Loss:  0.025421672 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  18000 \t Train Loss:  0.023434428 \t Train Accuracy:  99.23%\nStep:  18000 \t Eval Loss:  0.021399146 \t Eval Accuracy:  99.38%\n##########################################################\nStep:  18100 \t Train Loss:  0.024329912 \t Train Accuracy:  99.22%\nStep:  18100 \t Eval Loss:  0.020600552 \t Eval Accuracy:  99.33%\n##########################################################\nStep:  18200 \t Train Loss:  0.020424206 \t Train Accuracy:  99.39%\nStep:  18200 \t Eval Loss:  0.024378372 \t Eval Accuracy:  99.30%\n##########################################################\nStep:  18300 \t Train Loss:  0.024326107 \t Train Accuracy:  99.34%\nStep:  18300 \t Eval Loss:  0.025694288 \t Eval Accuracy:  99.34%\n##########################################################\nStep:  18400 \t Train Loss:  0.023478683 \t Train Accuracy:  99.33%\nStep:  18400 \t Eval Loss:  0.024465213 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  18500 \t Train Loss:  0.022770653 \t Train Accuracy:  99.33%\nStep:  18500 \t Eval Loss:  0.031716455 \t Eval Accuracy:  99.13%\n##########################################################\nStep:  18600 \t Train Loss:  0.022480382 \t Train Accuracy:  99.33%\nStep:  18600 \t Eval Loss:  0.025048584 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  18700 \t Train Loss:  0.027105967 \t Train Accuracy:  99.18%\nStep:  18700 \t Eval Loss:  0.023762077 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  18800 \t Train Loss:  0.026202599 \t Train Accuracy:  99.19%\nStep:  18800 \t Eval Loss:  0.031017119 \t Eval Accuracy:  99.11%\n##########################################################\nStep:  18900 \t Train Loss:  0.03137044 \t Train Accuracy:  99.17%\nStep:  18900 \t Eval Loss:  0.031746276 \t Eval Accuracy:  99.04%\n##########################################################\nStep:  19000 \t Train Loss:  0.024778353 \t Train Accuracy:  99.26%\nStep:  19000 \t Eval Loss:  0.024072848 \t Eval Accuracy:  99.40%\n##########################################################\nStep:  19100 \t Train Loss:  0.02307548 \t Train Accuracy:  99.32%\nStep:  19100 \t Eval Loss:  0.021689782 \t Eval Accuracy:  99.34%\n##########################################################\nStep:  19200 \t Train Loss:  0.02256868 \t Train Accuracy:  99.30%\nStep:  19200 \t Eval Loss:  0.023916412 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  19300 \t Train Loss:  0.02616717 \t Train Accuracy:  99.26%\nStep:  19300 \t Eval Loss:  0.032062568 \t Eval Accuracy:  99.15%\n##########################################################\nStep:  19400 \t Train Loss:  0.021486683 \t Train Accuracy:  99.29%\nStep:  19400 \t Eval Loss:  0.023998914 \t Eval Accuracy:  99.33%\n##########################################################\nStep:  19500 \t Train Loss:  0.021373905 \t Train Accuracy:  99.29%\nStep:  19500 \t Eval Loss:  0.023418427 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  19600 \t Train Loss:  0.023658536 \t Train Accuracy:  99.19%\nStep:  19600 \t Eval Loss:  0.029010274 \t Eval Accuracy:  99.12%\n##########################################################\nStep:  19700 \t Train Loss:  0.02348908 \t Train Accuracy:  99.29%\nStep:  19700 \t Eval Loss:  0.02165779 \t Eval Accuracy:  99.37%\n##########################################################\nStep:  19800 \t Train Loss:  0.02075056 \t Train Accuracy:  99.35%\nStep:  19800 \t Eval Loss:  0.025781509 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  19900 \t Train Loss:  0.0257169 \t Train Accuracy:  99.21%\nStep:  19900 \t Eval Loss:  0.023661135 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  20000 \t Train Loss:  0.019254213 \t Train Accuracy:  99.43%\nStep:  20000 \t Eval Loss:  0.025950132 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  20100 \t Train Loss:  0.028058307 \t Train Accuracy:  99.07%\nStep:  20100 \t Eval Loss:  0.034825463 \t Eval Accuracy:  98.95%\n##########################################################\nStep:  20200 \t Train Loss:  0.02337195 \t Train Accuracy:  99.24%\nStep:  20200 \t Eval Loss:  0.023500476 \t Eval Accuracy:  99.30%\n##########################################################\nStep:  20300 \t Train Loss:  0.026155818 \t Train Accuracy:  99.17%\nStep:  20300 \t Eval Loss:  0.02511301 \t Eval Accuracy:  99.30%\n##########################################################\nStep:  20400 \t Train Loss:  0.024366375 \t Train Accuracy:  99.28%\nStep:  20400 \t Eval Loss:  0.021688497 \t Eval Accuracy:  99.34%\n##########################################################\nStep:  20500 \t Train Loss:  0.028148206 \t Train Accuracy:  99.10%\nStep:  20500 \t Eval Loss:  0.025319174 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  20600 \t Train Loss:  0.022985984 \t Train Accuracy:  99.35%\nStep:  20600 \t Eval Loss:  0.022476029 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  20700 \t Train Loss:  0.022254655 \t Train Accuracy:  99.28%\nStep:  20700 \t Eval Loss:  0.028928265 \t Eval Accuracy:  99.13%\n##########################################################\nStep:  20800 \t Train Loss:  0.025371825 \t Train Accuracy:  99.18%\nStep:  20800 \t Eval Loss:  0.025439698 \t Eval Accuracy:  99.35%\n##########################################################\nStep:  20900 \t Train Loss:  0.02537775 \t Train Accuracy:  99.30%\nStep:  20900 \t Eval Loss:  0.023916174 \t Eval Accuracy:  99.30%\n##########################################################\nStep:  21000 \t Train Loss:  0.0265763 \t Train Accuracy:  99.22%\nStep:  21000 \t Eval Loss:  0.026914515 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  21100 \t Train Loss:  0.028596962 \t Train Accuracy:  99.11%\nStep:  21100 \t Eval Loss:  0.030186273 \t Eval Accuracy:  99.10%\n##########################################################\nStep:  21200 \t Train Loss:  0.02476148 \t Train Accuracy:  99.27%\nStep:  21200 \t Eval Loss:  0.03210929 \t Eval Accuracy:  99.11%\n##########################################################\nStep:  21300 \t Train Loss:  0.024216205 \t Train Accuracy:  99.23%\nStep:  21300 \t Eval Loss:  0.027653351 \t Eval Accuracy:  99.16%\n##########################################################\nStep:  21400 \t Train Loss:  0.024590608 \t Train Accuracy:  99.27%\nStep:  21400 \t Eval Loss:  0.030015007 \t Eval Accuracy:  99.13%\n##########################################################\nStep:  21500 \t Train Loss:  0.022444123 \t Train Accuracy:  99.30%\nStep:  21500 \t Eval Loss:  0.021353144 \t Eval Accuracy:  99.33%\n##########################################################\nStep:  21600 \t Train Loss:  0.02261862 \t Train Accuracy:  99.22%\nStep:  21600 \t Eval Loss:  0.02578076 \t Eval Accuracy:  99.30%\n##########################################################\nStep:  21700 \t Train Loss:  0.023881463 \t Train Accuracy:  99.28%\nStep:  21700 \t Eval Loss:  0.025466176 \t Eval Accuracy:  99.33%\n##########################################################\nStep:  21800 \t Train Loss:  0.021407213 \t Train Accuracy:  99.29%\nStep:  21800 \t Eval Loss:  0.025971789 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  21900 \t Train Loss:  0.023542244 \t Train Accuracy:  99.27%\nStep:  21900 \t Eval Loss:  0.024772424 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  22000 \t Train Loss:  0.019135468 \t Train Accuracy:  99.45%\nStep:  22000 \t Eval Loss:  0.024379516 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  22100 \t Train Loss:  0.023027707 \t Train Accuracy:  99.24%\nStep:  22100 \t Eval Loss:  0.0220287 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  22200 \t Train Loss:  0.021106273 \t Train Accuracy:  99.38%\nStep:  22200 \t Eval Loss:  0.023399584 \t Eval Accuracy:  99.32%\n##########################################################\nStep:  22300 \t Train Loss:  0.0236668 \t Train Accuracy:  99.18%\nStep:  22300 \t Eval Loss:  0.023181451 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  22400 \t Train Loss:  0.022781715 \t Train Accuracy:  99.29%\nStep:  22400 \t Eval Loss:  0.038178287 \t Eval Accuracy:  98.91%\n##########################################################\nStep:  22500 \t Train Loss:  0.03008893 \t Train Accuracy:  99.12%\nStep:  22500 \t Eval Loss:  0.028901089 \t Eval Accuracy:  99.13%\n##########################################################\nStep:  22600 \t Train Loss:  0.026449934 \t Train Accuracy:  99.19%\nStep:  22600 \t Eval Loss:  0.026400909 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  22700 \t Train Loss:  0.020510085 \t Train Accuracy:  99.34%\nStep:  22700 \t Eval Loss:  0.024694007 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  22800 \t Train Loss:  0.021007005 \t Train Accuracy:  99.34%\nStep:  22800 \t Eval Loss:  0.027608866 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  22900 \t Train Loss:  0.018683307 \t Train Accuracy:  99.43%\nStep:  22900 \t Eval Loss:  0.023136113 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  23000 \t Train Loss:  0.024392461 \t Train Accuracy:  99.22%\nStep:  23000 \t Eval Loss:  0.02169528 \t Eval Accuracy:  99.38%\n##########################################################\nStep:  23100 \t Train Loss:  0.020585794 \t Train Accuracy:  99.40%\nStep:  23100 \t Eval Loss:  0.022696063 \t Eval Accuracy:  99.32%\n##########################################################\nStep:  23200 \t Train Loss:  0.026537914 \t Train Accuracy:  99.21%\nStep:  23200 \t Eval Loss:  0.028944679 \t Eval Accuracy:  99.16%\n##########################################################\nStep:  23300 \t Train Loss:  0.023052525 \t Train Accuracy:  99.24%\nStep:  23300 \t Eval Loss:  0.026351815 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  23400 \t Train Loss:  0.025075369 \t Train Accuracy:  99.22%\nStep:  23400 \t Eval Loss:  0.027250601 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  23500 \t Train Loss:  0.023302684 \t Train Accuracy:  99.23%\nStep:  23500 \t Eval Loss:  0.025108192 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  23600 \t Train Loss:  0.023298673 \t Train Accuracy:  99.32%\nStep:  23600 \t Eval Loss:  0.024638392 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  23700 \t Train Loss:  0.023324646 \t Train Accuracy:  99.27%\nStep:  23700 \t Eval Loss:  0.026245669 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  23800 \t Train Loss:  0.021724984 \t Train Accuracy:  99.24%\nStep:  23800 \t Eval Loss:  0.032823462 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  23900 \t Train Loss:  0.024234332 \t Train Accuracy:  99.26%\nStep:  23900 \t Eval Loss:  0.024550185 \t Eval Accuracy:  99.15%\n##########################################################\nStep:  24000 \t Train Loss:  0.028167628 \t Train Accuracy:  99.21%\nStep:  24000 \t Eval Loss:  0.02713082 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  24100 \t Train Loss:  0.025215859 \t Train Accuracy:  99.29%\nStep:  24100 \t Eval Loss:  0.027946131 \t Eval Accuracy:  99.15%\n##########################################################\nStep:  24200 \t Train Loss:  0.021492813 \t Train Accuracy:  99.40%\nStep:  24200 \t Eval Loss:  0.027862104 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  24300 \t Train Loss:  0.020027906 \t Train Accuracy:  99.45%\nStep:  24300 \t Eval Loss:  0.028581131 \t Eval Accuracy:  99.17%\n##########################################################\nStep:  24400 \t Train Loss:  0.023194406 \t Train Accuracy:  99.30%\nStep:  24400 \t Eval Loss:  0.026584174 \t Eval Accuracy:  99.17%\n##########################################################\nStep:  24500 \t Train Loss:  0.022188522 \t Train Accuracy:  99.30%\nStep:  24500 \t Eval Loss:  0.025992185 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  24600 \t Train Loss:  0.026775237 \t Train Accuracy:  99.24%\nStep:  24600 \t Eval Loss:  0.024705645 \t Eval Accuracy:  99.17%\n##########################################################\nStep:  24700 \t Train Loss:  0.021167876 \t Train Accuracy:  99.39%\nStep:  24700 \t Eval Loss:  0.026343053 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  24800 \t Train Loss:  0.022884127 \t Train Accuracy:  99.28%\nStep:  24800 \t Eval Loss:  0.02598717 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  24900 \t Train Loss:  0.025225297 \t Train Accuracy:  99.23%\nStep:  24900 \t Eval Loss:  0.026918748 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  25000 \t Train Loss:  0.023996806 \t Train Accuracy:  99.23%\nStep:  25000 \t Eval Loss:  0.021397809 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  25100 \t Train Loss:  0.024731334 \t Train Accuracy:  99.23%\nStep:  25100 \t Eval Loss:  0.025563426 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  25200 \t Train Loss:  0.022117805 \t Train Accuracy:  99.29%\nStep:  25200 \t Eval Loss:  0.023501527 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  25300 \t Train Loss:  0.024931964 \t Train Accuracy:  99.26%\nStep:  25300 \t Eval Loss:  0.0281808 \t Eval Accuracy:  99.15%\n##########################################################\nStep:  25400 \t Train Loss:  0.025338313 \t Train Accuracy:  99.23%\nStep:  25400 \t Eval Loss:  0.025679052 \t Eval Accuracy:  99.32%\n##########################################################\nStep:  25500 \t Train Loss:  0.025858842 \t Train Accuracy:  99.27%\nStep:  25500 \t Eval Loss:  0.028390933 \t Eval Accuracy:  99.17%\n##########################################################\nStep:  25600 \t Train Loss:  0.019125689 \t Train Accuracy:  99.44%\nStep:  25600 \t Eval Loss:  0.024734508 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  25700 \t Train Loss:  0.026396796 \t Train Accuracy:  99.21%\nStep:  25700 \t Eval Loss:  0.022742074 \t Eval Accuracy:  99.37%\n##########################################################\nStep:  25800 \t Train Loss:  0.021561222 \t Train Accuracy:  99.35%\nStep:  25800 \t Eval Loss:  0.030224005 \t Eval Accuracy:  99.11%\n##########################################################\nStep:  25900 \t Train Loss:  0.020667527 \t Train Accuracy:  99.39%\nStep:  25900 \t Eval Loss:  0.024010472 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  26000 \t Train Loss:  0.021353252 \t Train Accuracy:  99.32%\nStep:  26000 \t Eval Loss:  0.025085233 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  26100 \t Train Loss:  0.022758324 \t Train Accuracy:  99.40%\nStep:  26100 \t Eval Loss:  0.024018727 \t Eval Accuracy:  99.32%\n##########################################################\nStep:  26200 \t Train Loss:  0.021001827 \t Train Accuracy:  99.29%\nStep:  26200 \t Eval Loss:  0.02441654 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  26300 \t Train Loss:  0.021900676 \t Train Accuracy:  99.24%\nStep:  26300 \t Eval Loss:  0.0251427 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  26400 \t Train Loss:  0.01822546 \t Train Accuracy:  99.44%\nStep:  26400 \t Eval Loss:  0.02567999 \t Eval Accuracy:  99.15%\n##########################################################\nStep:  26500 \t Train Loss:  0.023448147 \t Train Accuracy:  99.30%\nStep:  26500 \t Eval Loss:  0.029281706 \t Eval Accuracy:  99.15%\n##########################################################\nStep:  26600 \t Train Loss:  0.022357203 \t Train Accuracy:  99.38%\nStep:  26600 \t Eval Loss:  0.02417651 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  26700 \t Train Loss:  0.023119032 \t Train Accuracy:  99.28%\nStep:  26700 \t Eval Loss:  0.026423175 \t Eval Accuracy:  99.12%\n##########################################################\nStep:  26800 \t Train Loss:  0.020977713 \t Train Accuracy:  99.41%\nStep:  26800 \t Eval Loss:  0.025155762 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  26900 \t Train Loss:  0.02397484 \t Train Accuracy:  99.27%\nStep:  26900 \t Eval Loss:  0.026100136 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  27000 \t Train Loss:  0.020895813 \t Train Accuracy:  99.29%\nStep:  27000 \t Eval Loss:  0.026057616 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  27100 \t Train Loss:  0.027615357 \t Train Accuracy:  99.11%\nStep:  27100 \t Eval Loss:  0.024450706 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  27200 \t Train Loss:  0.02378431 \t Train Accuracy:  99.26%\nStep:  27200 \t Eval Loss:  0.028633196 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  27300 \t Train Loss:  0.024765484 \t Train Accuracy:  99.27%\nStep:  27300 \t Eval Loss:  0.02304253 \t Eval Accuracy:  99.30%\n##########################################################\nStep:  27400 \t Train Loss:  0.022781383 \t Train Accuracy:  99.27%\nStep:  27400 \t Eval Loss:  0.02359194 \t Eval Accuracy:  99.30%\n##########################################################\nStep:  27500 \t Train Loss:  0.022343427 \t Train Accuracy:  99.29%\nStep:  27500 \t Eval Loss:  0.02644123 \t Eval Accuracy:  99.17%\n##########################################################\nStep:  27600 \t Train Loss:  0.025205405 \t Train Accuracy:  99.29%\nStep:  27600 \t Eval Loss:  0.025291663 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  27700 \t Train Loss:  0.025594985 \t Train Accuracy:  99.19%\nStep:  27700 \t Eval Loss:  0.025554452 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  27800 \t Train Loss:  0.0213743 \t Train Accuracy:  99.33%\nStep:  27800 \t Eval Loss:  0.027310025 \t Eval Accuracy:  99.12%\n##########################################################\nStep:  27900 \t Train Loss:  0.02416458 \t Train Accuracy:  99.32%\nStep:  27900 \t Eval Loss:  0.025152298 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  28000 \t Train Loss:  0.024914978 \t Train Accuracy:  99.18%\nStep:  28000 \t Eval Loss:  0.029049236 \t Eval Accuracy:  99.13%\n##########################################################\nStep:  28100 \t Train Loss:  0.020262687 \t Train Accuracy:  99.35%\nStep:  28100 \t Eval Loss:  0.02549792 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  28200 \t Train Loss:  0.020835064 \t Train Accuracy:  99.40%\nStep:  28200 \t Eval Loss:  0.02746461 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  28300 \t Train Loss:  0.027607538 \t Train Accuracy:  99.17%\nStep:  28300 \t Eval Loss:  0.021208797 \t Eval Accuracy:  99.34%\n##########################################################\nStep:  28400 \t Train Loss:  0.020053174 \t Train Accuracy:  99.33%\nStep:  28400 \t Eval Loss:  0.025680313 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  28500 \t Train Loss:  0.018244648 \t Train Accuracy:  99.37%\nStep:  28500 \t Eval Loss:  0.020946 \t Eval Accuracy:  99.38%\n##########################################################\nStep:  28600 \t Train Loss:  0.02094879 \t Train Accuracy:  99.40%\nStep:  28600 \t Eval Loss:  0.02186474 \t Eval Accuracy:  99.39%\n##########################################################\nStep:  28700 \t Train Loss:  0.021001913 \t Train Accuracy:  99.38%\nStep:  28700 \t Eval Loss:  0.024131024 \t Eval Accuracy:  99.30%\n##########################################################\nStep:  28800 \t Train Loss:  0.02109927 \t Train Accuracy:  99.34%\nStep:  28800 \t Eval Loss:  0.02439823 \t Eval Accuracy:  99.34%\n##########################################################\nStep:  28900 \t Train Loss:  0.024284106 \t Train Accuracy:  99.27%\nStep:  28900 \t Eval Loss:  0.020806205 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  29000 \t Train Loss:  0.021810867 \t Train Accuracy:  99.35%\nStep:  29000 \t Eval Loss:  0.023164997 \t Eval Accuracy:  99.33%\n##########################################################\nStep:  29100 \t Train Loss:  0.019067766 \t Train Accuracy:  99.41%\nStep:  29100 \t Eval Loss:  0.02158522 \t Eval Accuracy:  99.37%\n##########################################################\nStep:  29200 \t Train Loss:  0.023810998 \t Train Accuracy:  99.27%\nStep:  29200 \t Eval Loss:  0.02362727 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  29300 \t Train Loss:  0.027166333 \t Train Accuracy:  99.19%\nStep:  29300 \t Eval Loss:  0.027687684 \t Eval Accuracy:  99.08%\n##########################################################\nStep:  29400 \t Train Loss:  0.02425595 \t Train Accuracy:  99.22%\nStep:  29400 \t Eval Loss:  0.023828581 \t Eval Accuracy:  99.32%\n##########################################################\nStep:  29500 \t Train Loss:  0.025631215 \t Train Accuracy:  99.24%\nStep:  29500 \t Eval Loss:  0.024319533 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  29600 \t Train Loss:  0.021435753 \t Train Accuracy:  99.33%\nStep:  29600 \t Eval Loss:  0.022099689 \t Eval Accuracy:  99.34%\n##########################################################\nStep:  29700 \t Train Loss:  0.02382687 \t Train Accuracy:  99.32%\nStep:  29700 \t Eval Loss:  0.023689808 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  29800 \t Train Loss:  0.022300255 \t Train Accuracy:  99.37%\nStep:  29800 \t Eval Loss:  0.02977341 \t Eval Accuracy:  99.11%\n##########################################################\nStep:  29900 \t Train Loss:  0.018340794 \t Train Accuracy:  99.34%\nStep:  29900 \t Eval Loss:  0.026398044 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  30000 \t Train Loss:  0.019137345 \t Train Accuracy:  99.33%\nStep:  30000 \t Eval Loss:  0.024897523 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  30100 \t Train Loss:  0.026576525 \t Train Accuracy:  99.27%\nStep:  30100 \t Eval Loss:  0.024761008 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  30200 \t Train Loss:  0.0186763 \t Train Accuracy:  99.34%\nStep:  30200 \t Eval Loss:  0.027450833 \t Eval Accuracy:  99.16%\n##########################################################\nStep:  30300 \t Train Loss:  0.02018397 \t Train Accuracy:  99.33%\nStep:  30300 \t Eval Loss:  0.025488213 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  30400 \t Train Loss:  0.020566514 \t Train Accuracy:  99.33%\nStep:  30400 \t Eval Loss:  0.022779189 \t Eval Accuracy:  99.35%\n##########################################################\nStep:  30500 \t Train Loss:  0.023993347 \t Train Accuracy:  99.35%\nStep:  30500 \t Eval Loss:  0.024553558 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  30600 \t Train Loss:  0.02080756 \t Train Accuracy:  99.35%\nStep:  30600 \t Eval Loss:  0.018893445 \t Eval Accuracy:  99.50%\n##########################################################\nStep:  30700 \t Train Loss:  0.021266934 \t Train Accuracy:  99.32%\nStep:  30700 \t Eval Loss:  0.023489717 \t Eval Accuracy:  99.34%\n##########################################################\nStep:  30800 \t Train Loss:  0.019537352 \t Train Accuracy:  99.40%\nStep:  30800 \t Eval Loss:  0.026846863 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  30900 \t Train Loss:  0.023570128 \t Train Accuracy:  99.29%\nStep:  30900 \t Eval Loss:  0.025491554 \t Eval Accuracy:  99.30%\n##########################################################\nStep:  31000 \t Train Loss:  0.023299329 \t Train Accuracy:  99.32%\nStep:  31000 \t Eval Loss:  0.023423215 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  31100 \t Train Loss:  0.025227336 \t Train Accuracy:  99.19%\nStep:  31100 \t Eval Loss:  0.028769603 \t Eval Accuracy:  99.16%\n##########################################################\nStep:  31200 \t Train Loss:  0.024421128 \t Train Accuracy:  99.19%\nStep:  31200 \t Eval Loss:  0.02175926 \t Eval Accuracy:  99.37%\n##########################################################\nStep:  31300 \t Train Loss:  0.022485688 \t Train Accuracy:  99.34%\nStep:  31300 \t Eval Loss:  0.022652205 \t Eval Accuracy:  99.32%\n##########################################################\nStep:  31400 \t Train Loss:  0.02261934 \t Train Accuracy:  99.28%\nStep:  31400 \t Eval Loss:  0.023411632 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  31500 \t Train Loss:  0.024532147 \t Train Accuracy:  99.28%\nStep:  31500 \t Eval Loss:  0.025389763 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  31600 \t Train Loss:  0.024447141 \t Train Accuracy:  99.16%\nStep:  31600 \t Eval Loss:  0.023452234 \t Eval Accuracy:  99.33%\n##########################################################\nStep:  31700 \t Train Loss:  0.026257183 \t Train Accuracy:  99.18%\nStep:  31700 \t Eval Loss:  0.025372896 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  31800 \t Train Loss:  0.022091623 \t Train Accuracy:  99.33%\nStep:  31800 \t Eval Loss:  0.020978557 \t Eval Accuracy:  99.43%\n##########################################################\nStep:  31900 \t Train Loss:  0.01874499 \t Train Accuracy:  99.44%\nStep:  31900 \t Eval Loss:  0.024465974 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  32000 \t Train Loss:  0.025888816 \t Train Accuracy:  99.17%\nStep:  32000 \t Eval Loss:  0.026220545 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  32100 \t Train Loss:  0.0210078 \t Train Accuracy:  99.37%\nStep:  32100 \t Eval Loss:  0.023076622 \t Eval Accuracy:  99.34%\n##########################################################\nStep:  32200 \t Train Loss:  0.0201162 \t Train Accuracy:  99.46%\nStep:  32200 \t Eval Loss:  0.027748082 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  32300 \t Train Loss:  0.019662596 \t Train Accuracy:  99.43%\nStep:  32300 \t Eval Loss:  0.020863088 \t Eval Accuracy:  99.39%\n##########################################################\nStep:  32400 \t Train Loss:  0.02220596 \t Train Accuracy:  99.27%\nStep:  32400 \t Eval Loss:  0.024938505 \t Eval Accuracy:  99.33%\n##########################################################\nStep:  32500 \t Train Loss:  0.026637793 \t Train Accuracy:  99.22%\nStep:  32500 \t Eval Loss:  0.02525577 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  32600 \t Train Loss:  0.020272885 \t Train Accuracy:  99.37%\nStep:  32600 \t Eval Loss:  0.027033208 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  32700 \t Train Loss:  0.02158375 \t Train Accuracy:  99.33%\nStep:  32700 \t Eval Loss:  0.021785643 \t Eval Accuracy:  99.30%\n##########################################################\nStep:  32800 \t Train Loss:  0.021499267 \t Train Accuracy:  99.35%\nStep:  32800 \t Eval Loss:  0.025506608 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  32900 \t Train Loss:  0.025626093 \t Train Accuracy:  99.19%\nStep:  32900 \t Eval Loss:  0.023692863 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  33000 \t Train Loss:  0.02119814 \t Train Accuracy:  99.32%\nStep:  33000 \t Eval Loss:  0.025842614 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  33100 \t Train Loss:  0.022617167 \t Train Accuracy:  99.32%\nStep:  33100 \t Eval Loss:  0.024717167 \t Eval Accuracy:  99.34%\n##########################################################\nStep:  33200 \t Train Loss:  0.022969287 \t Train Accuracy:  99.27%\nStep:  33200 \t Eval Loss:  0.022356108 \t Eval Accuracy:  99.40%\n##########################################################\nStep:  33300 \t Train Loss:  0.021319441 \t Train Accuracy:  99.24%\nStep:  33300 \t Eval Loss:  0.025023261 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  33400 \t Train Loss:  0.022740314 \t Train Accuracy:  99.32%\nStep:  33400 \t Eval Loss:  0.020441405 \t Eval Accuracy:  99.35%\n##########################################################\nStep:  33500 \t Train Loss:  0.020486854 \t Train Accuracy:  99.39%\nStep:  33500 \t Eval Loss:  0.025671396 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  33600 \t Train Loss:  0.021457482 \t Train Accuracy:  99.30%\nStep:  33600 \t Eval Loss:  0.02478468 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  33700 \t Train Loss:  0.020372761 \t Train Accuracy:  99.44%\nStep:  33700 \t Eval Loss:  0.020636568 \t Eval Accuracy:  99.39%\n##########################################################\nStep:  33800 \t Train Loss:  0.021549452 \t Train Accuracy:  99.35%\nStep:  33800 \t Eval Loss:  0.02149464 \t Eval Accuracy:  99.38%\n##########################################################\nStep:  33900 \t Train Loss:  0.023550756 \t Train Accuracy:  99.29%\nStep:  33900 \t Eval Loss:  0.024127215 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  34000 \t Train Loss:  0.024854383 \t Train Accuracy:  99.34%\nStep:  34000 \t Eval Loss:  0.022066467 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  34100 \t Train Loss:  0.020647772 \t Train Accuracy:  99.35%\nStep:  34100 \t Eval Loss:  0.024567537 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  34200 \t Train Loss:  0.028154599 \t Train Accuracy:  99.19%\nStep:  34200 \t Eval Loss:  0.032861114 \t Eval Accuracy:  99.12%\n##########################################################\nStep:  34300 \t Train Loss:  0.0204596 \t Train Accuracy:  99.33%\nStep:  34300 \t Eval Loss:  0.02634921 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  34400 \t Train Loss:  0.019655734 \t Train Accuracy:  99.41%\nStep:  34400 \t Eval Loss:  0.021688834 \t Eval Accuracy:  99.40%\n##########################################################\nStep:  34500 \t Train Loss:  0.020193994 \t Train Accuracy:  99.40%\nStep:  34500 \t Eval Loss:  0.026876617 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  34600 \t Train Loss:  0.021293875 \t Train Accuracy:  99.34%\nStep:  34600 \t Eval Loss:  0.01952165 \t Eval Accuracy:  99.38%\n##########################################################\nStep:  34700 \t Train Loss:  0.022563407 \t Train Accuracy:  99.23%\nStep:  34700 \t Eval Loss:  0.025221556 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  34800 \t Train Loss:  0.019992791 \t Train Accuracy:  99.48%\nStep:  34800 \t Eval Loss:  0.027496204 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  34900 \t Train Loss:  0.02381702 \t Train Accuracy:  99.29%\nStep:  34900 \t Eval Loss:  0.02286762 \t Eval Accuracy:  99.30%\n##########################################################\nStep:  35000 \t Train Loss:  0.01994405 \t Train Accuracy:  99.38%\nStep:  35000 \t Eval Loss:  0.022798935 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  35100 \t Train Loss:  0.021290794 \t Train Accuracy:  99.28%\nStep:  35100 \t Eval Loss:  0.020970758 \t Eval Accuracy:  99.32%\n##########################################################\nStep:  35200 \t Train Loss:  0.020979103 \t Train Accuracy:  99.37%\nStep:  35200 \t Eval Loss:  0.030093722 \t Eval Accuracy:  99.10%\n##########################################################\nStep:  35300 \t Train Loss:  0.021119064 \t Train Accuracy:  99.34%\nStep:  35300 \t Eval Loss:  0.028538289 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  35400 \t Train Loss:  0.024597853 \t Train Accuracy:  99.18%\nStep:  35400 \t Eval Loss:  0.024509821 \t Eval Accuracy:  99.33%\n##########################################################\nStep:  35500 \t Train Loss:  0.023741204 \t Train Accuracy:  99.27%\nStep:  35500 \t Eval Loss:  0.024819821 \t Eval Accuracy:  99.32%\n##########################################################\nStep:  35600 \t Train Loss:  0.024408937 \t Train Accuracy:  99.27%\nStep:  35600 \t Eval Loss:  0.023810351 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  35700 \t Train Loss:  0.021392234 \t Train Accuracy:  99.23%\nStep:  35700 \t Eval Loss:  0.025949128 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  35800 \t Train Loss:  0.022004265 \t Train Accuracy:  99.30%\nStep:  35800 \t Eval Loss:  0.02518166 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  35900 \t Train Loss:  0.021176744 \t Train Accuracy:  99.39%\nStep:  35900 \t Eval Loss:  0.022875454 \t Eval Accuracy:  99.32%\n##########################################################\nStep:  36000 \t Train Loss:  0.021108562 \t Train Accuracy:  99.29%\nStep:  36000 \t Eval Loss:  0.023763621 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  36100 \t Train Loss:  0.021159176 \t Train Accuracy:  99.37%\nStep:  36100 \t Eval Loss:  0.024756188 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  36200 \t Train Loss:  0.021750856 \t Train Accuracy:  99.38%\nStep:  36200 \t Eval Loss:  0.02608113 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  36300 \t Train Loss:  0.021832576 \t Train Accuracy:  99.27%\nStep:  36300 \t Eval Loss:  0.022194333 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  36400 \t Train Loss:  0.022609815 \t Train Accuracy:  99.29%\nStep:  36400 \t Eval Loss:  0.020240288 \t Eval Accuracy:  99.38%\n##########################################################\nStep:  36500 \t Train Loss:  0.021492895 \t Train Accuracy:  99.38%\nStep:  36500 \t Eval Loss:  0.02523633 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  36600 \t Train Loss:  0.022629468 \t Train Accuracy:  99.27%\nStep:  36600 \t Eval Loss:  0.023013674 \t Eval Accuracy:  99.34%\n##########################################################\nStep:  36700 \t Train Loss:  0.022713367 \t Train Accuracy:  99.26%\nStep:  36700 \t Eval Loss:  0.02338864 \t Eval Accuracy:  99.34%\n##########################################################\nStep:  36800 \t Train Loss:  0.021961516 \t Train Accuracy:  99.29%\nStep:  36800 \t Eval Loss:  0.026706774 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  36900 \t Train Loss:  0.0240187 \t Train Accuracy:  99.28%\nStep:  36900 \t Eval Loss:  0.023174731 \t Eval Accuracy:  99.30%\n##########################################################\nStep:  37000 \t Train Loss:  0.02013245 \t Train Accuracy:  99.40%\nStep:  37000 \t Eval Loss:  0.025783936 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  37100 \t Train Loss:  0.02076493 \t Train Accuracy:  99.33%\nStep:  37100 \t Eval Loss:  0.027521085 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  37200 \t Train Loss:  0.021475513 \t Train Accuracy:  99.32%\nStep:  37200 \t Eval Loss:  0.024538089 \t Eval Accuracy:  99.30%\n##########################################################\nStep:  37300 \t Train Loss:  0.019746067 \t Train Accuracy:  99.38%\nStep:  37300 \t Eval Loss:  0.02463767 \t Eval Accuracy:  99.32%\n##########################################################\nStep:  37400 \t Train Loss:  0.019474618 \t Train Accuracy:  99.41%\nStep:  37400 \t Eval Loss:  0.024555627 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  37500 \t Train Loss:  0.02265106 \t Train Accuracy:  99.30%\nStep:  37500 \t Eval Loss:  0.022466775 \t Eval Accuracy:  99.34%\n##########################################################\nStep:  37600 \t Train Loss:  0.022608025 \t Train Accuracy:  99.23%\nStep:  37600 \t Eval Loss:  0.024318079 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  37700 \t Train Loss:  0.025342306 \t Train Accuracy:  99.21%\nStep:  37700 \t Eval Loss:  0.027919324 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  37800 \t Train Loss:  0.023342656 \t Train Accuracy:  99.33%\nStep:  37800 \t Eval Loss:  0.021338234 \t Eval Accuracy:  99.43%\n##########################################################\nStep:  37900 \t Train Loss:  0.021740593 \t Train Accuracy:  99.34%\nStep:  37900 \t Eval Loss:  0.026473127 \t Eval Accuracy:  99.16%\n##########################################################\nStep:  38000 \t Train Loss:  0.020150576 \t Train Accuracy:  99.37%\nStep:  38000 \t Eval Loss:  0.025045961 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  38100 \t Train Loss:  0.01865236 \t Train Accuracy:  99.35%\nStep:  38100 \t Eval Loss:  0.025669819 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  38200 \t Train Loss:  0.021480735 \t Train Accuracy:  99.33%\nStep:  38200 \t Eval Loss:  0.026206866 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  38300 \t Train Loss:  0.022467721 \t Train Accuracy:  99.34%\nStep:  38300 \t Eval Loss:  0.021635722 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  38400 \t Train Loss:  0.022673413 \t Train Accuracy:  99.32%\nStep:  38400 \t Eval Loss:  0.020246327 \t Eval Accuracy:  99.34%\n##########################################################\nStep:  38500 \t Train Loss:  0.020048968 \t Train Accuracy:  99.48%\nStep:  38500 \t Eval Loss:  0.022880688 \t Eval Accuracy:  99.38%\n##########################################################\nStep:  38600 \t Train Loss:  0.021240117 \t Train Accuracy:  99.28%\nStep:  38600 \t Eval Loss:  0.024354633 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  38700 \t Train Loss:  0.01946291 \t Train Accuracy:  99.37%\nStep:  38700 \t Eval Loss:  0.024259321 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  38800 \t Train Loss:  0.021023057 \t Train Accuracy:  99.39%\nStep:  38800 \t Eval Loss:  0.029965306 \t Eval Accuracy:  99.13%\n##########################################################\nStep:  38900 \t Train Loss:  0.021811288 \t Train Accuracy:  99.37%\nStep:  38900 \t Eval Loss:  0.021923423 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  39000 \t Train Loss:  0.02313636 \t Train Accuracy:  99.30%\nStep:  39000 \t Eval Loss:  0.029971087 \t Eval Accuracy:  99.17%\n##########################################################\nStep:  39100 \t Train Loss:  0.018648587 \t Train Accuracy:  99.46%\nStep:  39100 \t Eval Loss:  0.024391882 \t Eval Accuracy:  99.30%\n##########################################################\nStep:  39200 \t Train Loss:  0.022402132 \t Train Accuracy:  99.38%\nStep:  39200 \t Eval Loss:  0.026141508 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  39300 \t Train Loss:  0.02180651 \t Train Accuracy:  99.30%\nStep:  39300 \t Eval Loss:  0.024625171 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  39400 \t Train Loss:  0.028463736 \t Train Accuracy:  99.10%\nStep:  39400 \t Eval Loss:  0.0269727 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  39500 \t Train Loss:  0.021100637 \t Train Accuracy:  99.29%\nStep:  39500 \t Eval Loss:  0.021642435 \t Eval Accuracy:  99.39%\n##########################################################\nStep:  39600 \t Train Loss:  0.020960465 \t Train Accuracy:  99.39%\nStep:  39600 \t Eval Loss:  0.02632919 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  39700 \t Train Loss:  0.021531906 \t Train Accuracy:  99.29%\nStep:  39700 \t Eval Loss:  0.02394773 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  39800 \t Train Loss:  0.023261774 \t Train Accuracy:  99.24%\nStep:  39800 \t Eval Loss:  0.025165897 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  39900 \t Train Loss:  0.018975552 \t Train Accuracy:  99.40%\nStep:  39900 \t Eval Loss:  0.024533242 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  40000 \t Train Loss:  0.021375801 \t Train Accuracy:  99.30%\nStep:  40000 \t Eval Loss:  0.02467024 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  40100 \t Train Loss:  0.0186317 \t Train Accuracy:  99.45%\nStep:  40100 \t Eval Loss:  0.021883134 \t Eval Accuracy:  99.32%\n##########################################################\nStep:  40200 \t Train Loss:  0.018919013 \t Train Accuracy:  99.38%\nStep:  40200 \t Eval Loss:  0.023266789 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  40300 \t Train Loss:  0.02224116 \t Train Accuracy:  99.32%\nStep:  40300 \t Eval Loss:  0.022857755 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  40400 \t Train Loss:  0.023200257 \t Train Accuracy:  99.28%\nStep:  40400 \t Eval Loss:  0.022333007 \t Eval Accuracy:  99.35%\n##########################################################\nStep:  40500 \t Train Loss:  0.023793478 \t Train Accuracy:  99.27%\nStep:  40500 \t Eval Loss:  0.022583269 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  40600 \t Train Loss:  0.021422002 \t Train Accuracy:  99.32%\nStep:  40600 \t Eval Loss:  0.022674814 \t Eval Accuracy:  99.34%\n##########################################################\nStep:  40700 \t Train Loss:  0.017891396 \t Train Accuracy:  99.46%\nStep:  40700 \t Eval Loss:  0.02037424 \t Eval Accuracy:  99.39%\n##########################################################\nStep:  40800 \t Train Loss:  0.021005971 \t Train Accuracy:  99.34%\nStep:  40800 \t Eval Loss:  0.019216765 \t Eval Accuracy:  99.39%\n##########################################################\nStep:  40900 \t Train Loss:  0.020646423 \t Train Accuracy:  99.30%\nStep:  40900 \t Eval Loss:  0.027590383 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  41000 \t Train Loss:  0.020718569 \t Train Accuracy:  99.38%\nStep:  41000 \t Eval Loss:  0.024853224 \t Eval Accuracy:  99.35%\n##########################################################\nStep:  41100 \t Train Loss:  0.020570945 \t Train Accuracy:  99.35%\nStep:  41100 \t Eval Loss:  0.02645402 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  41200 \t Train Loss:  0.020004312 \t Train Accuracy:  99.34%\nStep:  41200 \t Eval Loss:  0.019166786 \t Eval Accuracy:  99.43%\n##########################################################\nStep:  41300 \t Train Loss:  0.02464862 \t Train Accuracy:  99.24%\nStep:  41300 \t Eval Loss:  0.026542006 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  41400 \t Train Loss:  0.017206749 \t Train Accuracy:  99.46%\nStep:  41400 \t Eval Loss:  0.031722926 \t Eval Accuracy:  99.06%\n##########################################################\nStep:  41500 \t Train Loss:  0.020883437 \t Train Accuracy:  99.39%\nStep:  41500 \t Eval Loss:  0.024427377 \t Eval Accuracy:  99.33%\n##########################################################\nStep:  41600 \t Train Loss:  0.021140672 \t Train Accuracy:  99.39%\nStep:  41600 \t Eval Loss:  0.021577459 \t Eval Accuracy:  99.38%\n##########################################################\nStep:  41700 \t Train Loss:  0.017845284 \t Train Accuracy:  99.49%\nStep:  41700 \t Eval Loss:  0.027083099 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  41800 \t Train Loss:  0.023650032 \t Train Accuracy:  99.27%\nStep:  41800 \t Eval Loss:  0.023370799 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  41900 \t Train Loss:  0.020875562 \t Train Accuracy:  99.41%\nStep:  41900 \t Eval Loss:  0.024450757 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  42000 \t Train Loss:  0.022251522 \t Train Accuracy:  99.27%\nStep:  42000 \t Eval Loss:  0.02158827 \t Eval Accuracy:  99.41%\n##########################################################\nStep:  42100 \t Train Loss:  0.019699551 \t Train Accuracy:  99.34%\nStep:  42100 \t Eval Loss:  0.023899263 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  42200 \t Train Loss:  0.020284744 \t Train Accuracy:  99.29%\nStep:  42200 \t Eval Loss:  0.022762336 \t Eval Accuracy:  99.33%\n##########################################################\nStep:  42300 \t Train Loss:  0.022249011 \t Train Accuracy:  99.32%\nStep:  42300 \t Eval Loss:  0.020263562 \t Eval Accuracy:  99.35%\n##########################################################\nStep:  42400 \t Train Loss:  0.021364514 \t Train Accuracy:  99.30%\nStep:  42400 \t Eval Loss:  0.024366993 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  42500 \t Train Loss:  0.020243205 \t Train Accuracy:  99.34%\nStep:  42500 \t Eval Loss:  0.021701336 \t Eval Accuracy:  99.38%\n##########################################################\nStep:  42600 \t Train Loss:  0.021466225 \t Train Accuracy:  99.37%\nStep:  42600 \t Eval Loss:  0.024929069 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  42700 \t Train Loss:  0.02208167 \t Train Accuracy:  99.32%\nStep:  42700 \t Eval Loss:  0.022639502 \t Eval Accuracy:  99.38%\n##########################################################\nStep:  42800 \t Train Loss:  0.018152408 \t Train Accuracy:  99.43%\nStep:  42800 \t Eval Loss:  0.024744254 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  42900 \t Train Loss:  0.025374798 \t Train Accuracy:  99.24%\nStep:  42900 \t Eval Loss:  0.022056602 \t Eval Accuracy:  99.34%\n##########################################################\nStep:  43000 \t Train Loss:  0.02060874 \t Train Accuracy:  99.35%\nStep:  43000 \t Eval Loss:  0.024326097 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  43100 \t Train Loss:  0.020507624 \t Train Accuracy:  99.38%\nStep:  43100 \t Eval Loss:  0.023078497 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  43200 \t Train Loss:  0.01865454 \t Train Accuracy:  99.37%\nStep:  43200 \t Eval Loss:  0.025567092 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  43300 \t Train Loss:  0.02093672 \t Train Accuracy:  99.34%\nStep:  43300 \t Eval Loss:  0.021612305 \t Eval Accuracy:  99.32%\n##########################################################\nStep:  43400 \t Train Loss:  0.021686804 \t Train Accuracy:  99.32%\nStep:  43400 \t Eval Loss:  0.027511433 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  43500 \t Train Loss:  0.019089788 \t Train Accuracy:  99.38%\nStep:  43500 \t Eval Loss:  0.027268238 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  43600 \t Train Loss:  0.018464282 \t Train Accuracy:  99.49%\nStep:  43600 \t Eval Loss:  0.021758962 \t Eval Accuracy:  99.37%\n##########################################################\nStep:  43700 \t Train Loss:  0.020846084 \t Train Accuracy:  99.33%\nStep:  43700 \t Eval Loss:  0.02146945 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  43800 \t Train Loss:  0.018708602 \t Train Accuracy:  99.38%\nStep:  43800 \t Eval Loss:  0.02650319 \t Eval Accuracy:  99.08%\n##########################################################\nStep:  43900 \t Train Loss:  0.02283762 \t Train Accuracy:  99.32%\nStep:  43900 \t Eval Loss:  0.024452094 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  44000 \t Train Loss:  0.019451417 \t Train Accuracy:  99.32%\nStep:  44000 \t Eval Loss:  0.026543275 \t Eval Accuracy:  99.17%\n##########################################################\nStep:  44100 \t Train Loss:  0.024286047 \t Train Accuracy:  99.27%\nStep:  44100 \t Eval Loss:  0.02258612 \t Eval Accuracy:  99.33%\n##########################################################\nStep:  44200 \t Train Loss:  0.019813504 \t Train Accuracy:  99.34%\nStep:  44200 \t Eval Loss:  0.025285035 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  44300 \t Train Loss:  0.021626243 \t Train Accuracy:  99.34%\nStep:  44300 \t Eval Loss:  0.026258143 \t Eval Accuracy:  99.35%\n##########################################################\nStep:  44400 \t Train Loss:  0.019556623 \t Train Accuracy:  99.43%\nStep:  44400 \t Eval Loss:  0.022375472 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  44500 \t Train Loss:  0.017566796 \t Train Accuracy:  99.39%\nStep:  44500 \t Eval Loss:  0.023686165 \t Eval Accuracy:  99.39%\n##########################################################\nStep:  44600 \t Train Loss:  0.022072468 \t Train Accuracy:  99.26%\nStep:  44600 \t Eval Loss:  0.02439028 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  44700 \t Train Loss:  0.018832847 \t Train Accuracy:  99.34%\nStep:  44700 \t Eval Loss:  0.02576185 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  44800 \t Train Loss:  0.018851975 \t Train Accuracy:  99.29%\nStep:  44800 \t Eval Loss:  0.02587045 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  44900 \t Train Loss:  0.020236686 \t Train Accuracy:  99.35%\nStep:  44900 \t Eval Loss:  0.022089534 \t Eval Accuracy:  99.33%\n##########################################################\nStep:  45000 \t Train Loss:  0.02130757 \t Train Accuracy:  99.33%\nStep:  45000 \t Eval Loss:  0.026602201 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  45100 \t Train Loss:  0.02147983 \t Train Accuracy:  99.32%\nStep:  45100 \t Eval Loss:  0.024834257 \t Eval Accuracy:  99.30%\n##########################################################\nStep:  45200 \t Train Loss:  0.017769273 \t Train Accuracy:  99.51%\nStep:  45200 \t Eval Loss:  0.02456044 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  45300 \t Train Loss:  0.018234428 \t Train Accuracy:  99.43%\nStep:  45300 \t Eval Loss:  0.023212852 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  45400 \t Train Loss:  0.021149987 \t Train Accuracy:  99.32%\nStep:  45400 \t Eval Loss:  0.019826023 \t Eval Accuracy:  99.38%\n##########################################################\nStep:  45500 \t Train Loss:  0.019467257 \t Train Accuracy:  99.43%\nStep:  45500 \t Eval Loss:  0.021688167 \t Eval Accuracy:  99.35%\n##########################################################\nStep:  45600 \t Train Loss:  0.01897951 \t Train Accuracy:  99.41%\nStep:  45600 \t Eval Loss:  0.027602183 \t Eval Accuracy:  99.16%\n##########################################################\nStep:  45700 \t Train Loss:  0.021830179 \t Train Accuracy:  99.37%\nStep:  45700 \t Eval Loss:  0.025743764 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  45800 \t Train Loss:  0.020130407 \t Train Accuracy:  99.39%\nStep:  45800 \t Eval Loss:  0.02698717 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  45900 \t Train Loss:  0.020736912 \t Train Accuracy:  99.30%\nStep:  45900 \t Eval Loss:  0.022580735 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  46000 \t Train Loss:  0.017956175 \t Train Accuracy:  99.44%\nStep:  46000 \t Eval Loss:  0.024350334 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  46100 \t Train Loss:  0.022650119 \t Train Accuracy:  99.24%\nStep:  46100 \t Eval Loss:  0.025277833 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  46200 \t Train Loss:  0.022993077 \t Train Accuracy:  99.28%\nStep:  46200 \t Eval Loss:  0.021993704 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  46300 \t Train Loss:  0.021865673 \t Train Accuracy:  99.29%\nStep:  46300 \t Eval Loss:  0.020093378 \t Eval Accuracy:  99.35%\n##########################################################\nStep:  46400 \t Train Loss:  0.024069672 \t Train Accuracy:  99.15%\nStep:  46400 \t Eval Loss:  0.021543263 \t Eval Accuracy:  99.35%\n##########################################################\nStep:  46500 \t Train Loss:  0.020932913 \t Train Accuracy:  99.34%\nStep:  46500 \t Eval Loss:  0.024745692 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  46600 \t Train Loss:  0.022742743 \t Train Accuracy:  99.37%\nStep:  46600 \t Eval Loss:  0.028046483 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  46700 \t Train Loss:  0.018957697 \t Train Accuracy:  99.41%\nStep:  46700 \t Eval Loss:  0.02311933 \t Eval Accuracy:  99.30%\n##########################################################\nStep:  46800 \t Train Loss:  0.023659106 \t Train Accuracy:  99.26%\nStep:  46800 \t Eval Loss:  0.019858794 \t Eval Accuracy:  99.39%\n##########################################################\nStep:  46900 \t Train Loss:  0.020660445 \t Train Accuracy:  99.35%\nStep:  46900 \t Eval Loss:  0.022522276 \t Eval Accuracy:  99.30%\n##########################################################\nStep:  47000 \t Train Loss:  0.020345468 \t Train Accuracy:  99.45%\nStep:  47000 \t Eval Loss:  0.02135238 \t Eval Accuracy:  99.34%\n##########################################################\nStep:  47100 \t Train Loss:  0.01895778 \t Train Accuracy:  99.45%\nStep:  47100 \t Eval Loss:  0.025482323 \t Eval Accuracy:  99.34%\n##########################################################\nStep:  47200 \t Train Loss:  0.020594962 \t Train Accuracy:  99.34%\nStep:  47200 \t Eval Loss:  0.023938425 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  47300 \t Train Loss:  0.023398947 \t Train Accuracy:  99.28%\nStep:  47300 \t Eval Loss:  0.02094357 \t Eval Accuracy:  99.40%\n##########################################################\nStep:  47400 \t Train Loss:  0.021670246 \t Train Accuracy:  99.37%\nStep:  47400 \t Eval Loss:  0.025063932 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  47500 \t Train Loss:  0.0231514 \t Train Accuracy:  99.27%\nStep:  47500 \t Eval Loss:  0.027731873 \t Eval Accuracy:  99.16%\n##########################################################\nStep:  47600 \t Train Loss:  0.018574374 \t Train Accuracy:  99.37%\nStep:  47600 \t Eval Loss:  0.023546407 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  47700 \t Train Loss:  0.018928437 \t Train Accuracy:  99.37%\nStep:  47700 \t Eval Loss:  0.025592765 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  47800 \t Train Loss:  0.02220337 \t Train Accuracy:  99.27%\nStep:  47800 \t Eval Loss:  0.028661607 \t Eval Accuracy:  99.16%\n##########################################################\nStep:  47900 \t Train Loss:  0.0204475 \t Train Accuracy:  99.39%\nStep:  47900 \t Eval Loss:  0.01916741 \t Eval Accuracy:  99.40%\n##########################################################\nStep:  48000 \t Train Loss:  0.01911043 \t Train Accuracy:  99.40%\nStep:  48000 \t Eval Loss:  0.025325645 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  48100 \t Train Loss:  0.021765884 \t Train Accuracy:  99.32%\nStep:  48100 \t Eval Loss:  0.02881717 \t Eval Accuracy:  99.13%\n##########################################################\nStep:  48200 \t Train Loss:  0.021340044 \t Train Accuracy:  99.34%\nStep:  48200 \t Eval Loss:  0.024751969 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  48300 \t Train Loss:  0.01822358 \t Train Accuracy:  99.40%\nStep:  48300 \t Eval Loss:  0.026815346 \t Eval Accuracy:  99.32%\n##########################################################\nStep:  48400 \t Train Loss:  0.021073133 \t Train Accuracy:  99.35%\nStep:  48400 \t Eval Loss:  0.02268394 \t Eval Accuracy:  99.38%\n##########################################################\nStep:  48500 \t Train Loss:  0.01794109 \t Train Accuracy:  99.41%\nStep:  48500 \t Eval Loss:  0.029392816 \t Eval Accuracy:  99.15%\n##########################################################\nStep:  48600 \t Train Loss:  0.019246057 \t Train Accuracy:  99.37%\nStep:  48600 \t Eval Loss:  0.022463275 \t Eval Accuracy:  99.30%\n##########################################################\nStep:  48700 \t Train Loss:  0.022771452 \t Train Accuracy:  99.30%\nStep:  48700 \t Eval Loss:  0.025699737 \t Eval Accuracy:  99.33%\n##########################################################\nStep:  48800 \t Train Loss:  0.016891439 \t Train Accuracy:  99.46%\nStep:  48800 \t Eval Loss:  0.019614113 \t Eval Accuracy:  99.45%\n##########################################################\nStep:  48900 \t Train Loss:  0.018247813 \t Train Accuracy:  99.44%\nStep:  48900 \t Eval Loss:  0.024636414 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  49000 \t Train Loss:  0.02171891 \t Train Accuracy:  99.35%\nStep:  49000 \t Eval Loss:  0.02249721 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  49100 \t Train Loss:  0.020963423 \t Train Accuracy:  99.39%\nStep:  49100 \t Eval Loss:  0.020833714 \t Eval Accuracy:  99.37%\n##########################################################\nStep:  49200 \t Train Loss:  0.020565804 \t Train Accuracy:  99.38%\nStep:  49200 \t Eval Loss:  0.026762491 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  49300 \t Train Loss:  0.021121535 \t Train Accuracy:  99.43%\nStep:  49300 \t Eval Loss:  0.020239826 \t Eval Accuracy:  99.37%\n##########################################################\nStep:  49400 \t Train Loss:  0.022271367 \t Train Accuracy:  99.37%\nStep:  49400 \t Eval Loss:  0.021716073 \t Eval Accuracy:  99.35%\n##########################################################\nStep:  49500 \t Train Loss:  0.019049542 \t Train Accuracy:  99.38%\nStep:  49500 \t Eval Loss:  0.022098523 \t Eval Accuracy:  99.34%\n##########################################################\nStep:  49600 \t Train Loss:  0.019005638 \t Train Accuracy:  99.41%\nStep:  49600 \t Eval Loss:  0.02387447 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  49700 \t Train Loss:  0.018376969 \t Train Accuracy:  99.39%\nStep:  49700 \t Eval Loss:  0.026254445 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  49800 \t Train Loss:  0.024371613 \t Train Accuracy:  99.17%\nStep:  49800 \t Eval Loss:  0.029999442 \t Eval Accuracy:  99.17%\n##########################################################\nStep:  49900 \t Train Loss:  0.019859489 \t Train Accuracy:  99.43%\nStep:  49900 \t Eval Loss:  0.02356872 \t Eval Accuracy:  99.23%\nCPU times: user 1h 29min, sys: 1h 57s, total: 2h 29min 57s\nWall time: 2h 29min 46s\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt  # Visualization\n\n# Plot loss and accuracy in subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\nax1.set_title('Loss')\nax2.set_title('Accuracy')\n\n\n\nax1.plot(all_train_losses, label='train_loss')\nax1.plot(all_eval_losses, label='eval_loss')\n\nax2.plot(all_train_accuracy, label='train_accuracy')\nax2.plot(all_test_accuracy, label='eval_accuracy')\n\nax1.legend()\nax2.legend()\nplt.show()\nplt.clf()","metadata":{"execution":{"iopub.status.busy":"2024-06-26T13:46:47.505495Z","iopub.execute_input":"2024-06-26T13:46:47.505842Z","iopub.status.idle":"2024-06-26T13:46:48.012392Z","shell.execute_reply.started":"2024-06-26T13:46:47.505817Z","shell.execute_reply":"2024-06-26T13:46:48.011561Z"},"trusted":true},"execution_count":39,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1500x500 with 2 Axes>","image/png":"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"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 0 Axes>"},"metadata":{}}]},{"cell_type":"code","source":"@partial(jax.jit, static_argnames=(\"length\"))\ndef generate_text(rng, params, var_params, length):\n    def _scan_generate(carry, _):\n        random_key, context = carry\n        logits = model.apply({'params': params, **var_params}, context, training=False, mutable=['other_variables'])[0]\n        rng, rng_subkey = jax.random.split(random_key)\n        new_token = jax.random.categorical(\n          rng_subkey, logits[:, -n_tokens, :], axis=-1, shape=(1, 1)\n        )\n        context = jnp.concatenate([context[:, 1:], new_token], axis=1)\n        print(context.shape)\n        return (rng, context), new_token\n\n    _, new_tokens = jax.lax.scan(\n    _scan_generate,\n    (rng, jnp.expand_dims(test_data[852:852+block_size], axis=0)),\n    (),\n    length=length,\n    )\n    return new_tokens","metadata":{"execution":{"iopub.status.busy":"2024-06-26T13:46:48.013640Z","iopub.execute_input":"2024-06-26T13:46:48.013933Z","iopub.status.idle":"2024-06-26T13:46:48.022748Z","shell.execute_reply.started":"2024-06-26T13:46:48.013908Z","shell.execute_reply":"2024-06-26T13:46:48.021927Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"test_data[852:852+block_size]","metadata":{"execution":{"iopub.status.busy":"2024-06-26T13:46:48.023733Z","iopub.execute_input":"2024-06-26T13:46:48.024027Z","iopub.status.idle":"2024-06-26T13:46:48.381805Z","shell.execute_reply.started":"2024-06-26T13:46:48.023997Z","shell.execute_reply":"2024-06-26T13:46:48.380787Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"Array([51,  6,  1, 53, 56,  1, 43, 50, 57, 43,  1, 63, 53, 59,  1, 42, 53,\n        1, 51, 43,  1, 61, 56, 53, 52, 45, 10,  0, 20, 47, 57,  1, 52, 39,\n       51, 43,  1, 47, 57,  1, 24, 47, 41, 47, 53,  6,  1, 40, 53, 56, 52,\n        1, 47, 52,  1, 25, 39, 52, 58, 59, 39,  8,  0,  0], dtype=int32)"},"metadata":{}}]},{"cell_type":"code","source":"i = 852\ndecode(test_data[i:i+block_size].tolist())","metadata":{"execution":{"iopub.status.busy":"2024-06-26T13:46:48.383012Z","iopub.execute_input":"2024-06-26T13:46:48.383311Z","iopub.status.idle":"2024-06-26T13:46:48.390578Z","shell.execute_reply.started":"2024-06-26T13:46:48.383287Z","shell.execute_reply":"2024-06-26T13:46:48.389581Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"'m, or else you do me wrong:\\nHis name is Licio, born in Mantua.\\n\\n'"},"metadata":{}}]},{"cell_type":"code","source":"new_tokenz = 1000\nkey, subkey = jax.random.split(jax.random.PRNGKey(156))\n# key, subkey = jax.random.split(key)\n# token_gen = generate_text(jnp.zeros((1,block_size)).astype(jnp.int32), new_tokenz, {'params': state.params})\ntoken_gen = generate_text(key, params, var_params, new_tokenz)[:, 0, 0].tolist()\nprint(token_gen)\nprint('\\n')\nprint(decode(token_gen))","metadata":{"execution":{"iopub.status.busy":"2024-06-26T13:46:48.391703Z","iopub.execute_input":"2024-06-26T13:46:48.391965Z","iopub.status.idle":"2024-06-26T13:47:03.491267Z","shell.execute_reply.started":"2024-06-26T13:46:48.391943Z","shell.execute_reply":"2024-06-26T13:47:03.490327Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"(1, 64)\n[24, 33, 15, 17, 26, 32, 21, 27, 10, 0, 26, 53, 58, 1, 61, 46, 39, 58, 1, 57, 39, 47, 42, 1, 47, 52, 1, 46, 47, 57, 1, 50, 59, 50, 50, 63, 11, 1, 21, 1, 56, 43, 39, 42, 1, 52, 53, 1, 57, 43, 58, 1, 43, 60, 43, 52, 1, 58, 53, 1, 58, 46, 63, 1, 54, 43, 56, 44, 43, 41, 58, 47, 53, 52, 0, 13, 54, 54, 56, 53, 53, 56, 1, 46, 43, 1, 41, 39, 52, 52, 53, 58, 1, 40, 39, 52, 47, 57, 46, 43, 56, 57, 1, 53, 5, 43, 56, 7, 44, 43, 57, 57, 43, 41, 58, 53, 56, 5, 57, 46, 39, 51, 43, 52, 58, 0, 31, 59, 54, 54, 43, 56, 57, 1, 58, 46, 53, 59, 45, 46, 58, 1, 47, 52, 1, 39, 1, 42, 53, 47, 58, 1, 41, 53, 51, 43, 1, 58, 53, 1, 46, 43, 60, 43, 8, 0, 0, 28, 30, 27, 31, 28, 27, 21, 19, 10, 0, 14, 56, 43, 39, 58, 46, 43, 43, 6, 1, 40, 59, 58, 1, 57, 50, 43, 43, 54, 1, 53, 44, 1, 47, 52, 60, 53, 57, 58, 8, 0, 0, 28, 30, 21, 26, 15, 17, 1, 17, 16, 35, 13, 30, 16, 10, 0, 35, 46, 63, 6, 1, 58, 46, 43, 1, 20, 53, 56, 58, 43, 52, 57, 47, 53, 6, 0, 37, 53, 59, 1, 54, 39, 57, 58, 1, 40, 63, 1, 58, 46, 43, 47, 56, 1, 53, 61, 52, 1, 44, 53, 56, 1, 40, 56, 47, 52, 45, 1, 58, 46, 39, 58, 1, 40, 43, 43, 52, 1, 58, 46, 43, 1, 40, 47, 45, 0, 21, 1, 41, 53, 51, 43, 1, 63, 53, 59, 56, 1, 41, 53, 59, 52, 58, 56, 63, 51, 43, 52, 6, 1, 39, 50, 50, 1, 58, 46, 43, 1, 41, 56, 53, 61, 52, 1, 61, 43, 56, 43, 1, 40, 43, 1, 39, 0, 51, 39, 52, 21, 1, 39, 58, 1, 53, 59, 52, 1, 44, 53, 43, 51, 47, 52, 45, 6, 1, 58, 46, 47, 57, 11, 1, 61, 46, 47, 41, 46, 1, 39, 58, 58, 43, 52, 42, 58, 46, 47, 53, 52, 1, 56, 53, 53, 58, 12, 0, 0, 28, 27, 24, 21, 36, 17, 26, 17, 31, 10, 0, 32, 46, 39, 58, 1, 42, 43, 39, 42, 50, 63, 1, 46, 43, 56, 1, 46, 39, 58, 46, 1, 61, 43, 1, 42, 53, 2, 0, 0, 25, 21, 30, 13, 26, 16, 13, 10, 0, 35, 46, 39, 58, 1, 58, 46, 53, 59, 1, 50, 39, 57, 58, 1, 52, 39, 58, 59, 56, 43, 6, 1, 57, 46, 53, 59, 50, 42, 0, 1, 22, 59, 50, 47, 43, 58, 1, 51, 43, 1, 47, 57, 1, 39, 1, 50, 53, 40, 43, 56, 58, 1, 57, 51, 47, 50, 43, 8, 0, 35, 46, 47, 41, 46, 1, 54, 50, 43, 39, 57, 47, 52, 45, 1, 46, 39, 52, 42, 8, 1, 32, 46, 43, 52, 1, 21, 1, 46, 39, 60, 43, 1, 47, 58, 1, 51, 43, 56, 41, 63, 2, 0, 26, 53, 58, 1, 51, 39, 49, 43, 1, 58, 46, 43, 1, 53, 56, 47, 58, 46, 6, 1, 58, 53, 1, 45, 47, 60, 43, 57, 1, 58, 46, 43, 1, 39, 54, 54, 43, 50, 47, 52, 45, 8, 0, 0, 31, 43, 41, 53, 52, 42, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 21, 1, 46, 39, 60, 43, 1, 57, 47, 56, 6, 1, 47, 52, 1, 43, 39, 41, 46, 1, 51, 39, 52, 1, 52, 53, 52, 43, 6, 1, 39, 52, 42, 1, 58, 46, 43, 1, 58, 56, 39, 47, 58, 53, 56, 8, 0, 0, 31, 21, 15, 21, 26, 21, 33, 31, 10, 0, 35, 47, 58, 46, 1, 58, 46, 56, 43, 43, 1, 57, 46, 53, 59, 50, 42, 1, 40, 43, 1, 57, 47, 52, 8, 0, 0, 14, 33, 15, 23, 21, 26, 19, 20, 13, 25, 10, 0, 27, 1, 51, 63, 1, 42, 39, 52, 45, 43, 56, 1, 58, 46, 63, 1, 56, 43, 51, 53, 60, 43, 42, 1, 61, 43, 1, 42, 47, 43, 1, 58, 46, 39, 58, 8, 0, 0, 28, 30, 21, 26, 15, 13, 10, 0, 32, 53, 1, 58, 46, 43, 51, 1, 45, 53, 42, 42, 43, 57, 54, 39, 52, 42, 43, 2, 0, 0, 44, 50, 39, 51, 43, 1, 58, 46, 63, 1, 51, 53, 58, 46, 43, 56, 8, 0, 0, 15, 13, 32, 17, 31, 14, 37, 10, 0, 35, 46, 39, 41, 46, 1, 46, 43, 1, 50, 39, 41, 49, 1, 58, 46, 47, 57, 11, 1, 58, 46, 43, 1, 56, 47, 41, 46, 1, 41, 46, 43, 43, 56, 1, 39, 41, 41, 53, 59, 52, 58, 10, 0, 53, 59, 56, 1, 46, 53, 59, 56, 57, 6, 1, 58, 53, 1, 57, 43, 58, 1, 46, 43, 1, 58, 46, 39, 58, 1, 39, 50, 50, 1, 58, 46, 39, 58, 6, 1, 58, 43, 50, 50, 1, 51, 43, 43, 58, 1, 39, 64, 39, 56, 1, 58, 61, 43, 50, 60, 43, 6, 1, 39, 52, 42, 1, 51, 39, 56, 41, 46, 1, 40, 43, 57, 58, 1, 46, 39, 60, 43, 1, 47, 52, 1, 44, 53, 43, 10, 1, 58, 46, 43, 52, 1, 41, 50, 39, 47, 51, 5, 42, 0, 35, 47, 58, 46, 1, 42, 43, 57, 54, 47, 52, 47, 52, 45, 1, 61, 47, 58, 46, 5, 43, 56, 1, 61, 39, 57, 46, 59, 52, 52, 47, 52, 45, 1, 58, 53, 1, 57, 46, 53, 58, 6, 0, 31, 53, 1, 44, 43, 39, 56, 1, 58, 46, 43, 57, 43, 1]\n\n\nLUCENTIO:\nNot what said in his lully; I read no set even to thy perfection\nApproor he cannot banishers o'er-fessector'shament\nSuppers thought in a doit come to heve.\n\nPROSPOIG:\nBreathee, but sleep of invost.\n\nPRINCE EDWARD:\nWhy, the Hortensio,\nYou past by their own for bring that been the big\nI come your countrymen, all the crown were be a\nmanI at oun foeming, this; which attendthion root?\n\nPOLIXENES:\nThat deadly her hath we do!\n\nMIRANDA:\nWhat thou last nature, should\n Juliet me is a lobert smile.\nWhich pleasing hand. Then I have it mercy!\nNot make the orith, to gives the appeling.\n\nSecond Citizen:\nI have sir, in each man none, and the traitor.\n\nSICINIUS:\nWith three should be sin.\n\nBUCKINGHAM:\nO my danger thy removed we die that.\n\nPRINCA:\nTo them goddespande!\n\nflame thy mother.\n\nCATESBY:\nWhach he lack this; the rich cheer account:\nour hours, to set he that all that, tell meet azar twelve, and march best have in foe: then claim'd\nWith despining with'er washunning to shot,\nSo fear these \n","output_type":"stream"}]},{"cell_type":"code","source":"dsfsdhfgjdg hfdgjdgjgfjhs'####################","metadata":{"execution":{"iopub.status.busy":"2024-06-26T13:47:03.492365Z","iopub.execute_input":"2024-06-26T13:47:03.492647Z","iopub.status.idle":"2024-06-26T13:47:03.498813Z","shell.execute_reply.started":"2024-06-26T13:47:03.492622Z","shell.execute_reply":"2024-06-26T13:47:03.497293Z"},"trusted":true},"execution_count":44,"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[44], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    dsfsdhfgjdg hfdgjdgjgfjhs'####################\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 1)\n"],"ename":"SyntaxError","evalue":"unterminated string literal (detected at line 1) (2630675753.py, line 1)","output_type":"error"}]},{"cell_type":"code","source":"len(token_gen)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T13:47:03.500222Z","iopub.status.idle":"2024-06-26T13:47:03.500697Z","shell.execute_reply.started":"2024-06-26T13:47:03.500447Z","shell.execute_reply":"2024-06-26T13:47:03.500466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx = 882\ntokenizer.decode(test_data[idx:idx+32])","metadata":{"execution":{"iopub.status.busy":"2024-06-26T13:47:03.502442Z","iopub.status.idle":"2024-06-26T13:47:03.502905Z","shell.execute_reply.started":"2024-06-26T13:47:03.502676Z","shell.execute_reply":"2024-06-26T13:47:03.502695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer('bestopleled', return_tensors='np')","metadata":{"execution":{"iopub.status.busy":"2024-06-26T13:47:03.504572Z","iopub.status.idle":"2024-06-26T13:47:03.505010Z","shell.execute_reply.started":"2024-06-26T13:47:03.504791Z","shell.execute_reply":"2024-06-26T13:47:03.504810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.decode([1991])","metadata":{"execution":{"iopub.status.busy":"2024-06-26T13:47:03.506128Z","iopub.status.idle":"2024-06-26T13:47:03.506448Z","shell.execute_reply.started":"2024-06-26T13:47:03.506288Z","shell.execute_reply":"2024-06-26T13:47:03.506301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params['Dense_12']['kernel'].shape","metadata":{"execution":{"iopub.status.busy":"2024-06-26T13:47:03.508031Z","iopub.status.idle":"2024-06-26T13:47:03.508379Z","shell.execute_reply.started":"2024-06-26T13:47:03.508222Z","shell.execute_reply":"2024-06-26T13:47:03.508236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rngk = jax.random.PRNGKey(389)\nxs, ys = get_batch(rngk, train_data)\nprint(xs[0])\nprint(ys[0])","metadata":{"execution":{"iopub.status.busy":"2024-06-26T13:47:03.509585Z","iopub.status.idle":"2024-06-26T13:47:03.509886Z","shell.execute_reply.started":"2024-06-26T13:47:03.509736Z","shell.execute_reply":"2024-06-26T13:47:03.509748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logits = model.apply({'params': params, **var_params}, xs[0].reshape((1,64)), training=False, mutable=['other_variables'])[0]\nrng, rng_subkey = jax.random.split(rngk)\nfor pso in range(n_tokens):\n    new_token = jax.random.categorical(\n      rng_subkey, logits[:, -1*(n_tokens-pso), :], axis=-1, shape=(1, 1)\n    )\n    print(new_token)","metadata":{"execution":{"iopub.status.busy":"2024-06-26T13:47:03.511569Z","iopub.status.idle":"2024-06-26T13:47:03.512000Z","shell.execute_reply.started":"2024-06-26T13:47:03.511775Z","shell.execute_reply":"2024-06-26T13:47:03.511793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_tok = [51,49,46,46,46,52]\nprint(decode(ys[0].tolist()))","metadata":{"execution":{"iopub.status.busy":"2024-06-26T13:47:03.513304Z","iopub.status.idle":"2024-06-26T13:47:03.513640Z","shell.execute_reply.started":"2024-06-26T13:47:03.513476Z","shell.execute_reply":"2024-06-26T13:47:03.513490Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"act_tk = [60, 43, 50, 57,  1, 47]\nprint(decode(act_tk))","metadata":{"execution":{"iopub.status.busy":"2024-06-26T13:47:03.514624Z","iopub.status.idle":"2024-06-26T13:47:03.514947Z","shell.execute_reply.started":"2024-06-26T13:47:03.514787Z","shell.execute_reply":"2024-06-26T13:47:03.514800Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jax.nn.standardize(jnp.array([2.0,3.0,4.0]))","metadata":{"id":"Oe_GIDP2HFyt","outputId":"5d3dce16-fcc2-40b9-c49a-00a8c4013ca2","execution":{"iopub.status.busy":"2024-06-26T13:47:03.515928Z","iopub.status.idle":"2024-06-26T13:47:03.516279Z","shell.execute_reply.started":"2024-06-26T13:47:03.516107Z","shell.execute_reply":"2024-06-26T13:47:03.516121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@struct.dataclass\nclass Metrics(metrics.Collection):\n    accuracy: metrics.Accuracy\n    loss: metrics.Average.from_output('loss')","metadata":{"id":"s3nN1jOiHFyu","execution":{"iopub.status.busy":"2024-06-26T13:47:03.517720Z","iopub.status.idle":"2024-06-26T13:47:03.518049Z","shell.execute_reply.started":"2024-06-26T13:47:03.517888Z","shell.execute_reply":"2024-06-26T13:47:03.517902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainState(train_state.TrainState):\n    metrics: Metrics\n\ndef create_train_state(module, rng, learning_rate, train_shape):\n    \"\"\"Creates an initial `TrainState`.\"\"\"\n    params = module.init(rng, jnp.ones(train_shape).astype(jnp.int32), \n                         training=False)['params'] # initialize parameters by passing a template image\n    tx = optax.adamw(learning_rate)\n    return TrainState.create(\n      apply_fn=module.apply, params=params, tx=tx,\n      metrics=Metrics.empty(),\n    )","metadata":{"id":"7LLDTSFQHFyu","execution":{"iopub.status.busy":"2024-06-26T13:47:03.519329Z","iopub.status.idle":"2024-06-26T13:47:03.519737Z","shell.execute_reply.started":"2024-06-26T13:47:03.519507Z","shell.execute_reply":"2024-06-26T13:47:03.519521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TrainState.create(","metadata":{"execution":{"iopub.status.busy":"2024-06-26T13:47:03.521482Z","iopub.status.idle":"2024-06-26T13:47:03.521784Z","shell.execute_reply.started":"2024-06-26T13:47:03.521634Z","shell.execute_reply":"2024-06-26T13:47:03.521646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@jax.jit\ndef train_step(state, inputs, targets):\n    \"\"\"Train for a single step.\"\"\"\n    def loss_fn(params):\n        logits = state.apply_fn({'params': params}, inputs, training=True, \n                                rngs={\"dropout\": key})[0]\n        loss = optax.softmax_cross_entropy_with_integer_labels(\n            logits=logits, labels=targets).mean()\n        return loss\n    grad_fn = jax.grad(loss_fn)\n    grads = grad_fn(state.params)\n    state = state.apply_gradients(grads=grads)\n    return state","metadata":{"id":"zApWXUDaHFyu","execution":{"iopub.status.busy":"2024-06-26T13:47:03.523857Z","iopub.status.idle":"2024-06-26T13:47:03.524315Z","shell.execute_reply.started":"2024-06-26T13:47:03.524067Z","shell.execute_reply":"2024-06-26T13:47:03.524106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@jax.jit\ndef compute_metrics(*, state, inputs, targets):\n    logits = state.apply_fn({'params': state.params}, inputs, training=False)[0]\n    loss = optax.softmax_cross_entropy_with_integer_labels(\n        logits=logits, labels=targets).mean()\n    metric_updates = state.metrics.single_from_model_output(\n    logits=logits, labels=targets, loss=loss)\n    metrics = state.metrics.merge(metric_updates)\n    state = state.replace(metrics=metrics)\n    return state","metadata":{"id":"VzukZ4iEHFyv","execution":{"iopub.status.busy":"2024-06-26T13:47:03.525593Z","iopub.status.idle":"2024-06-26T13:47:03.526034Z","shell.execute_reply.started":"2024-06-26T13:47:03.525812Z","shell.execute_reply":"2024-06-26T13:47:03.525830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 10\nlearning_rate = 0.005\ninit_rng = jax.random.key(0)","metadata":{"id":"ehYvMeuNHFyv","execution":{"iopub.status.busy":"2024-06-26T13:47:03.527372Z","iopub.status.idle":"2024-06-26T13:47:03.527804Z","shell.execute_reply.started":"2024-06-26T13:47:03.527584Z","shell.execute_reply":"2024-06-26T13:47:03.527602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"state = create_train_state(fin_model, init_rng, learning_rate, train_shape)\ndel init_rng  # Must not be used anymore.","metadata":{"id":"D60UHLFHHFyv","execution":{"iopub.status.busy":"2024-06-26T13:47:03.529958Z","iopub.status.idle":"2024-06-26T13:47:03.530414Z","shell.execute_reply.started":"2024-06-26T13:47:03.530181Z","shell.execute_reply":"2024-06-26T13:47:03.530200Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics_history = {'train_loss': [],\n                   'train_accuracy': [],\n                   'test_loss': [],\n                   'test_accuracy': []}","metadata":{"id":"Jl-9TlHEHFyv","execution":{"iopub.status.busy":"2024-06-26T13:47:03.531650Z","iopub.status.idle":"2024-06-26T13:47:03.531982Z","shell.execute_reply.started":"2024-06-26T13:47:03.531819Z","shell.execute_reply":"2024-06-26T13:47:03.531833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 442\nkey = jax.random.PRNGKey(SEED)\nloss = 10\ncounter = 0\n# for step in tqdm(range(max_iters)): # increase number of steps for good results...\nwhile counter==max_iters or loss > 1.0:\n\n      # sample a batch of data\n    xb, yb = get_batch(key, train_data)\n    state = train_step(state, xb, yb)\n    state = compute_metrics(state=state, inputs=xb, targets=yb)\n\n    key = (jax.random.split(key)[0])\n\n    if step == 0 or (step+1) % 100 == 0: # one training epoch has passed\n        for metric,value in state.metrics.compute().items(): # compute metrics\n            metrics_history[f'train_{metric}'].append(value) # record metrics\n        state = state.replace(metrics=state.metrics.empty()) # reset train_metrics for next training epoch\n\n        # Compute metrics on the test set after each training epoch\n        test_state = state\n        x_test, y_test = get_batch(key, test_data)\n    #     for test_batch in test_ds.as_numpy_iterator():\n        test_state = compute_metrics(state=test_state, inputs=x_test, targets=y_test)\n\n        for metric,value in test_state.metrics.compute().items():\n            metrics_history[f'test_{metric}'].append(value)\n\n        print(f\"train epoch: {(step+1)}, \"\n              f\"loss: {metrics_history['train_loss'][-1]}, \"\n              f\"accuracy: {metrics_history['train_accuracy'][-1] * 100}\")\n        print(f\"test epoch: {(step+1) }, \"\n          f\"loss: {metrics_history['test_loss'][-1]}, \"\n          f\"accuracy: {metrics_history['test_accuracy'][-1] * 100}\")","metadata":{"id":"CaNt9JazHFyw","outputId":"ba447ddf-9940-44a6-f4b2-d27ed78a88c2","execution":{"iopub.status.busy":"2024-06-26T13:47:03.533641Z","iopub.status.idle":"2024-06-26T13:47:03.534018Z","shell.execute_reply.started":"2024-06-26T13:47:03.533856Z","shell.execute_reply":"2024-06-26T13:47:03.533870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt  # Visualization\n\n# Plot loss and accuracy in subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\nax1.set_title('Loss')\nax2.set_title('Accuracy')\nfor dataset in ('train','test'):\n    ax1.plot(metrics_history[f'{dataset}_loss'], label=f'{dataset}_loss')\n    ax2.plot(metrics_history[f'{dataset}_accuracy'], label=f'{dataset}_accuracy')\nax1.legend()\nax2.legend()\nplt.show()\nplt.clf()","metadata":{"id":"Y40JGx1YHFyw","execution":{"iopub.status.busy":"2024-06-26T13:47:03.535345Z","iopub.status.idle":"2024-06-26T13:47:03.535657Z","shell.execute_reply.started":"2024-06-26T13:47:03.535505Z","shell.execute_reply":"2024-06-26T13:47:03.535518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nlogits = fin_model.apply(fin_params, xb, training=False)[0]\nloss = optax.softmax_cross_entropy_with_integer_labels(\n            logits=logits, labels=yb).mean()\n\nprint(loss)","metadata":{"id":"7pJlFXpVHFyw","execution":{"iopub.status.busy":"2024-06-26T13:47:03.538711Z","iopub.status.idle":"2024-06-26T13:47:03.539049Z","shell.execute_reply.started":"2024-06-26T13:47:03.538887Z","shell.execute_reply":"2024-06-26T13:47:03.538901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def generate_text(idx, max_new_tokens, params):\n# # idx is (B, T) array of indices in the current context\n#     for i in range(max_new_tokens):\n#         # crop idx to the last block_size tokens\n#         idx_cond = idx[:, -block_size:]\n#         # get the predictions\n#         logits = fin_model.apply(params, idx_cond)\n#         # focus only on the last time step\n#         logits = logits[:, -1, :] # becomes (B, C)\n\n#         if i == 0:\n#             rng, rng_subkey = jax.random.split(jax.random.PRNGKey(12))\n#         else:\n#             rng, rng_subkey = jax.random.split(rng)\n\n#         idx_next = jax.random.categorical(rng_subkey, logits, axis=-1, shape=(1, 1)) # (B, 1)\n\n\n#         # append sampled index to the running sequence\n#         idx = jnp.concatenate([idx, idx_next], axis=-1) # (B, T+1)\n\n#     return idx","metadata":{"id":"9d28o-dTHFyx","execution":{"iopub.status.busy":"2024-06-26T13:47:03.540266Z","iopub.status.idle":"2024-06-26T13:47:03.540608Z","shell.execute_reply.started":"2024-06-26T13:47:03.540437Z","shell.execute_reply":"2024-06-26T13:47:03.540451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@partial(jax.jit, static_argnames=(\"self\", \"length\"))\ndef generate_text(rng, params, length):\n    def _scan_generate(carry, _):\n        random_key, context = carry\n        logits = fin_model.apply(params, context, training=False)[0]\n        rng, rng_subkey = jax.random.split(random_key)\n        new_token = jax.random.categorical(\n          rng_subkey, logits[:, -1, :], axis=-1, shape=(1, 1)\n        )\n        context = jnp.concatenate([context[:, 1:], new_token], axis=1)\n        return (rng, context), new_token\n\n    _, new_tokens = jax.lax.scan(\n    _scan_generate,\n    (rng, jnp.zeros((1, block_size), dtype=jnp.int32)),\n    (),\n    length=length,\n    )\n    return new_tokens","metadata":{"id":"WB0og7pAHFyx","execution":{"iopub.status.busy":"2024-06-26T13:47:03.542327Z","iopub.status.idle":"2024-06-26T13:47:03.542663Z","shell.execute_reply.started":"2024-06-26T13:47:03.542501Z","shell.execute_reply":"2024-06-26T13:47:03.542515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_tokenz = 1000\nkey, subkey = jax.random.split(jax.random.PRNGKey(156))\n# key, subkey = jax.random.split(key)\n# token_gen = generate_text(jnp.zeros((1,block_size)).astype(jnp.int32), new_tokenz, {'params': state.params})\ntoken_gen = generate_text(key, {'params': state.params}, new_tokenz)[:, 0, 0].tolist()\nprint(token_gen)\nprint(decode(token_gen))","metadata":{"id":"50Vpg2lEHFyx","execution":{"iopub.status.busy":"2024-06-26T13:47:03.544220Z","iopub.status.idle":"2024-06-26T13:47:03.544560Z","shell.execute_reply.started":"2024-06-26T13:47:03.544388Z","shell.execute_reply":"2024-06-26T13:47:03.544401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sdgh  fs","metadata":{"execution":{"iopub.status.busy":"2024-06-26T13:47:03.545892Z","iopub.status.idle":"2024-06-26T13:47:03.546250Z","shell.execute_reply.started":"2024-06-26T13:47:03.546058Z","shell.execute_reply":"2024-06-26T13:47:03.546089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"state.params","metadata":{"execution":{"iopub.status.busy":"2024-06-26T13:47:03.547406Z","iopub.status.idle":"2024-06-26T13:47:03.547852Z","shell.execute_reply.started":"2024-06-26T13:47:03.547627Z","shell.execute_reply":"2024-06-26T13:47:03.547646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install mamba-ssm","metadata":{"id":"MOw_xjbrHFy0","execution":{"iopub.status.busy":"2024-06-26T13:47:03.549764Z","iopub.status.idle":"2024-06-26T13:47:03.550214Z","shell.execute_reply.started":"2024-06-26T13:47:03.549971Z","shell.execute_reply":"2024-06-26T13:47:03.549989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ones = lambda *size: torch.ones(*size).float().cuda()\nzeros = lambda *size: torch.zeros(*size).float().cuda()\narange = lambda n: torch.arange(n).float().cuda()\nrand = lambda size: torch.rand(*size).abs().float().cuda()\n\ndef create_torch(S = 128, Ba = 2, D = 4, N = 4):\n    x = rand((Ba, 1, D, S))\n    a = -ones((Ba, N, D, 1))\n    b = ones((Ba, N, 1, S)) * 0.1\n    c = rand((Ba, N, 1, S)) * 0.1\n    delta = rand((Ba, 1, D, S)) * 0.1\n    return x, a, b, c, delta","metadata":{"id":"W_PAnYcEOR22","execution":{"iopub.status.busy":"2024-06-26T13:47:03.551783Z","iopub.status.idle":"2024-06-26T13:47:03.552248Z","shell.execute_reply.started":"2024-06-26T13:47:03.551990Z","shell.execute_reply":"2024-06-26T13:47:03.552008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import selective_scan_cuda\n\nxx, aa, bb, cc, ddelta = create_torch()\ny_from_repo = selective_scan_cuda.fwd(xx.squeeze(1), ddelta.squeeze(1), aa[0].squeeze(-1).T, bb.squeeze(-2)[:, None, :, :], cc.squeeze(-2)[:, None, :, :], None, None, None, False)\ny_from_repo","metadata":{"id":"ykh4GTvtOrak","execution":{"iopub.status.busy":"2024-06-26T13:47:03.553922Z","iopub.status.idle":"2024-06-26T13:47:03.554286Z","shell.execute_reply.started":"2024-06-26T13:47:03.554115Z","shell.execute_reply":"2024-06-26T13:47:03.554131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def discretize(a, b, delta):\n    da = delta * a\n    a_ = jnp.exp(da)\n    b_ = b * delta\n    return a_, b_\n\ndef ssm(x, a, b, c, delta):\n    \"Jax Implementation\"\n    y = []\n    h = 0\n    a_, b_ = discretize(a, b, delta)\n    for k in range(x.shape[-1]):\n        h = a_[..., k] * h + b_[..., k] * x[..., k]\n        y.append((c[..., k] * h).sum(1, keepdims=True))\n    return h, jnp.stack(y, -1)\n","metadata":{"id":"NEdG1yPNOtxU","execution":{"iopub.status.busy":"2024-06-26T13:47:03.555525Z","iopub.status.idle":"2024-06-26T13:47:03.555849Z","shell.execute_reply.started":"2024-06-26T13:47:03.555690Z","shell.execute_reply":"2024-06-26T13:47:03.555704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_, y_ = ssm(xx.cpu().numpy(), aa.cpu().numpy(), bb.cpu().numpy(), cc.cpu().numpy(), ddelta.cpu().numpy())","metadata":{"id":"GEjNcZSZPIp_","execution":{"iopub.status.busy":"2024-06-26T13:47:03.558249Z","iopub.status.idle":"2024-06-26T13:47:03.558582Z","shell.execute_reply.started":"2024-06-26T13:47:03.558422Z","shell.execute_reply":"2024-06-26T13:47:03.558435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"tWlqZZOmPnYk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from mamba_ssm import Mamba as Mamba_T\ntorch_mamba = Mamba_T(\n      # This module uses roughly 3 * expand * d_model^2 parameters\n      d_model=n_embd, # Model dimension d_model\n      d_state=16,  # SSM state expansion factor\n      d_conv=4,    # Local convolution width\n      expand=2,    # Block expansion factor\n)","metadata":{"id":"5RHAE_I1Pql9","execution":{"iopub.status.busy":"2024-06-26T13:47:03.559791Z","iopub.status.idle":"2024-06-26T13:47:03.560121Z","shell.execute_reply.started":"2024-06-26T13:47:03.559942Z","shell.execute_reply":"2024-06-26T13:47:03.559954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xm = x = rand((1, 1, n_embd, 32))\nxm.shape","metadata":{"id":"l9zw_M-USrDt","execution":{"iopub.status.busy":"2024-06-26T13:47:03.561331Z","iopub.status.idle":"2024-06-26T13:47:03.561655Z","shell.execute_reply.started":"2024-06-26T13:47:03.561496Z","shell.execute_reply":"2024-06-26T13:47:03.561510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch_mamba(xm.squeeze(1))","metadata":{"id":"gGmA2EWlTCo0","execution":{"iopub.status.busy":"2024-06-26T13:47:03.563364Z","iopub.status.idle":"2024-06-26T13:47:03.563702Z","shell.execute_reply.started":"2024-06-26T13:47:03.563543Z","shell.execute_reply":"2024-06-26T13:47:03.563557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch_mamba.in_proj","metadata":{"id":"73ek9mx9UBBl","execution":{"iopub.status.busy":"2024-06-26T13:47:03.565205Z","iopub.status.idle":"2024-06-26T13:47:03.565666Z","shell.execute_reply.started":"2024-06-26T13:47:03.565422Z","shell.execute_reply":"2024-06-26T13:47:03.565442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import CLIPTokenizer\ntokenizer_1 = CLIPTokenizer.from_pretrained('openai/clip-vit-base-patch32')","metadata":{"id":"P3l_ssIYbiYT","execution":{"iopub.status.busy":"2024-06-26T13:47:03.566829Z","iopub.status.idle":"2024-06-26T13:47:03.567299Z","shell.execute_reply.started":"2024-06-26T13:47:03.567043Z","shell.execute_reply":"2024-06-26T13:47:03.567062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenise_prompts(prompt):\n    inputs = []\n    for tokenizer in [tokenizer_1, tokenizer_2]:\n        text_inputs = tokenizer(\n            positive_prompt,\n            padding=\"max_length\",\n            max_length=tokenizer.model_max_length,\n            truncation=True,\n            return_tensors=\"np\",\n        )\n        inputs.append(text_inputs.input_ids)\n    return jnp.stack(inputs, axis=1)","metadata":{"id":"-X7hXQRMZhl3","execution":{"iopub.status.busy":"2024-06-26T13:47:03.568698Z","iopub.status.idle":"2024-06-26T13:47:03.569167Z","shell.execute_reply.started":"2024-06-26T13:47:03.568916Z","shell.execute_reply":"2024-06-26T13:47:03.568934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CNN(nn.Module):\n    \"\"\"A simple CNN model.\"\"\"\n\n    @nn.compact\n    def __call__(self, x):\n        x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n        print(x.shape)\n        x = nn.relu(x)\n        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n        print(x.shape)\n        x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n        print(x.shape)\n        x = nn.relu(x)\n        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n        print(x.shape)\n        x = x.reshape((x.shape[0], -1))  # flatten\n        print(x.shape)\n        x = nn.Dense(features=256)(x)\n        print(x.shape)\n        x = nn.relu(x)\n        x = nn.Dense(features=10)(x)\n        print(x.shape)\n        return x","metadata":{"id":"rJhKQ_Oua9Gy","execution":{"iopub.status.busy":"2024-06-26T13:47:03.570367Z","iopub.status.idle":"2024-06-26T13:47:03.570694Z","shell.execute_reply.started":"2024-06-26T13:47:03.570535Z","shell.execute_reply":"2024-06-26T13:47:03.570549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rng = jax.random.PRNGKey(20)\ncnn = CNN()\nparams = cnn.init(rng, jnp.ones([1, 28, 28, 1]))['params']","metadata":{"id":"mzkoYrSVkoJj","execution":{"iopub.status.busy":"2024-06-26T13:47:03.571617Z","iopub.status.idle":"2024-06-26T13:47:03.571913Z","shell.execute_reply.started":"2024-06-26T13:47:03.571764Z","shell.execute_reply":"2024-06-26T13:47:03.571776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Conv1d(nn.Module):\n    \"\"\"A simple 1D CNN model.\"\"\"\n\n    @nn.compact\n    def __call__(self, x):\n        x = nn.Embed(65, 256)(x) + nn.Embed(64, 256)(jnp.arange(64))\n        print(x.shape)\n#         x = nn.Dense(features=512)(x)\n#         print(x.shape)\n        x = nn.Conv(features=256, kernel_size=3, padding=1)(x)\n        print(x.shape)\n        x = nn.avg_pool(x, window_shape=(2,), strides=(2,))\n        print(x.shape)\n        x = nn.Conv(features=512, kernel_size=3, padding=1)(x)\n        print(x.shape)\n        x = nn.avg_pool(x, window_shape=(2,), strides=(2,))\n        print(x.shape)\n        x = nn.ConvTranspose(features=512, kernel_size=2, padding='same')(x)\n        print(x.shape)\n        x = nn.Dense(features=65)(x)\n        print(x.shape)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-06-26T13:47:03.573150Z","iopub.status.idle":"2024-06-26T13:47:03.573456Z","shell.execute_reply.started":"2024-06-26T13:47:03.573304Z","shell.execute_reply":"2024-06-26T13:47:03.573316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rng = jax.random.PRNGKey(204)\nmycon1d = Conv1d()\nparams = mycon1d.init(rng, jnp.ones((1, 64), dtype=jnp.int32))['params']","metadata":{"execution":{"iopub.status.busy":"2024-06-26T13:47:03.574682Z","iopub.status.idle":"2024-06-26T13:47:03.575131Z","shell.execute_reply.started":"2024-06-26T13:47:03.574887Z","shell.execute_reply":"2024-06-26T13:47:03.574904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}