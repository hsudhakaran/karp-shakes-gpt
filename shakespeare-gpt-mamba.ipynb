{"metadata":{"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8613626,"sourceType":"datasetVersion","datasetId":5155031}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install -q clu","metadata":{"id":"gS6euWNvHFye","outputId":"45b149a7-9450-439c-da67-ab8678a3b0d0","execution":{"iopub.status.busy":"2024-06-13T10:24:41.845994Z","iopub.execute_input":"2024-06-13T10:24:41.846517Z","iopub.status.idle":"2024-06-13T10:24:41.852325Z","shell.execute_reply.started":"2024-06-13T10:24:41.846479Z","shell.execute_reply":"2024-06-13T10:24:41.851420Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# # We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt","metadata":{"id":"7jjCLfuUHFyg","outputId":"dfe048f0-dd44-40ef-edf3-2fa56558672f","execution":{"iopub.status.busy":"2024-06-13T10:24:41.853795Z","iopub.execute_input":"2024-06-13T10:24:41.854071Z","iopub.status.idle":"2024-06-13T10:24:41.864674Z","shell.execute_reply.started":"2024-06-13T10:24:41.854048Z","shell.execute_reply":"2024-06-13T10:24:41.863748Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from functools import partial\nimport jax\nimport jax.numpy as jnp\nfrom flax import linen as nn\nfrom jax.nn.initializers import lecun_normal, normal\nfrom jax.numpy.linalg import eigh, inv, matrix_power\nfrom jax.scipy.signal import convolve\n\nimport torch\n\nfrom dataclasses import dataclass\n\nfrom typing import Union\n\nimport matplotlib.pyplot as plt\nimport seaborn\n\n# from clu import metrics\nfrom flax.training import train_state  # Useful dataclass to keep train state\nfrom flax import struct                # Flax dataclasses\nimport optax                           # Common loss functions and optimizers\nfrom tqdm import tqdm","metadata":{"id":"YXSCJzupHFyh","execution":{"iopub.status.busy":"2024-06-13T10:24:41.866114Z","iopub.execute_input":"2024-06-13T10:24:41.866395Z","iopub.status.idle":"2024-06-13T10:24:48.781660Z","shell.execute_reply.started":"2024-06-13T10:24:41.866372Z","shell.execute_reply":"2024-06-13T10:24:48.780696Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# read it in to inspect it\nwith open('/kaggle/input/shak-new-input/input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()","metadata":{"id":"KpJoV3KQHFyh","execution":{"iopub.status.busy":"2024-06-13T10:24:48.783409Z","iopub.execute_input":"2024-06-13T10:24:48.783954Z","iopub.status.idle":"2024-06-13T10:24:48.810834Z","shell.execute_reply.started":"2024-06-13T10:24:48.783920Z","shell.execute_reply":"2024-06-13T10:24:48.810075Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)","metadata":{"id":"PsWxZqyRHFyi","outputId":"b1730724-647e-45cd-edfa-97af24995830","execution":{"iopub.status.busy":"2024-06-13T10:24:48.811861Z","iopub.execute_input":"2024-06-13T10:24:48.812123Z","iopub.status.idle":"2024-06-13T10:24:48.857730Z","shell.execute_reply.started":"2024-06-13T10:24:48.812099Z","shell.execute_reply":"2024-06-13T10:24:48.856703Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"\n !\"&',-.:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n64\n","output_type":"stream"}]},{"cell_type":"code","source":"# from transformers import AutoTokenizer\n\n# tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Phi-3-mini-4k-instruct\", padding_side=\"left\")","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:24:48.859702Z","iopub.execute_input":"2024-06-13T10:24:48.859966Z","iopub.status.idle":"2024-06-13T10:24:48.868159Z","shell.execute_reply.started":"2024-06-13T10:24:48.859942Z","shell.execute_reply":"2024-06-13T10:24:48.867399Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# text_inputs = tokenizer(text, return_tensors=\"np\")\n# data = jnp.array(text_inputs['input_ids'][0])","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:24:48.869078Z","iopub.execute_input":"2024-06-13T10:24:48.869331Z","iopub.status.idle":"2024-06-13T10:24:48.879909Z","shell.execute_reply.started":"2024-06-13T10:24:48.869307Z","shell.execute_reply":"2024-06-13T10:24:48.879186Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# vocab_size = tokenizer.vocab_size\n# print(vocab_size)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:24:48.880817Z","iopub.execute_input":"2024-06-13T10:24:48.881084Z","iopub.status.idle":"2024-06-13T10:24:48.890241Z","shell.execute_reply.started":"2024-06-13T10:24:48.881061Z","shell.execute_reply":"2024-06-13T10:24:48.889390Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# print(tokenizer.decode((text_inputs['input_ids'][0][0:100]).tolist()))","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:24:48.891211Z","iopub.execute_input":"2024-06-13T10:24:48.891497Z","iopub.status.idle":"2024-06-13T10:24:48.901059Z","shell.execute_reply.started":"2024-06-13T10:24:48.891475Z","shell.execute_reply":"2024-06-13T10:24:48.900397Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# create a mapping from characters to integers\nstoi = { ch: i for i,ch in enumerate(chars) }\nitos = { i: ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))","metadata":{"id":"S-mzLOk1HFyi","outputId":"f56e2f85-5a1c-4099-87df-436ba39f4363","execution":{"iopub.status.busy":"2024-06-13T10:24:48.902105Z","iopub.execute_input":"2024-06-13T10:24:48.902414Z","iopub.status.idle":"2024-06-13T10:24:48.912876Z","shell.execute_reply.started":"2024-06-13T10:24:48.902390Z","shell.execute_reply":"2024-06-13T10:24:48.912088Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"[45, 46, 46, 1, 57, 45, 42, 55, 42]\nhii there\n","output_type":"stream"}]},{"cell_type":"code","source":"data = jnp.array(encode(text), dtype=jnp.int32)\nprint(data.shape, data.dtype)\nprint(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this","metadata":{"id":"HImuqDd8HFyj","outputId":"91dcd15f-f068-4551-ad29-e6e41e52fd91","execution":{"iopub.status.busy":"2024-06-13T10:24:48.913805Z","iopub.execute_input":"2024-06-13T10:24:48.914077Z","iopub.status.idle":"2024-06-13T10:24:54.221002Z","shell.execute_reply.started":"2024-06-13T10:24:48.914053Z","shell.execute_reply":"2024-06-13T10:24:54.220082Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"(2643247,) int32\n[17 46 55 56 57  1 14 46 57 46 63 42 51  9  0 13 42 43 52 55 42  1 60 42\n  1 53 55 52 40 42 42 41  1 38 51 62  1 43 58 55 57 45 42 55  6  1 45 42\n 38 55  1 50 42  1 56 53 42 38 48  8  0  0 12 49 49  9  0 30 53 42 38 48\n  6  1 56 53 42 38 48  8  0  0 17 46 55 56 57  1 14 46 57 46 63 42 51  9\n  0 36 52 58  1 38 55 42  1 38 49 49  1 55 42 56 52 49 59 42 41  1 55 38\n 57 45 42 55  1 57 52  1 41 46 42  1 57 45 38 51  1 57 52  1 43 38 50 46\n 56 45 11  0  0 12 49 49  9  0 29 42 56 52 49 59 42 41  8  1 55 42 56 52\n 49 59 42 41  8  0  0 17 46 55 56 57  1 14 46 57 46 63 42 51  9  0 17 46\n 55 56 57  6  1 62 52 58  1 48 51 52 60  1 14 38 46 58 56  1 24 38 55 40\n 46 58 56  1 46 56  1 40 45 46 42 43  1 42 51 42 50 62  1 57 52  1 57 45\n 42  1 53 42 52 53 49 42  8  0  0 12 49 49  9  0 34 42  1 48 51 52 60  5\n 57  6  1 60 42  1 48 51 52 60  5 57  8  0  0 17 46 55 56 57  1 14 46 57\n 46 63 42 51  9  0 23 42 57  1 58 56  1 48 46 49 49  1 45 46 50  6  1 38\n 51 41  1 60 42  5 49 49  1 45 38 59 42  1 40 52 55 51  1 38 57  1 52 58\n 55  1 52 60 51  1 53 55 46 40 42  8  0 20 56  5 57  1 38  1 59 42 55 41\n 46 40 57 11  0  0 12 49 49  9  0 25 52  1 50 52 55 42  1 57 38 49 48 46\n 51 44  1 52 51  5 57 10  1 49 42 57  1 46 57  1 39 42  1 41 52 51 42  9\n  1 38 60 38 62  6  1 38 60 38 62  2  0  0 30 42 40 52 51 41  1 14 46 57\n 46 63 42 51  9  0 26 51 42  1 60 52 55 41  6  1 44 52 52 41  1 40 46 57\n 46 63 42 51 56  8  0  0 17 46 55 56 57  1 14 46 57 46 63 42 51  9  0 34\n 42  1 38 55 42  1 38 40 40 52 58 51 57 42 41  1 53 52 52 55  1 40 46 57\n 46 63 42 51 56  6  1 57 45 42  1 53 38 57 55 46 40 46 38 51 56  1 44 52\n 52 41  8  0 34 45 38 57  1 38 58 57 45 52 55 46 57 62  1 56 58 55 43 42\n 46 57 56  1 52 51  1 60 52 58 49 41  1 55 42 49 46 42 59 42  1 58 56  9\n  1 46 43  1 57 45 42 62  0 60 52 58 49 41  1 62 46 42 49 41  1 58 56  1\n 39 58 57  1 57 45 42  1 56 58 53 42 55 43 49 58 46 57 62  6  1 60 45 46\n 49 42  1 46 57  1 60 42 55 42  0 60 45 52 49 42 56 52 50 42  6  1 60 42\n  1 50 46 44 45 57  1 44 58 42 56 56  1 57 45 42 62  1 55 42 49 46 42 59\n 42 41  1 58 56  1 45 58 50 38 51 42 49 62 10  0 39 58 57  1 57 45 42 62\n  1 57 45 46 51 48  1 60 42  1 38 55 42  1 57 52 52  1 41 42 38 55  9  1\n 57 45 42  1 49 42 38 51 51 42 56 56  1 57 45 38 57  0 38 43 43 49 46 40\n 57 56  1 58 56  6  1 57 45 42  1 52 39 47 42 40 57  1 52 43  1 52 58 55\n  1 50 46 56 42 55 62  6  1 46 56  1 38 56  1 38 51  0 46 51 59 42 51 57\n 52 55 62  1 57 52  1 53 38 55 57 46 40 58 49 38 55 46 56 42  1 57 45 42\n 46 55  1 38 39 58 51 41 38 51 40 42 10  1 52 58 55  0 56 58 43 43 42 55\n 38 51 40 42  1 46 56  1 38  1 44 38 46 51  1 57 52  1 57 45 42 50  1 23\n 42 57  1 58 56  1 55 42 59 42 51 44 42  1 57 45 46 56  1 60 46 57 45  0\n 52 58 55  1 53 46 48 42 56  6  1 42 55 42  1 60 42  1 39 42 40 52 50 42\n  1 55 38 48 42 56  9  1 43 52 55  1 57 45 42  1 44 52 41 56  1 48 51 52\n 60  1 20  0 56 53 42 38 48  1 57 45 46 56  1 46 51  1 45 58 51 44 42 55\n  1 43 52 55  1 39 55 42 38 41  6  1 51 52 57  1 46 51  1 57 45 46 55 56\n 57  1 43 52 55  1 55 42 59 42 51 44 42  8  0  0]\n","output_type":"stream"}]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:24:54.225449Z","iopub.execute_input":"2024-06-13T10:24:54.225820Z","iopub.status.idle":"2024-06-13T10:24:54.235337Z","shell.execute_reply.started":"2024-06-13T10:24:54.225791Z","shell.execute_reply":"2024-06-13T10:24:54.234335Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"Array([17, 46, 55, ..., 38, 62,  8], dtype=int32)"},"metadata":{}}]},{"cell_type":"code","source":"len(data)/64/32","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:24:54.236428Z","iopub.execute_input":"2024-06-13T10:24:54.236703Z","iopub.status.idle":"2024-06-13T10:24:54.243690Z","shell.execute_reply.started":"2024-06-13T10:24:54.236679Z","shell.execute_reply":"2024-06-13T10:24:54.242901Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"1290.64794921875"},"metadata":{}}]},{"cell_type":"code","source":"# train_test_split = 0.9\n# n = int(train_test_split*len(data))\n# train_data = data[:n]\n# test_data = data[n:]\n\ntrain_test_split = 0.9\nn = int(train_test_split*len(data))\ntrain_data = data[:n]\ntest_data = data[n:]","metadata":{"id":"pXrAqMxRHFyj","execution":{"iopub.status.busy":"2024-06-13T10:24:54.244723Z","iopub.execute_input":"2024-06-13T10:24:54.244973Z","iopub.status.idle":"2024-06-13T10:24:54.404509Z","shell.execute_reply.started":"2024-06-13T10:24:54.244951Z","shell.execute_reply":"2024-06-13T10:24:54.403721Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"block_size = 8\ntrain_data[:block_size+1]","metadata":{"id":"ahhKyiAzHFyj","outputId":"98306c96-5082-4dfa-ba66-915051831fc8","execution":{"iopub.status.busy":"2024-06-13T10:24:54.405509Z","iopub.execute_input":"2024-06-13T10:24:54.405754Z","iopub.status.idle":"2024-06-13T10:24:54.484037Z","shell.execute_reply.started":"2024-06-13T10:24:54.405733Z","shell.execute_reply":"2024-06-13T10:24:54.482117Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"Array([17, 46, 55, 56, 57,  1, 14, 46, 57], dtype=int32)"},"metadata":{}}]},{"cell_type":"code","source":"x = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")","metadata":{"id":"HIpsznQmHFyk","outputId":"be9d197b-0b79-43ed-f3a9-e74295d51c79","execution":{"iopub.status.busy":"2024-06-13T10:24:54.485294Z","iopub.execute_input":"2024-06-13T10:24:54.485602Z","iopub.status.idle":"2024-06-13T10:24:55.089647Z","shell.execute_reply.started":"2024-06-13T10:24:54.485575Z","shell.execute_reply":"2024-06-13T10:24:55.088491Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"when input is [17] the target: 46\nwhen input is [17 46] the target: 55\nwhen input is [17 46 55] the target: 56\nwhen input is [17 46 55 56] the target: 57\nwhen input is [17 46 55 56 57] the target: 1\nwhen input is [17 46 55 56 57  1] the target: 14\nwhen input is [17 46 55 56 57  1 14] the target: 46\nwhen input is [17 46 55 56 57  1 14 46] the target: 57\n","output_type":"stream"}]},{"cell_type":"code","source":"batch_size = 128 # how many independent sequences will we process in parallel?\nblock_size = 64 # what is the maximum context length for predictions?\nmax_iters = 15000\nlearning_rate = 5e-4\n# device = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 100\nn_embd = 256\nexpans = 2\nn_heads = 1\nchannel_size = n_embd // n_heads\nn_layers = 6\ndropout = 0.2\nconv_k_size = 3\nn_latent_dim = 16\nn_tokens = 1\n\nrng_key = jax.random.PRNGKey(1564)\n\ndynamic_slice_vmap = jax.vmap(jax.lax.dynamic_slice, in_axes=(None, 0, None))\n\n@jax.jit\ndef get_batch(random_key, data):\n    \"\"\"Prepares a random batch of training data.\n\n    Args:\n      random_key: A random seed for sampling a batch.\n      data: The complete training dataset.\n\n    Returns:\n      x: Input sequences.\n      y: Target sequences (shifted inputs).\n    \"\"\"\n    ix = jax.random.randint(\n      random_key, shape=(batch_size, 1), minval=0, maxval=len(data) - block_size\n    )\n    x = dynamic_slice_vmap(data, ix, (block_size,))\n    y = dynamic_slice_vmap(data, ix + n_tokens, (block_size,))\n    return x, y\n\nxb, yb = get_batch(rng_key, train_data)\ntrain_shape = xb.shape\nprint('inputs:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\n# print('----')\n\n# for b in range(batch_size): # batch dimension\n#     for t in range(block_size): # time dimension\n#         context = xb[b, :t+1]\n#         target = yb[b,t]\n#         print(f\"when input is {context} the target: {target}\")","metadata":{"id":"UuAjtqPeHFyk","outputId":"6a88fb2b-b798-4ee9-9f4f-f38ce898d576","execution":{"iopub.status.busy":"2024-06-13T10:24:55.091109Z","iopub.execute_input":"2024-06-13T10:24:55.091466Z","iopub.status.idle":"2024-06-13T10:24:55.453346Z","shell.execute_reply.started":"2024-06-13T10:24:55.091434Z","shell.execute_reply":"2024-06-13T10:24:55.452430Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"inputs:\n(128, 64)\n[[31 16 30 ... 20 25 14]\n [44 43 58 ...  1 38 44]\n [42 56  6 ... 52 55 56]\n ...\n [ 6  1 39 ... 25 52 57]\n [46 41 46 ... 56 60 52]\n [57 52  1 ... 49 52 55]]\ntargets:\n(128, 64)\n[[16 30 30 ... 25 14 16]\n [43 58 49 ... 38 44 38]\n [56  6  1 ... 55 56 42]\n ...\n [ 1 39 42 ... 52 57 45]\n [41 46 58 ... 60 52 55]\n [52  1 50 ... 52 55 41]]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(xb[0])\nprint(yb[0])","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:24:55.454579Z","iopub.execute_input":"2024-06-13T10:24:55.454939Z","iopub.status.idle":"2024-06-13T10:24:55.556345Z","shell.execute_reply.started":"2024-06-13T10:24:55.454901Z","shell.execute_reply":"2024-06-13T10:24:55.555203Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"[31 16 30 30  9  0 34 45 52  1 48 51 52 40 48 56  1 56 52  1 49 52 58 41\n  1 38 57  1 41 52 52 55 11  1 23 52 52 48  1 57 52  1 57 45  5  1 41 52\n 52 55  1 57 45 42 55 42  6  0  0 27 29 20 25 14]\n[16 30 30  9  0 34 45 52  1 48 51 52 40 48 56  1 56 52  1 49 52 58 41  1\n 38 57  1 41 52 52 55 11  1 23 52 52 48  1 57 52  1 57 45  5  1 41 52 52\n 55  1 57 45 42 55 42  6  0  0 27 29 20 25 14 16]\n","output_type":"stream"}]},{"cell_type":"code","source":"# hidden_state = [jnp.zeros((1,n_latent_dim, n_embd * expans)) for _ in range(n_layers)]\n# hidden_state[0].shape","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:24:55.557542Z","iopub.execute_input":"2024-06-13T10:24:55.557829Z","iopub.status.idle":"2024-06-13T10:24:55.561853Z","shell.execute_reply.started":"2024-06-13T10:24:55.557803Z","shell.execute_reply":"2024-06-13T10:24:55.560904Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# Mamba Block\nDense --> Conv1D --> Silu --> SSM --> Silu -->","metadata":{"id":"yOccqzJlHFym"}},{"cell_type":"code","source":"class Mamba(nn.Module):\n\n    def setup(self):\n        emb_features = n_embd * expans\n        self.in_proj1 = nn.Conv(features=n_embd, kernel_size=conv_k_size,padding=1) #nn.Dense(features=emb_features)\n        self.in_proj2 = nn.Dense(features=emb_features)\n\n        # Adjusted for Flax. Flax does not have nn.Conv1d, so you might need to reshape or use a different approach\n        self.conv1d = nn.Conv(features=emb_features,\n                              kernel_size=conv_k_size,\n                              padding=1,\n                              )\n\n        self.A = -1*self.param('A', nn.initializers.ones, (1, n_latent_dim, emb_features, 1))\n        self.B = 0.1*self.param('B', nn.initializers.ones, (1, n_latent_dim, 1, block_size))\n        self.C = 0.09*self.param('C', jax.random.normal, (1, n_latent_dim, 1, block_size))\n        self.D = 0.1*self.param('D', jax.random.normal, (1, 1,emb_features, block_size))\n        self.delta = 0.05*self.param('delta', jax.random.normal, (1, 1,emb_features, block_size))\n\n        self.out_proj = nn.Dense(n_embd // n_heads)\n        \n        self.hidden_state = self.variable('other_variables','hidden_state', \n                                          jnp.zeros, \n                                          (1,n_latent_dim, emb_features))\n#         self.rms_norm = nn.RMSNorm()\n\n    def __call__(self, embeds):\n        x = self.in_proj1(embeds)\n        x = jax.nn.silu(x) #new\n        x = self.conv1d(x)\n        x = jax.nn.silu(x)\n        x = x.reshape((x.shape[0],1,x.shape[2],x.shape[1]))\n        x = self.ssm(x)\n        x = x.reshape((x.shape[0],x.shape[3],x.shape[2]))\n        x = x*jax.nn.silu(self.in_proj2(embeds))\n\n        x = self.out_proj(x)\n\n#         x = self.rms_norm(x)\n\n        return x\n    def discretize(self):\n        da = self.delta * self.A\n        a_ = jnp.exp(da)\n        b_ = self.B * self.delta\n        return a_, b_\n\n    def ssm(self, x):\n        y = []\n        a_, b_ = self.discretize()\n        h = 0\n        for k in range(x.shape[-1]):\n            h = a_[..., k] * h + b_[..., k] * x[..., k]\n            \n#         for l in range(x.shape[-1]):\n#             print(self.C.shape, h.shape)\n\n        y = ((self.C * jax.lax.expand_dims(h,[3])).sum(1, keepdims=True) + self.D*x)\n        \n#         self.hidden_state.value = jax.nn.standardize(h.mean(0, keepdims=True))\n        return y","metadata":{"id":"4qOdblU5HFyo","execution":{"iopub.status.busy":"2024-06-13T10:24:55.562959Z","iopub.execute_input":"2024-06-13T10:24:55.563308Z","iopub.status.idle":"2024-06-13T10:24:55.580007Z","shell.execute_reply.started":"2024-06-13T10:24:55.563259Z","shell.execute_reply":"2024-06-13T10:24:55.579050Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"class MultiHeadMamba(nn.Module):\n    def setup(self):\n        self.heads = [Mamba() for _ in range(n_heads)]\n        self.rms_norm = nn.RMSNorm()\n\n    def __call__(self, x):\n        out = jnp.concatenate([h(x) for h in self.heads], axis=-1)\n        x = self.rms_norm(out)\n        return x","metadata":{"id":"0bH9vlLZHFyq","execution":{"iopub.status.busy":"2024-06-13T10:24:55.581097Z","iopub.execute_input":"2024-06-13T10:24:55.581399Z","iopub.status.idle":"2024-06-13T10:24:55.595111Z","shell.execute_reply.started":"2024-06-13T10:24:55.581376Z","shell.execute_reply":"2024-06-13T10:24:55.594119Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# class FeedForward(nn.Module):\n#     def setup(self):\n#         self.ffn = nn.Sequential([\n#             nn.Dense(4 * n_embd),\n#             nn.relu,\n#             nn.Dense(n_embd)]\n#         )\n#     def __call__(self, x):\n#         return self.ffn(x)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:24:55.596196Z","iopub.execute_input":"2024-06-13T10:24:55.596479Z","iopub.status.idle":"2024-06-13T10:24:55.608236Z","shell.execute_reply.started":"2024-06-13T10:24:55.596455Z","shell.execute_reply":"2024-06-13T10:24:55.607431Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# class MambaBlock(nn.Module):\n#     def setup(self):\n#         self.mamba_block = Mamba()\n#         self.ln1 = nn.RMSNorm()\n#         self.ffn = FeedForward()\n#         self.ln2 = nn.LayerNorm()\n\n#     def __call__(self, x):\n#         x = x + self.mamba_block(self.ln2(x))\n#         x = x + self.ffn(self.ln1(x))\n#         return x\n","metadata":{"id":"UiCxIjoEp2QA","execution":{"iopub.status.busy":"2024-06-13T10:24:55.609380Z","iopub.execute_input":"2024-06-13T10:24:55.609685Z","iopub.status.idle":"2024-06-13T10:24:55.618330Z","shell.execute_reply.started":"2024-06-13T10:24:55.609650Z","shell.execute_reply":"2024-06-13T10:24:55.617552Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# class MambaModel(nn.Module):\n\n#     def setup(self):\n#         self.tok_embeddings = nn.Embed(vocab_size, n_embd)\n#         self.pos_embeddings = nn.Embed(block_size, n_embd)\n#         self.ln = nn.LayerNorm()\n#         self.mamba_layers = [MambaBlock() for _ in range(n_layers)]\n#         self.preds_out = nn.Dense(vocab_size)\n\n#     def __call__(self, x, training: bool):\n#         x = self.tok_embeddings(x) + self.pos_embeddings(jnp.arange(block_size))\n# #         x = self.ln(x)\n#         for layer in self.mamba_layers:\n#             x = layer(x)\n            \n#         return self.preds_out(x)\n\n#     @jax.jit\n#     def generate(self, idx, max_new_tokens, params):\n#     # idx is (B, T) array of indices in the current context\n#         for _ in range(max_new_tokens):\n#             # crop idx to the last block_size tokens\n#             idx_cond = idx[:, -block_size:]\n#             # get the predictions\n#             logits = self.apply(params, idx_cond)\n#             # focus only on the last time step\n#             logits = logits[:, -1, :] # becomes (B, C)\n#             # apply softmax to get probabilities\n#             ##probs = tf.keras.activations.softmax(logits, dim=-1) # (B, C)\n#             # sample from the distribution\n#             idx_next = jax.random.categorical(jax.random.PRNGKey(52), logits) # (B, 1)\n#             # append sampled index to the running sequence\n#             idx = jax.numpy.expand_dims(jnp.concatenate([idx[0], idx_next], axis=0), 0) # (B, T+1)\n#     #         print(idx_next)\n#     #         print(idx)\n\n#         return idx","metadata":{"id":"y4C7OWL8HFyq","execution":{"iopub.status.busy":"2024-06-13T10:24:55.619306Z","iopub.execute_input":"2024-06-13T10:24:55.619622Z","iopub.status.idle":"2024-06-13T10:24:55.629398Z","shell.execute_reply.started":"2024-06-13T10:24:55.619591Z","shell.execute_reply":"2024-06-13T10:24:55.628525Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# model = Mamba()\n# params = model.init(jax.random.key(42), jnp.ones((1,64,256)))\n# # print(params['other_variables']['hidden_state'].shape, params['other_variables']['hidden_state'].min(), params['other_variables']['hidden_state'].max())\n# # print(model.tabulate(jax.random.key(0), jnp.ones((1,64,256)),\n# #                    compute_flops=True, compute_vjp_flops=True))\n# xs = model.apply(params, jnp.ones((1,64,256)), mutable=['other_variables'])\n# # # print(params['other_variables']['hidden_state'].shape, params['other_variables']['hidden_state'].min(), params['other_variables']['hidden_state'].max())\n# xb.shape, xs[0].shape, xs[1].keys()","metadata":{"id":"wTd3jSQWHFyp","execution":{"iopub.status.busy":"2024-06-13T10:24:55.630457Z","iopub.execute_input":"2024-06-13T10:24:55.630737Z","iopub.status.idle":"2024-06-13T10:24:55.643165Z","shell.execute_reply.started":"2024-06-13T10:24:55.630715Z","shell.execute_reply":"2024-06-13T10:24:55.642338Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# print(xs[1]['other_variables']['hidden_state'].shape, xs[1]['other_variables']['hidden_state'].min(), xs[1]['other_variables']['hidden_state'].max())","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:24:55.644194Z","iopub.execute_input":"2024-06-13T10:24:55.644516Z","iopub.status.idle":"2024-06-13T10:24:55.656209Z","shell.execute_reply.started":"2024-06-13T10:24:55.644493Z","shell.execute_reply":"2024-06-13T10:24:55.655479Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# xfs = model.apply(params, 2*jnp.ones((1,64,256)), mutable=['other_variables'])\n# print(params['other_variables']['hidden_state'].shape, params['other_variables']['hidden_state'].min(), params['other_variables']['hidden_state'].max())\n# print(xfs[1]['other_variables']['hidden_state'].shape, xfs[1]['other_variables']['hidden_state'].min(), xfs[1]['other_variables']['hidden_state'].max())","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:24:55.657132Z","iopub.execute_input":"2024-06-13T10:24:55.657392Z","iopub.status.idle":"2024-06-13T10:24:55.667063Z","shell.execute_reply.started":"2024-06-13T10:24:55.657370Z","shell.execute_reply":"2024-06-13T10:24:55.666316Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# test_model = Mamba()\n# test_params = test_model.init(jax.random.key(42), xb)\n# n_params = sum(p.size for p in jax.tree_util.tree_leaves(test_params))\n# print(f\"Total number of parameters: {n_params:_}\")\n# # print(fin_model.tabulate(jax.random.key(42), xb,\n# #                    compute_flops=True, compute_vjp_flops=True))\n# xf = test_model.apply(test_params, xb)\n# xb.shape, xf.shape","metadata":{"id":"cm2a0nepHFyq","execution":{"iopub.status.busy":"2024-06-13T10:24:55.667928Z","iopub.execute_input":"2024-06-13T10:24:55.668150Z","iopub.status.idle":"2024-06-13T10:24:55.677725Z","shell.execute_reply.started":"2024-06-13T10:24:55.668129Z","shell.execute_reply":"2024-06-13T10:24:55.676936Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"class NanoLM(nn.Module):\n    \"\"\"NanoLM model.\"\"\"\n    vocab_size: int = 65\n    num_layers: int = 6\n    num_heads: int = 8\n    head_size: int = 32\n    dropout_rate: float = 0.2\n    embed_size: int = 256\n    block_size: int = 64\n\n    @nn.compact\n    def __call__(self, x, training: bool):\n        x = nn.Embed(self.vocab_size, self.embed_size)(x) + nn.Embed(\n            self.block_size, self.embed_size\n        )(jnp.arange(self.block_size))\n        \n        for i in range(self.num_layers):\n#             x = x + nn.MultiHeadDotProductAttention(\n#               num_heads=self.num_heads,\n#               qkv_features=self.head_size,\n#               out_features=self.head_size * self.num_heads,\n#               dropout_rate=self.dropout_rate,\n#             )(\n#               x_norm,\n#               x_norm,\n#               mask=jnp.tril(jnp.ones((x.shape[-2], x.shape[-2]))),\n#               deterministic=not training,\n#             )\n    \n            x = x + Mamba()(nn.RMSNorm()(x))\n\n#             x = x + nn.Sequential([\n#               nn.Dense(4 * self.embed_size),\n#               nn.relu,\n#               nn.Dropout(self.dropout_rate, deterministic=not training),\n#               nn.Dense(self.embed_size),\n#             ])(nn.RMSNorm()(x))\n\n        x = nn.Dense(self.vocab_size)(nn.RMSNorm()(x))\n        return x","metadata":{"id":"zuiaFP6WHFyr","execution":{"iopub.status.busy":"2024-06-13T10:24:55.678731Z","iopub.execute_input":"2024-06-13T10:24:55.678983Z","iopub.status.idle":"2024-06-13T10:24:55.690563Z","shell.execute_reply.started":"2024-06-13T10:24:55.678961Z","shell.execute_reply":"2024-06-13T10:24:55.689705Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# key = jax.random.key(42)\n\n# # fin_model = MambaModel()\n# # fin_params = fin_model.init(key, xb, training=False)\n\n\n# fin_model = NanoLM(\n#     vocab_size=vocab_size,\n#     num_layers=n_layers,\n#     num_heads=8,\n#     head_size=32,\n#     dropout_rate=0.2,\n#     embed_size=n_embd,\n#     block_size=block_size,\n# )\n\n# fin_params = fin_model.init(\n#     {'params': key},\n#     jnp.ones((batch_size, block_size), dtype=jnp.int32),\n#     training=False\n# )\n\n# n_params = sum(p.size for p in jax.tree_util.tree_leaves(fin_params))\n# print(f\"Total number of parameters: {n_params:_}\")\n# # print(fin_model.tabulate(jax.random.key(42), xb,\n# #                    compute_flops=True, compute_vjp_flops=True))\n# xf = fin_model.apply(fin_params, xb, training=False)[0]\n# xb.shape, xf.shape","metadata":{"id":"fnUQPyuvHFys","outputId":"f04ebf31-d67f-4488-dd5d-7fd5b20dd1ea","execution":{"iopub.status.busy":"2024-06-13T10:24:55.698233Z","iopub.execute_input":"2024-06-13T10:24:55.698516Z","iopub.status.idle":"2024-06-13T10:24:55.704566Z","shell.execute_reply.started":"2024-06-13T10:24:55.698493Z","shell.execute_reply":"2024-06-13T10:24:55.703831Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"def loss_fun(params, x, y, var_params,dropout_key):\n    logits, updated_variables = model.apply({'params': params, **var_params}, x, training=True, rngs={\"dropout\": dropout_key}, mutable=['other_variables'])\n    accuracy = jnp.mean(jnp.argmax(logits, axis=-1) == y)\n    return optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=y).mean(), (updated_variables, accuracy)\n\n@jax.jit\ndef eval_step(params, x, y, var_params):\n    logits, _ = model.apply({'params': params, **var_params}, x, training=False, mutable=['other_variables'])\n    accuracy = jnp.mean(jnp.argmax(logits, axis=-1) == y)\n    return optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=y).mean(), accuracy","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:24:55.705698Z","iopub.execute_input":"2024-06-13T10:24:55.705984Z","iopub.status.idle":"2024-06-13T10:24:55.718511Z","shell.execute_reply.started":"2024-06-13T10:24:55.705960Z","shell.execute_reply":"2024-06-13T10:24:55.717728Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"key = jax.random.PRNGKey(42)\nkey, subkey = jax.random.split(key)\n\nmodel = NanoLM(\n    vocab_size=vocab_size,\n    num_layers=n_layers,\n    num_heads=8,\n    head_size=32,\n    dropout_rate=0.2,\n    embed_size=n_embd,\n    block_size=block_size,\n)\n\nvar_params = model.init(\n    key,\n    jnp.ones((batch_size, block_size), dtype=jnp.int32),\n    training=False,\n)\nprint(var_params.keys())\nn_params = sum(p.size for p in jax.tree_util.tree_leaves(var_params))\n\nprint(f\"Total number of parameters: {n_params:_}\")","metadata":{"id":"PKpb3864HFyt","execution":{"iopub.status.busy":"2024-06-13T10:24:55.719555Z","iopub.execute_input":"2024-06-13T10:24:55.719828Z","iopub.status.idle":"2024-06-13T10:25:04.210793Z","shell.execute_reply.started":"2024-06-13T10:24:55.719805Z","shell.execute_reply":"2024-06-13T10:25:04.209556Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"dict_keys(['params', 'other_variables'])\nTotal number of parameters: 5_675_840\n","output_type":"stream"}]},{"cell_type":"code","source":"var_params['params']['Embed_0']['embedding'].shape","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:25:04.211995Z","iopub.execute_input":"2024-06-13T10:25:04.212342Z","iopub.status.idle":"2024-06-13T10:25:04.218992Z","shell.execute_reply.started":"2024-06-13T10:25:04.212293Z","shell.execute_reply":"2024-06-13T10:25:04.218024Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"(64, 256)"},"metadata":{}}]},{"cell_type":"code","source":"params = var_params.pop('params')","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:25:04.220136Z","iopub.execute_input":"2024-06-13T10:25:04.220847Z","iopub.status.idle":"2024-06-13T10:25:04.234527Z","shell.execute_reply.started":"2024-06-13T10:25:04.220813Z","shell.execute_reply":"2024-06-13T10:25:04.233661Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"var_params = jax.tree_map(lambda x: jnp.zeros_like(x), var_params)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:25:04.235653Z","iopub.execute_input":"2024-06-13T10:25:04.236089Z","iopub.status.idle":"2024-06-13T10:25:04.249988Z","shell.execute_reply.started":"2024-06-13T10:25:04.236057Z","shell.execute_reply":"2024-06-13T10:25:04.249289Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# decay_rate = 0.96\n# learning_rate_schedule = optax.exponential_decay(learning_rate, decay_rate, max_iters//1000)\nopt = optax.adamw(learning_rate=learning_rate)\n\nopt_state = opt.init(params)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:25:04.251051Z","iopub.execute_input":"2024-06-13T10:25:04.251474Z","iopub.status.idle":"2024-06-13T10:25:04.571126Z","shell.execute_reply.started":"2024-06-13T10:25:04.251439Z","shell.execute_reply":"2024-06-13T10:25:04.570105Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"%%time\n\nall_train_losses = []\nall_eval_losses = []\n\nall_train_accuracy =  []\nall_test_accuracy = []\n\n# we define one iteration of the optimizer and JIT this function\n@jax.jit\ndef step(key, params, var_params, opt_state):\n    key, subkey = jax.random.split(key)\n    xb, yb = get_batch(key, train_data)\n    (loss, aux_data), grad = jax.value_and_grad(loss_fun, has_aux=True)(params, xb, yb, var_params, subkey)\n    var_params, train_accuracy = aux_data\n    updates, opt_state = opt.update(grad, opt_state, params)\n    params = optax.apply_updates(params, updates)\n    return params, key, opt_state, loss, var_params, train_accuracy\n\n# for i in tqdm(range(max_iters)):\ncounter = 0\nloss = 10\nwhile counter<max_iters: # and loss > 1.0:\n\n    params, key, opt_state, loss, var_params, train_accuracy = step(key, params, var_params, opt_state)\n    \n\n    # once every N_FREQ_EVAL we compute loss on the validation set\n    if counter % eval_iters == 0:\n        key, subkey = jax.random.split(key)\n        eval_loss, eval_accuracy = eval_step(params, *get_batch(subkey, test_data), var_params)\n        all_train_losses.append(loss)\n        all_eval_losses.append(eval_loss)\n        all_train_accuracy.append(train_accuracy)\n        all_test_accuracy.append(eval_accuracy)\n        print('##########################################################')\n        print(\"Step: \", counter,\"\\t Train Loss: \", loss,\"\\t Train Accuracy: \", format(train_accuracy, \".2%\"))\n        print(\"Step: \", counter,\"\\t Eval Loss: \", eval_loss,\"\\t Eval Accuracy: \", format(eval_accuracy, \".2%\"))\n        \n    counter += 1\n        ","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:25:04.572327Z","iopub.execute_input":"2024-06-13T10:25:04.572636Z","iopub.status.idle":"2024-06-13T10:53:16.872527Z","shell.execute_reply.started":"2024-06-13T10:25:04.572611Z","shell.execute_reply":"2024-06-13T10:53:16.871563Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"##########################################################\nStep:  0 \t Train Loss:  4.6607275 \t Train Accuracy:  1.32%\nStep:  0 \t Eval Loss:  4.315344 \t Eval Accuracy:  3.33%\n##########################################################\nStep:  100 \t Train Loss:  0.061989706 \t Train Accuracy:  98.79%\nStep:  100 \t Eval Loss:  0.065420985 \t Eval Accuracy:  98.66%\n##########################################################\nStep:  200 \t Train Loss:  0.036058284 \t Train Accuracy:  99.11%\nStep:  200 \t Eval Loss:  0.04527366 \t Eval Accuracy:  98.83%\n##########################################################\nStep:  300 \t Train Loss:  0.03766068 \t Train Accuracy:  99.05%\nStep:  300 \t Eval Loss:  0.039712146 \t Eval Accuracy:  98.89%\n##########################################################\nStep:  400 \t Train Loss:  0.035780817 \t Train Accuracy:  99.01%\nStep:  400 \t Eval Loss:  0.03439277 \t Eval Accuracy:  99.06%\n##########################################################\nStep:  500 \t Train Loss:  0.029342031 \t Train Accuracy:  99.11%\nStep:  500 \t Eval Loss:  0.03208597 \t Eval Accuracy:  99.08%\n##########################################################\nStep:  600 \t Train Loss:  0.033347968 \t Train Accuracy:  99.01%\nStep:  600 \t Eval Loss:  0.03665484 \t Eval Accuracy:  99.00%\n##########################################################\nStep:  700 \t Train Loss:  0.031053254 \t Train Accuracy:  99.08%\nStep:  700 \t Eval Loss:  0.03517333 \t Eval Accuracy:  99.01%\n##########################################################\nStep:  800 \t Train Loss:  0.030365987 \t Train Accuracy:  99.10%\nStep:  800 \t Eval Loss:  0.032465573 \t Eval Accuracy:  99.08%\n##########################################################\nStep:  900 \t Train Loss:  0.029143404 \t Train Accuracy:  99.11%\nStep:  900 \t Eval Loss:  0.02885731 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  1000 \t Train Loss:  0.028603915 \t Train Accuracy:  99.13%\nStep:  1000 \t Eval Loss:  0.032824006 \t Eval Accuracy:  99.06%\n##########################################################\nStep:  1100 \t Train Loss:  0.031387858 \t Train Accuracy:  99.12%\nStep:  1100 \t Eval Loss:  0.029771212 \t Eval Accuracy:  99.12%\n##########################################################\nStep:  1200 \t Train Loss:  0.027414445 \t Train Accuracy:  99.17%\nStep:  1200 \t Eval Loss:  0.030093621 \t Eval Accuracy:  99.12%\n##########################################################\nStep:  1300 \t Train Loss:  0.027021065 \t Train Accuracy:  99.21%\nStep:  1300 \t Eval Loss:  0.029997578 \t Eval Accuracy:  99.12%\n##########################################################\nStep:  1400 \t Train Loss:  0.024882399 \t Train Accuracy:  99.21%\nStep:  1400 \t Eval Loss:  0.028301 \t Eval Accuracy:  99.13%\n##########################################################\nStep:  1500 \t Train Loss:  0.027000092 \t Train Accuracy:  99.27%\nStep:  1500 \t Eval Loss:  0.02765508 \t Eval Accuracy:  99.16%\n##########################################################\nStep:  1600 \t Train Loss:  0.024479665 \t Train Accuracy:  99.32%\nStep:  1600 \t Eval Loss:  0.026732946 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  1700 \t Train Loss:  0.028372906 \t Train Accuracy:  99.16%\nStep:  1700 \t Eval Loss:  0.027917746 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  1800 \t Train Loss:  0.029119872 \t Train Accuracy:  99.17%\nStep:  1800 \t Eval Loss:  0.027992684 \t Eval Accuracy:  99.15%\n##########################################################\nStep:  1900 \t Train Loss:  0.030155752 \t Train Accuracy:  99.05%\nStep:  1900 \t Eval Loss:  0.03268655 \t Eval Accuracy:  99.05%\n##########################################################\nStep:  2000 \t Train Loss:  0.029435547 \t Train Accuracy:  99.13%\nStep:  2000 \t Eval Loss:  0.030167276 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  2100 \t Train Loss:  0.028013349 \t Train Accuracy:  99.28%\nStep:  2100 \t Eval Loss:  0.031338457 \t Eval Accuracy:  99.13%\n##########################################################\nStep:  2200 \t Train Loss:  0.025155166 \t Train Accuracy:  99.38%\nStep:  2200 \t Eval Loss:  0.027249316 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  2300 \t Train Loss:  0.021630403 \t Train Accuracy:  99.37%\nStep:  2300 \t Eval Loss:  0.03314671 \t Eval Accuracy:  99.07%\n##########################################################\nStep:  2400 \t Train Loss:  0.025953263 \t Train Accuracy:  99.23%\nStep:  2400 \t Eval Loss:  0.03291369 \t Eval Accuracy:  99.04%\n##########################################################\nStep:  2500 \t Train Loss:  0.024614908 \t Train Accuracy:  99.24%\nStep:  2500 \t Eval Loss:  0.027367901 \t Eval Accuracy:  99.12%\n##########################################################\nStep:  2600 \t Train Loss:  0.02802193 \t Train Accuracy:  99.17%\nStep:  2600 \t Eval Loss:  0.03140852 \t Eval Accuracy:  99.11%\n##########################################################\nStep:  2700 \t Train Loss:  0.022119775 \t Train Accuracy:  99.39%\nStep:  2700 \t Eval Loss:  0.02653493 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  2800 \t Train Loss:  0.02610587 \t Train Accuracy:  99.28%\nStep:  2800 \t Eval Loss:  0.030460805 \t Eval Accuracy:  99.08%\n##########################################################\nStep:  2900 \t Train Loss:  0.023537222 \t Train Accuracy:  99.32%\nStep:  2900 \t Eval Loss:  0.032988943 \t Eval Accuracy:  99.11%\n##########################################################\nStep:  3000 \t Train Loss:  0.023635015 \t Train Accuracy:  99.33%\nStep:  3000 \t Eval Loss:  0.03140361 \t Eval Accuracy:  99.16%\n##########################################################\nStep:  3100 \t Train Loss:  0.025707316 \t Train Accuracy:  99.27%\nStep:  3100 \t Eval Loss:  0.029837364 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  3200 \t Train Loss:  0.029426048 \t Train Accuracy:  99.13%\nStep:  3200 \t Eval Loss:  0.029361166 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  3300 \t Train Loss:  0.026462018 \t Train Accuracy:  99.22%\nStep:  3300 \t Eval Loss:  0.030639317 \t Eval Accuracy:  99.08%\n##########################################################\nStep:  3400 \t Train Loss:  0.024741096 \t Train Accuracy:  99.22%\nStep:  3400 \t Eval Loss:  0.025209375 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  3500 \t Train Loss:  0.024479803 \t Train Accuracy:  99.27%\nStep:  3500 \t Eval Loss:  0.025085013 \t Eval Accuracy:  99.30%\n##########################################################\nStep:  3600 \t Train Loss:  0.026015949 \t Train Accuracy:  99.24%\nStep:  3600 \t Eval Loss:  0.021996615 \t Eval Accuracy:  99.34%\n##########################################################\nStep:  3700 \t Train Loss:  0.02566908 \t Train Accuracy:  99.17%\nStep:  3700 \t Eval Loss:  0.028283898 \t Eval Accuracy:  99.13%\n##########################################################\nStep:  3800 \t Train Loss:  0.028076971 \t Train Accuracy:  99.13%\nStep:  3800 \t Eval Loss:  0.029901464 \t Eval Accuracy:  99.11%\n##########################################################\nStep:  3900 \t Train Loss:  0.02638743 \t Train Accuracy:  99.21%\nStep:  3900 \t Eval Loss:  0.027260616 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  4000 \t Train Loss:  0.023647102 \t Train Accuracy:  99.29%\nStep:  4000 \t Eval Loss:  0.026895 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  4100 \t Train Loss:  0.025721926 \t Train Accuracy:  99.22%\nStep:  4100 \t Eval Loss:  0.03116151 \t Eval Accuracy:  99.16%\n##########################################################\nStep:  4200 \t Train Loss:  0.021575255 \t Train Accuracy:  99.28%\nStep:  4200 \t Eval Loss:  0.026418379 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  4300 \t Train Loss:  0.026477508 \t Train Accuracy:  99.18%\nStep:  4300 \t Eval Loss:  0.032036945 \t Eval Accuracy:  99.05%\n##########################################################\nStep:  4400 \t Train Loss:  0.027007245 \t Train Accuracy:  99.21%\nStep:  4400 \t Eval Loss:  0.029128151 \t Eval Accuracy:  99.17%\n##########################################################\nStep:  4500 \t Train Loss:  0.02540689 \t Train Accuracy:  99.34%\nStep:  4500 \t Eval Loss:  0.028259037 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  4600 \t Train Loss:  0.024825258 \t Train Accuracy:  99.22%\nStep:  4600 \t Eval Loss:  0.030384032 \t Eval Accuracy:  99.16%\n##########################################################\nStep:  4700 \t Train Loss:  0.025352834 \t Train Accuracy:  99.22%\nStep:  4700 \t Eval Loss:  0.024069948 \t Eval Accuracy:  99.32%\n##########################################################\nStep:  4800 \t Train Loss:  0.022335958 \t Train Accuracy:  99.37%\nStep:  4800 \t Eval Loss:  0.02589181 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  4900 \t Train Loss:  0.026428558 \t Train Accuracy:  99.18%\nStep:  4900 \t Eval Loss:  0.030878317 \t Eval Accuracy:  99.16%\n##########################################################\nStep:  5000 \t Train Loss:  0.026112843 \t Train Accuracy:  99.15%\nStep:  5000 \t Eval Loss:  0.02810271 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  5100 \t Train Loss:  0.023425175 \t Train Accuracy:  99.28%\nStep:  5100 \t Eval Loss:  0.030951092 \t Eval Accuracy:  99.12%\n##########################################################\nStep:  5200 \t Train Loss:  0.02239976 \t Train Accuracy:  99.37%\nStep:  5200 \t Eval Loss:  0.032807685 \t Eval Accuracy:  99.08%\n##########################################################\nStep:  5300 \t Train Loss:  0.02433275 \t Train Accuracy:  99.29%\nStep:  5300 \t Eval Loss:  0.028893415 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  5400 \t Train Loss:  0.025651164 \t Train Accuracy:  99.26%\nStep:  5400 \t Eval Loss:  0.02263199 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  5500 \t Train Loss:  0.026993906 \t Train Accuracy:  99.26%\nStep:  5500 \t Eval Loss:  0.029490806 \t Eval Accuracy:  99.15%\n##########################################################\nStep:  5600 \t Train Loss:  0.027449396 \t Train Accuracy:  99.15%\nStep:  5600 \t Eval Loss:  0.02570527 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  5700 \t Train Loss:  0.024653891 \t Train Accuracy:  99.23%\nStep:  5700 \t Eval Loss:  0.028058313 \t Eval Accuracy:  99.17%\n##########################################################\nStep:  5800 \t Train Loss:  0.020297278 \t Train Accuracy:  99.38%\nStep:  5800 \t Eval Loss:  0.025708286 \t Eval Accuracy:  99.17%\n##########################################################\nStep:  5900 \t Train Loss:  0.0254293 \t Train Accuracy:  99.21%\nStep:  5900 \t Eval Loss:  0.024727786 \t Eval Accuracy:  99.34%\n##########################################################\nStep:  6000 \t Train Loss:  0.024028009 \t Train Accuracy:  99.30%\nStep:  6000 \t Eval Loss:  0.026051834 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  6100 \t Train Loss:  0.024853334 \t Train Accuracy:  99.24%\nStep:  6100 \t Eval Loss:  0.02833447 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  6200 \t Train Loss:  0.02326107 \t Train Accuracy:  99.34%\nStep:  6200 \t Eval Loss:  0.028534533 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  6300 \t Train Loss:  0.024471425 \t Train Accuracy:  99.23%\nStep:  6300 \t Eval Loss:  0.026289258 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  6400 \t Train Loss:  0.022834096 \t Train Accuracy:  99.35%\nStep:  6400 \t Eval Loss:  0.02381662 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  6500 \t Train Loss:  0.024384515 \t Train Accuracy:  99.23%\nStep:  6500 \t Eval Loss:  0.025660269 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  6600 \t Train Loss:  0.02259764 \t Train Accuracy:  99.34%\nStep:  6600 \t Eval Loss:  0.027942918 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  6700 \t Train Loss:  0.022983614 \t Train Accuracy:  99.24%\nStep:  6700 \t Eval Loss:  0.03182242 \t Eval Accuracy:  99.08%\n##########################################################\nStep:  6800 \t Train Loss:  0.021981189 \t Train Accuracy:  99.32%\nStep:  6800 \t Eval Loss:  0.026309978 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  6900 \t Train Loss:  0.022696216 \t Train Accuracy:  99.30%\nStep:  6900 \t Eval Loss:  0.024415303 \t Eval Accuracy:  99.32%\n##########################################################\nStep:  7000 \t Train Loss:  0.027989285 \t Train Accuracy:  99.16%\nStep:  7000 \t Eval Loss:  0.028389705 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  7100 \t Train Loss:  0.027073551 \t Train Accuracy:  99.16%\nStep:  7100 \t Eval Loss:  0.029833257 \t Eval Accuracy:  99.16%\n##########################################################\nStep:  7200 \t Train Loss:  0.022884503 \t Train Accuracy:  99.30%\nStep:  7200 \t Eval Loss:  0.02725355 \t Eval Accuracy:  99.16%\n##########################################################\nStep:  7300 \t Train Loss:  0.021195387 \t Train Accuracy:  99.34%\nStep:  7300 \t Eval Loss:  0.033498336 \t Eval Accuracy:  99.11%\n##########################################################\nStep:  7400 \t Train Loss:  0.020616421 \t Train Accuracy:  99.40%\nStep:  7400 \t Eval Loss:  0.028081847 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  7500 \t Train Loss:  0.027127288 \t Train Accuracy:  99.16%\nStep:  7500 \t Eval Loss:  0.029472796 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  7600 \t Train Loss:  0.024932634 \t Train Accuracy:  99.19%\nStep:  7600 \t Eval Loss:  0.027891483 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  7700 \t Train Loss:  0.025960729 \t Train Accuracy:  99.26%\nStep:  7700 \t Eval Loss:  0.030170968 \t Eval Accuracy:  99.15%\n##########################################################\nStep:  7800 \t Train Loss:  0.022855043 \t Train Accuracy:  99.34%\nStep:  7800 \t Eval Loss:  0.028439853 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  7900 \t Train Loss:  0.021957237 \t Train Accuracy:  99.33%\nStep:  7900 \t Eval Loss:  0.030984435 \t Eval Accuracy:  99.11%\n##########################################################\nStep:  8000 \t Train Loss:  0.023829328 \t Train Accuracy:  99.28%\nStep:  8000 \t Eval Loss:  0.027292235 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  8100 \t Train Loss:  0.02933256 \t Train Accuracy:  99.19%\nStep:  8100 \t Eval Loss:  0.024971534 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  8200 \t Train Loss:  0.025651757 \t Train Accuracy:  99.22%\nStep:  8200 \t Eval Loss:  0.03092537 \t Eval Accuracy:  99.08%\n##########################################################\nStep:  8300 \t Train Loss:  0.025724497 \t Train Accuracy:  99.19%\nStep:  8300 \t Eval Loss:  0.02469952 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  8400 \t Train Loss:  0.025984928 \t Train Accuracy:  99.17%\nStep:  8400 \t Eval Loss:  0.024443213 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  8500 \t Train Loss:  0.01957435 \t Train Accuracy:  99.39%\nStep:  8500 \t Eval Loss:  0.026801016 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  8600 \t Train Loss:  0.02214381 \t Train Accuracy:  99.33%\nStep:  8600 \t Eval Loss:  0.026189122 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  8700 \t Train Loss:  0.023242027 \t Train Accuracy:  99.26%\nStep:  8700 \t Eval Loss:  0.023516562 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  8800 \t Train Loss:  0.023020638 \t Train Accuracy:  99.33%\nStep:  8800 \t Eval Loss:  0.02729753 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  8900 \t Train Loss:  0.022603668 \t Train Accuracy:  99.24%\nStep:  8900 \t Eval Loss:  0.024809455 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  9000 \t Train Loss:  0.020824067 \t Train Accuracy:  99.45%\nStep:  9000 \t Eval Loss:  0.022867683 \t Eval Accuracy:  99.32%\n##########################################################\nStep:  9100 \t Train Loss:  0.022902872 \t Train Accuracy:  99.18%\nStep:  9100 \t Eval Loss:  0.026918482 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  9200 \t Train Loss:  0.026778938 \t Train Accuracy:  99.19%\nStep:  9200 \t Eval Loss:  0.028935779 \t Eval Accuracy:  99.15%\n##########################################################\nStep:  9300 \t Train Loss:  0.021535179 \t Train Accuracy:  99.37%\nStep:  9300 \t Eval Loss:  0.02100937 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  9400 \t Train Loss:  0.026610173 \t Train Accuracy:  99.21%\nStep:  9400 \t Eval Loss:  0.025170982 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  9500 \t Train Loss:  0.02602433 \t Train Accuracy:  99.16%\nStep:  9500 \t Eval Loss:  0.026643608 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  9600 \t Train Loss:  0.02425516 \t Train Accuracy:  99.27%\nStep:  9600 \t Eval Loss:  0.029828917 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  9700 \t Train Loss:  0.02480165 \t Train Accuracy:  99.22%\nStep:  9700 \t Eval Loss:  0.021766327 \t Eval Accuracy:  99.37%\n##########################################################\nStep:  9800 \t Train Loss:  0.022380099 \t Train Accuracy:  99.27%\nStep:  9800 \t Eval Loss:  0.026792042 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  9900 \t Train Loss:  0.024488388 \t Train Accuracy:  99.26%\nStep:  9900 \t Eval Loss:  0.024539493 \t Eval Accuracy:  99.35%\n##########################################################\nStep:  10000 \t Train Loss:  0.02103481 \t Train Accuracy:  99.33%\nStep:  10000 \t Eval Loss:  0.02919975 \t Eval Accuracy:  99.13%\n##########################################################\nStep:  10100 \t Train Loss:  0.017520908 \t Train Accuracy:  99.49%\nStep:  10100 \t Eval Loss:  0.022774208 \t Eval Accuracy:  99.35%\n##########################################################\nStep:  10200 \t Train Loss:  0.025809381 \t Train Accuracy:  99.21%\nStep:  10200 \t Eval Loss:  0.029451698 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  10300 \t Train Loss:  0.020852465 \t Train Accuracy:  99.38%\nStep:  10300 \t Eval Loss:  0.02762524 \t Eval Accuracy:  99.11%\n##########################################################\nStep:  10400 \t Train Loss:  0.022977192 \t Train Accuracy:  99.34%\nStep:  10400 \t Eval Loss:  0.030071981 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  10500 \t Train Loss:  0.02209254 \t Train Accuracy:  99.34%\nStep:  10500 \t Eval Loss:  0.025865525 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  10600 \t Train Loss:  0.022305291 \t Train Accuracy:  99.35%\nStep:  10600 \t Eval Loss:  0.027069092 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  10700 \t Train Loss:  0.024923023 \t Train Accuracy:  99.27%\nStep:  10700 \t Eval Loss:  0.029867072 \t Eval Accuracy:  99.07%\n##########################################################\nStep:  10800 \t Train Loss:  0.024148926 \t Train Accuracy:  99.26%\nStep:  10800 \t Eval Loss:  0.02577407 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  10900 \t Train Loss:  0.024095355 \t Train Accuracy:  99.26%\nStep:  10900 \t Eval Loss:  0.027146567 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  11000 \t Train Loss:  0.021501403 \t Train Accuracy:  99.39%\nStep:  11000 \t Eval Loss:  0.031055637 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  11100 \t Train Loss:  0.023318695 \t Train Accuracy:  99.24%\nStep:  11100 \t Eval Loss:  0.02968867 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  11200 \t Train Loss:  0.021747176 \t Train Accuracy:  99.29%\nStep:  11200 \t Eval Loss:  0.028672883 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  11300 \t Train Loss:  0.02499552 \t Train Accuracy:  99.24%\nStep:  11300 \t Eval Loss:  0.023663279 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  11400 \t Train Loss:  0.021574128 \t Train Accuracy:  99.24%\nStep:  11400 \t Eval Loss:  0.02287564 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  11500 \t Train Loss:  0.026207745 \t Train Accuracy:  99.24%\nStep:  11500 \t Eval Loss:  0.025664115 \t Eval Accuracy:  99.17%\n##########################################################\nStep:  11600 \t Train Loss:  0.025869 \t Train Accuracy:  99.22%\nStep:  11600 \t Eval Loss:  0.02351905 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  11700 \t Train Loss:  0.023595262 \t Train Accuracy:  99.28%\nStep:  11700 \t Eval Loss:  0.023617063 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  11800 \t Train Loss:  0.02294244 \t Train Accuracy:  99.27%\nStep:  11800 \t Eval Loss:  0.024425285 \t Eval Accuracy:  99.33%\n##########################################################\nStep:  11900 \t Train Loss:  0.025711838 \t Train Accuracy:  99.23%\nStep:  11900 \t Eval Loss:  0.022040782 \t Eval Accuracy:  99.37%\n##########################################################\nStep:  12000 \t Train Loss:  0.020546302 \t Train Accuracy:  99.45%\nStep:  12000 \t Eval Loss:  0.028469415 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  12100 \t Train Loss:  0.02227049 \t Train Accuracy:  99.33%\nStep:  12100 \t Eval Loss:  0.028739873 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  12200 \t Train Loss:  0.019878667 \t Train Accuracy:  99.38%\nStep:  12200 \t Eval Loss:  0.02482062 \t Eval Accuracy:  99.27%\n##########################################################\nStep:  12300 \t Train Loss:  0.023624273 \t Train Accuracy:  99.30%\nStep:  12300 \t Eval Loss:  0.029326119 \t Eval Accuracy:  99.13%\n##########################################################\nStep:  12400 \t Train Loss:  0.02147522 \t Train Accuracy:  99.35%\nStep:  12400 \t Eval Loss:  0.026763584 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  12500 \t Train Loss:  0.022692082 \t Train Accuracy:  99.26%\nStep:  12500 \t Eval Loss:  0.021712221 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  12600 \t Train Loss:  0.02304105 \t Train Accuracy:  99.28%\nStep:  12600 \t Eval Loss:  0.02759483 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  12700 \t Train Loss:  0.02391274 \t Train Accuracy:  99.21%\nStep:  12700 \t Eval Loss:  0.023357142 \t Eval Accuracy:  99.38%\n##########################################################\nStep:  12800 \t Train Loss:  0.02641806 \t Train Accuracy:  99.24%\nStep:  12800 \t Eval Loss:  0.023717774 \t Eval Accuracy:  99.30%\n##########################################################\nStep:  12900 \t Train Loss:  0.024252143 \t Train Accuracy:  99.21%\nStep:  12900 \t Eval Loss:  0.026331909 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  13000 \t Train Loss:  0.020873867 \t Train Accuracy:  99.34%\nStep:  13000 \t Eval Loss:  0.025404748 \t Eval Accuracy:  99.30%\n##########################################################\nStep:  13100 \t Train Loss:  0.02130739 \t Train Accuracy:  99.34%\nStep:  13100 \t Eval Loss:  0.028914656 \t Eval Accuracy:  99.15%\n##########################################################\nStep:  13200 \t Train Loss:  0.020351298 \t Train Accuracy:  99.32%\nStep:  13200 \t Eval Loss:  0.025469106 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  13300 \t Train Loss:  0.018720046 \t Train Accuracy:  99.41%\nStep:  13300 \t Eval Loss:  0.025094088 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  13400 \t Train Loss:  0.0254983 \t Train Accuracy:  99.18%\nStep:  13400 \t Eval Loss:  0.022124883 \t Eval Accuracy:  99.44%\n##########################################################\nStep:  13500 \t Train Loss:  0.021526448 \t Train Accuracy:  99.30%\nStep:  13500 \t Eval Loss:  0.026363082 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  13600 \t Train Loss:  0.024978973 \t Train Accuracy:  99.23%\nStep:  13600 \t Eval Loss:  0.031808544 \t Eval Accuracy:  99.13%\n##########################################################\nStep:  13700 \t Train Loss:  0.024798205 \t Train Accuracy:  99.28%\nStep:  13700 \t Eval Loss:  0.025705943 \t Eval Accuracy:  99.29%\n##########################################################\nStep:  13800 \t Train Loss:  0.020477682 \t Train Accuracy:  99.44%\nStep:  13800 \t Eval Loss:  0.02529208 \t Eval Accuracy:  99.26%\n##########################################################\nStep:  13900 \t Train Loss:  0.018678384 \t Train Accuracy:  99.43%\nStep:  13900 \t Eval Loss:  0.027823448 \t Eval Accuracy:  99.23%\n##########################################################\nStep:  14000 \t Train Loss:  0.021312231 \t Train Accuracy:  99.24%\nStep:  14000 \t Eval Loss:  0.0262045 \t Eval Accuracy:  99.21%\n##########################################################\nStep:  14100 \t Train Loss:  0.019229872 \t Train Accuracy:  99.34%\nStep:  14100 \t Eval Loss:  0.024334218 \t Eval Accuracy:  99.32%\n##########################################################\nStep:  14200 \t Train Loss:  0.021693306 \t Train Accuracy:  99.29%\nStep:  14200 \t Eval Loss:  0.029773105 \t Eval Accuracy:  99.16%\n##########################################################\nStep:  14300 \t Train Loss:  0.025359388 \t Train Accuracy:  99.22%\nStep:  14300 \t Eval Loss:  0.024011668 \t Eval Accuracy:  99.28%\n##########################################################\nStep:  14400 \t Train Loss:  0.020159569 \t Train Accuracy:  99.35%\nStep:  14400 \t Eval Loss:  0.025135798 \t Eval Accuracy:  99.24%\n##########################################################\nStep:  14500 \t Train Loss:  0.019785652 \t Train Accuracy:  99.43%\nStep:  14500 \t Eval Loss:  0.021222241 \t Eval Accuracy:  99.48%\n##########################################################\nStep:  14600 \t Train Loss:  0.019827398 \t Train Accuracy:  99.48%\nStep:  14600 \t Eval Loss:  0.02704807 \t Eval Accuracy:  99.22%\n##########################################################\nStep:  14700 \t Train Loss:  0.0243477 \t Train Accuracy:  99.24%\nStep:  14700 \t Eval Loss:  0.027104486 \t Eval Accuracy:  99.19%\n##########################################################\nStep:  14800 \t Train Loss:  0.019686885 \t Train Accuracy:  99.39%\nStep:  14800 \t Eval Loss:  0.027268093 \t Eval Accuracy:  99.18%\n##########################################################\nStep:  14900 \t Train Loss:  0.021529978 \t Train Accuracy:  99.32%\nStep:  14900 \t Eval Loss:  0.03156714 \t Eval Accuracy:  99.17%\nCPU times: user 17min 27s, sys: 10min 47s, total: 28min 14s\nWall time: 28min 12s\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt  # Visualization\n\n# Plot loss and accuracy in subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\nax1.set_title('Loss')\nax2.set_title('Accuracy')\n\n\n\nax1.plot(all_train_losses, label='train_loss')\nax1.plot(all_eval_losses, label='eval_loss')\n\nax2.plot(all_train_accuracy, label='train_accuracy')\nax2.plot(all_test_accuracy, label='eval_accuracy')\n\nax1.legend()\nax2.legend()\nplt.show()\nplt.clf()","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:53:16.873871Z","iopub.execute_input":"2024-06-13T10:53:16.874282Z","iopub.status.idle":"2024-06-13T10:53:17.388280Z","shell.execute_reply.started":"2024-06-13T10:53:16.874231Z","shell.execute_reply":"2024-06-13T10:53:17.387330Z"},"trusted":true},"execution_count":38,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1500x500 with 2 Axes>","image/png":"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"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 0 Axes>"},"metadata":{}}]},{"cell_type":"code","source":"@partial(jax.jit, static_argnames=(\"length\"))\ndef generate_text(rng, params, var_params, length):\n    def _scan_generate(carry, _):\n        random_key, context = carry\n        logits = model.apply({'params': params, **var_params}, context, training=False, mutable=['other_variables'])[0]\n        rng, rng_subkey = jax.random.split(random_key)\n        new_token = jax.random.categorical(\n          rng_subkey, logits[:, -n_tokens, :], axis=-1, shape=(1, 1)\n        )\n        context = jnp.concatenate([context[:, 1:], new_token], axis=1)\n        print(context.shape)\n        return (rng, context), new_token\n\n    _, new_tokens = jax.lax.scan(\n    _scan_generate,\n    (rng, jnp.expand_dims(test_data[850:850+block_size], axis=0)),\n    (),\n    length=length,\n    )\n    return new_tokens","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:53:17.389433Z","iopub.execute_input":"2024-06-13T10:53:17.389734Z","iopub.status.idle":"2024-06-13T10:53:17.398531Z","shell.execute_reply.started":"2024-06-13T10:53:17.389709Z","shell.execute_reply":"2024-06-13T10:53:17.397629Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"test_data[850:850+block_size]","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:53:17.399666Z","iopub.execute_input":"2024-06-13T10:53:17.400042Z","iopub.status.idle":"2024-06-13T10:53:17.504588Z","shell.execute_reply.started":"2024-06-13T10:53:17.400017Z","shell.execute_reply":"2024-06-13T10:53:17.503618Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"Array([58, 41,  1, 12, 57, 45, 42, 51, 56,  1, 52, 51,  1, 38,  1, 45, 42,\n       38, 53,  7,  0,  0, 31, 20, 24, 26, 25,  9,  0, 34, 38, 55,  5, 56,\n       57,  1, 57, 45, 52, 58,  1,  5, 44, 38, 46, 51, 56, 57,  1, 12, 57,\n       45, 42, 51, 56, 11,  0,  0, 12, 23, 14, 20, 13, 20], dtype=int32)"},"metadata":{}}]},{"cell_type":"code","source":"new_tokenz = 1000\nkey, subkey = jax.random.split(jax.random.PRNGKey(156))\n# key, subkey = jax.random.split(key)\n# token_gen = generate_text(jnp.zeros((1,block_size)).astype(jnp.int32), new_tokenz, {'params': state.params})\ntoken_gen = generate_text(key, params, var_params, new_tokenz)[:, 0, 0].tolist()\nprint(token_gen)\nprint(decode(token_gen))","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:53:17.505896Z","iopub.execute_input":"2024-06-13T10:53:17.506249Z","iopub.status.idle":"2024-06-13T10:53:33.547796Z","shell.execute_reply.started":"2024-06-13T10:53:17.506214Z","shell.execute_reply":"2024-06-13T10:53:33.546838Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"(1, 64)\n[12, 15, 16, 30, 31, 16, 29, 9, 0, 12, 45, 6, 1, 57, 45, 38, 57, 1, 46, 57, 1, 60, 42, 51, 52, 58, 55, 1, 57, 45, 46, 56, 1, 53, 58, 55, 53, 42, 40, 57, 1, 62, 52, 58, 57, 45, 1, 45, 42, 55, 1, 50, 52, 55, 42, 6, 0, 0, 17, 20, 29, 30, 31, 1, 27, 16, 29, 16, 29, 20, 14, 22, 9, 0, 18, 58, 46, 41, 42, 55, 8, 0, 0, 13, 20, 12, 25, 14, 12, 9, 0, 13, 42, 46, 51, 44, 1, 52, 43, 1, 57, 45, 42, 1, 22, 46, 51, 44, 8, 0, 0, 12, 25, 31, 26, 25, 36, 9, 0, 20, 5, 49, 49, 1, 45, 42, 1, 50, 58, 56, 57, 1, 56, 53, 52, 55, 57, 1, 38, 41, 52, 1, 53, 55, 52, 56, 53, 42, 55, 1, 56, 52, 6, 1, 56, 46, 51, 40, 42, 1, 20, 1, 38, 50, 1, 46, 51, 1, 45, 42, 55, 5, 56, 57, 1, 33, 38, 55, 56, 52, 10, 0, 34, 45, 52, 50, 1, 62, 52, 58, 1, 60, 46, 49, 49, 1, 51, 52, 60, 1, 51, 52, 1, 49, 46, 40, 57, 46, 52, 51, 1, 45, 46, 57, 45, 42, 55, 8, 1, 26, 2, 0, 0, 12, 23, 23, 9, 0, 30, 46, 55, 6, 1, 51, 52, 60, 1, 20, 1, 53, 55, 52, 49, 52, 58, 55, 10, 0, 26, 55, 1, 40, 52, 58, 51, 57, 1, 45, 46, 50, 1, 52, 58, 57, 1, 38, 51, 41, 0, 51, 46, 44, 45, 57, 6, 1, 38, 56, 1, 20, 1, 38, 55, 55, 52, 38, 41, 6, 1, 46, 57, 1, 46, 56, 1, 51, 52, 57, 1, 38, 1, 41, 42, 56, 57, 55, 42, 51, 44, 57, 45, 42, 51, 6, 1, 57, 45, 38, 57, 1, 57, 45, 42, 62, 1, 40, 38, 51, 51, 52, 57, 1, 57, 52, 1, 56, 38, 62, 6, 1, 39, 58, 57, 1, 60, 45, 42, 55, 42, 1, 46, 56, 1, 44, 52, 42, 56, 1, 40, 52, 50, 43, 52, 55, 57, 46, 51, 44, 1, 46, 51, 1, 38, 55, 50, 6, 1, 46, 1, 50, 38, 62, 42, 55, 1, 38, 1, 39, 55, 42, 38, 57, 45, 11, 0, 0, 31, 29, 20, 25, 14, 32, 23, 26, 9, 0, 34, 45, 62, 6, 1, 40, 52, 50, 42, 56, 1, 43, 55, 42, 42, 62, 1, 53, 38, 56, 56, 42, 41, 0, 45, 42, 55, 42, 1, 46, 51, 1, 60, 52, 51, 41, 55, 52, 58, 56, 51, 52, 58, 56, 1, 51, 52, 57, 0, 12, 51, 41, 1, 41, 52, 58, 39, 57, 1, 52, 43, 1, 38, 57, 1, 56, 42, 51, 41, 1, 57, 45, 38, 57, 1, 40, 42, 55, 57, 38, 46, 51, 1, 60, 46, 57, 45, 1, 57, 45, 46, 56, 1, 44, 55, 52, 58, 51, 41, 1, 60, 45, 52, 56, 42, 1, 45, 46, 50, 1, 52, 43, 1, 38, 49, 49, 1, 50, 38, 62, 6, 1, 46, 51, 57, 42, 51, 41, 6, 0, 20, 43, 1, 44, 58, 38, 55, 41, 62, 1, 53, 46, 55, 53, 52, 55, 56, 42, 10, 1, 20, 1, 39, 42, 57, 45, 38, 53, 53, 5, 41, 1, 38, 57, 1, 40, 52, 50, 50, 38, 51, 41, 42, 41, 6, 1, 20, 56, 38, 39, 42, 49, 49, 46, 52, 50, 42, 41, 1, 60, 46, 57, 10, 1, 62, 52, 58, 1, 45, 42, 51, 40, 42, 1, 51, 52, 39, 49, 42, 56, 1, 57, 38, 58, 44, 45, 57, 1, 45, 62, 56, 42, 49, 41, 46, 42, 55, 6, 1, 60, 46, 57, 45, 1, 5, 42, 50, 6, 1, 57, 45, 42, 1, 44, 46, 51, 44, 1, 57, 52, 1, 57, 45, 38, 51, 48, 8, 1, 17, 52, 55, 1, 20, 1, 45, 38, 59, 42, 1, 45, 52, 50, 42, 1, 58, 51, 57, 52, 51, 42, 56, 6, 0, 12, 51, 41, 1, 41, 52, 1, 60, 46, 57, 45, 1, 57, 45, 42, 1, 53, 52, 55, 57, 42, 55, 1, 38, 51, 41, 1, 38, 49, 49, 1, 46, 56, 1, 52, 58, 55, 1, 40, 52, 58, 55, 38, 44, 42, 8, 5, 1, 52, 58, 55, 1, 41, 58, 56, 57, 1, 57, 45, 46, 56, 1, 62, 52, 58, 55, 56, 42, 49, 59, 5, 56, 1, 59, 42, 55, 62, 1, 43, 55, 46, 42, 51, 41, 1, 38, 51, 41, 1, 38, 49, 49, 1, 46, 51, 43, 52, 49, 41, 1, 51, 52, 60, 1, 51, 52, 57, 1, 40, 52, 50, 53, 38, 51, 51, 42, 56, 56, 0, 12, 51, 41, 1, 50, 62, 1, 53, 52, 56, 57, 56, 1, 43, 52, 55, 1, 20, 1, 57, 42, 49, 49, 1, 45, 42, 55, 1, 45, 52, 39, 12, 9, 0, 19, 38, 41, 1, 39, 42, 38, 55, 1, 45, 46, 50, 1, 52, 58, 55, 1, 43, 38, 57, 45, 42, 55, 1, 50, 38, 48, 42, 8, 0, 23, 42, 57, 1, 57, 38, 49, 48, 56, 1, 43, 52, 51, 41, 1, 52, 43, 1, 20, 56, 38, 39, 42, 49, 49, 2, 0, 0, 26, 31, 19, 16, 23, 23, 26, 9, 0, 24, 62, 1, 23, 52, 55, 41, 1, 21, 52, 59, 42, 1, 62, 52, 58, 55, 0, 49, 42, 51, 44, 57, 45, 10, 1, 46, 43, 1, 20, 1, 60, 52, 58, 49, 41, 1, 57, 45, 46, 56, 11, 0, 23, 52, 56, 57, 49, 42, 46, 51, 44, 1, 34, 38, 55, 60, 46, 40, 48, 1, 46, 51, 1, 56, 58, 40, 45, 1, 38, 62, 1, 57, 45, 46, 50, 44, 49, 42, 0, 31, 45, 38, 51, 1, 43, 49, 52, 60, 42, 10]\nADESTER:\nAh, that it wenour this purpect youth her more,\n\nFIRST PERERICK:\nGuider.\n\nBIANCA:\nBeing of the King.\n\nANTONY:\nI'll he must sport ado prosper so, since I am in her'st Varso;\nWhom you will now no liction hither. O!\n\nALL:\nSir, now I prolour;\nOr count him out and\nnight, as I arroad, it is not a destrengthen, that they cannot to say, but where is goes comforting in arm, i mayer a breath?\n\nTRINCULO:\nWhy, comes freey passed\nhere in wondrousnous not\nAnd doubt of at send that certain with this ground whose him of all may, intend,\nIf guardy pirporse; I bethapp'd at commanded, Isabelliomed wit; you hence nobles taught hyseldier, with 'em, the ging to thank. For I have home untones,\nAnd do with the porter and all is our courage.' our dust this yourselv's very friend and all infold now not companness\nAnd my posts for I tell her hobA:\nHad bear him our father make.\nLet talks fond of Isabell!\n\nOTHELLO:\nMy Lord Jove your\nlength; if I would this?\nLostleing Warwick in such ay thimgle\nThan flowe;\n","output_type":"stream"}]},{"cell_type":"code","source":"dsfsdhfgjdg hfdgjdgjgfjhs'####################","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:53:33.549159Z","iopub.execute_input":"2024-06-13T10:53:33.549549Z","iopub.status.idle":"2024-06-13T10:53:33.555314Z","shell.execute_reply.started":"2024-06-13T10:53:33.549515Z","shell.execute_reply":"2024-06-13T10:53:33.554054Z"},"trusted":true},"execution_count":42,"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[42], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    dsfsdhfgjdg hfdgjdgjgfjhs'####################\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 1)\n"],"ename":"SyntaxError","evalue":"unterminated string literal (detected at line 1) (2630675753.py, line 1)","output_type":"error"}]},{"cell_type":"code","source":"len(token_gen)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:53:33.556289Z","iopub.status.idle":"2024-06-13T10:53:33.556777Z","shell.execute_reply.started":"2024-06-13T10:53:33.556548Z","shell.execute_reply":"2024-06-13T10:53:33.556568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx = 882\ntokenizer.decode(test_data[idx:idx+32])","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:53:33.558556Z","iopub.status.idle":"2024-06-13T10:53:33.559076Z","shell.execute_reply.started":"2024-06-13T10:53:33.558778Z","shell.execute_reply":"2024-06-13T10:53:33.558800Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer('bestopleled', return_tensors='np')","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:53:33.560661Z","iopub.status.idle":"2024-06-13T10:53:33.560978Z","shell.execute_reply.started":"2024-06-13T10:53:33.560820Z","shell.execute_reply":"2024-06-13T10:53:33.560834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.decode([1991])","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:53:33.562404Z","iopub.status.idle":"2024-06-13T10:53:33.562715Z","shell.execute_reply.started":"2024-06-13T10:53:33.562560Z","shell.execute_reply":"2024-06-13T10:53:33.562575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params['Dense_12']['kernel'].shape","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:53:33.564479Z","iopub.status.idle":"2024-06-13T10:53:33.564915Z","shell.execute_reply.started":"2024-06-13T10:53:33.564686Z","shell.execute_reply":"2024-06-13T10:53:33.564705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rngk = jax.random.PRNGKey(389)\nxs, ys = get_batch(rngk, train_data)\nprint(xs[0])\nprint(ys[0])","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:53:33.566427Z","iopub.status.idle":"2024-06-13T10:53:33.566771Z","shell.execute_reply.started":"2024-06-13T10:53:33.566606Z","shell.execute_reply":"2024-06-13T10:53:33.566621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logits = model.apply({'params': params, **var_params}, xs[0].reshape((1,64)), training=False, mutable=['other_variables'])[0]\nrng, rng_subkey = jax.random.split(rngk)\nfor pso in range(n_tokens):\n    new_token = jax.random.categorical(\n      rng_subkey, logits[:, -1*(n_tokens-pso), :], axis=-1, shape=(1, 1)\n    )\n    print(new_token)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:53:33.569197Z","iopub.status.idle":"2024-06-13T10:53:33.569628Z","shell.execute_reply.started":"2024-06-13T10:53:33.569406Z","shell.execute_reply":"2024-06-13T10:53:33.569427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_tok = [51,49,46,46,46,52]\nprint(decode(ys[0].tolist()))","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:53:33.570918Z","iopub.status.idle":"2024-06-13T10:53:33.571315Z","shell.execute_reply.started":"2024-06-13T10:53:33.571122Z","shell.execute_reply":"2024-06-13T10:53:33.571141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"act_tk = [60, 43, 50, 57,  1, 47]\nprint(decode(act_tk))","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:53:33.573107Z","iopub.status.idle":"2024-06-13T10:53:33.573523Z","shell.execute_reply.started":"2024-06-13T10:53:33.573316Z","shell.execute_reply":"2024-06-13T10:53:33.573338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jax.nn.standardize(jnp.array([2.0,3.0,4.0]))","metadata":{"id":"Oe_GIDP2HFyt","outputId":"5d3dce16-fcc2-40b9-c49a-00a8c4013ca2","execution":{"iopub.status.busy":"2024-06-13T10:53:33.575038Z","iopub.status.idle":"2024-06-13T10:53:33.575438Z","shell.execute_reply.started":"2024-06-13T10:53:33.575223Z","shell.execute_reply":"2024-06-13T10:53:33.575237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@struct.dataclass\nclass Metrics(metrics.Collection):\n    accuracy: metrics.Accuracy\n    loss: metrics.Average.from_output('loss')","metadata":{"id":"s3nN1jOiHFyu","execution":{"iopub.status.busy":"2024-06-13T10:53:33.576333Z","iopub.status.idle":"2024-06-13T10:53:33.576643Z","shell.execute_reply.started":"2024-06-13T10:53:33.576486Z","shell.execute_reply":"2024-06-13T10:53:33.576499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TrainState(train_state.TrainState):\n    metrics: Metrics\n\ndef create_train_state(module, rng, learning_rate, train_shape):\n    \"\"\"Creates an initial `TrainState`.\"\"\"\n    params = module.init(rng, jnp.ones(train_shape).astype(jnp.int32), \n                         training=False)['params'] # initialize parameters by passing a template image\n    tx = optax.adamw(learning_rate)\n    return TrainState.create(\n      apply_fn=module.apply, params=params, tx=tx,\n      metrics=Metrics.empty(),\n    )","metadata":{"id":"7LLDTSFQHFyu","execution":{"iopub.status.busy":"2024-06-13T10:53:33.578369Z","iopub.status.idle":"2024-06-13T10:53:33.578835Z","shell.execute_reply.started":"2024-06-13T10:53:33.578592Z","shell.execute_reply":"2024-06-13T10:53:33.578613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TrainState.create(","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:53:33.579885Z","iopub.status.idle":"2024-06-13T10:53:33.580348Z","shell.execute_reply.started":"2024-06-13T10:53:33.580093Z","shell.execute_reply":"2024-06-13T10:53:33.580113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@jax.jit\ndef train_step(state, inputs, targets):\n    \"\"\"Train for a single step.\"\"\"\n    def loss_fn(params):\n        logits = state.apply_fn({'params': params}, inputs, training=True, \n                                rngs={\"dropout\": key})[0]\n        loss = optax.softmax_cross_entropy_with_integer_labels(\n            logits=logits, labels=targets).mean()\n        return loss\n    grad_fn = jax.grad(loss_fn)\n    grads = grad_fn(state.params)\n    state = state.apply_gradients(grads=grads)\n    return state","metadata":{"id":"zApWXUDaHFyu","execution":{"iopub.status.busy":"2024-06-13T10:53:33.581943Z","iopub.status.idle":"2024-06-13T10:53:33.582458Z","shell.execute_reply.started":"2024-06-13T10:53:33.582191Z","shell.execute_reply":"2024-06-13T10:53:33.582210Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@jax.jit\ndef compute_metrics(*, state, inputs, targets):\n    logits = state.apply_fn({'params': state.params}, inputs, training=False)[0]\n    loss = optax.softmax_cross_entropy_with_integer_labels(\n        logits=logits, labels=targets).mean()\n    metric_updates = state.metrics.single_from_model_output(\n    logits=logits, labels=targets, loss=loss)\n    metrics = state.metrics.merge(metric_updates)\n    state = state.replace(metrics=metrics)\n    return state","metadata":{"id":"VzukZ4iEHFyv","execution":{"iopub.status.busy":"2024-06-13T10:53:33.583804Z","iopub.status.idle":"2024-06-13T10:53:33.584228Z","shell.execute_reply.started":"2024-06-13T10:53:33.584009Z","shell.execute_reply":"2024-06-13T10:53:33.584027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 10\nlearning_rate = 0.005\ninit_rng = jax.random.key(0)","metadata":{"id":"ehYvMeuNHFyv","execution":{"iopub.status.busy":"2024-06-13T10:53:33.586114Z","iopub.status.idle":"2024-06-13T10:53:33.586573Z","shell.execute_reply.started":"2024-06-13T10:53:33.586341Z","shell.execute_reply":"2024-06-13T10:53:33.586360Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"state = create_train_state(fin_model, init_rng, learning_rate, train_shape)\ndel init_rng  # Must not be used anymore.","metadata":{"id":"D60UHLFHHFyv","execution":{"iopub.status.busy":"2024-06-13T10:53:33.587544Z","iopub.status.idle":"2024-06-13T10:53:33.587980Z","shell.execute_reply.started":"2024-06-13T10:53:33.587749Z","shell.execute_reply":"2024-06-13T10:53:33.587768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics_history = {'train_loss': [],\n                   'train_accuracy': [],\n                   'test_loss': [],\n                   'test_accuracy': []}","metadata":{"id":"Jl-9TlHEHFyv","execution":{"iopub.status.busy":"2024-06-13T10:53:33.589515Z","iopub.status.idle":"2024-06-13T10:53:33.589948Z","shell.execute_reply.started":"2024-06-13T10:53:33.589721Z","shell.execute_reply":"2024-06-13T10:53:33.589739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 442\nkey = jax.random.PRNGKey(SEED)\nloss = 10\ncounter = 0\n# for step in tqdm(range(max_iters)): # increase number of steps for good results...\nwhile counter==max_iters or loss > 1.0:\n\n      # sample a batch of data\n    xb, yb = get_batch(key, train_data)\n    state = train_step(state, xb, yb)\n    state = compute_metrics(state=state, inputs=xb, targets=yb)\n\n    key = (jax.random.split(key)[0])\n\n    if step == 0 or (step+1) % 100 == 0: # one training epoch has passed\n        for metric,value in state.metrics.compute().items(): # compute metrics\n            metrics_history[f'train_{metric}'].append(value) # record metrics\n        state = state.replace(metrics=state.metrics.empty()) # reset train_metrics for next training epoch\n\n        # Compute metrics on the test set after each training epoch\n        test_state = state\n        x_test, y_test = get_batch(key, test_data)\n    #     for test_batch in test_ds.as_numpy_iterator():\n        test_state = compute_metrics(state=test_state, inputs=x_test, targets=y_test)\n\n        for metric,value in test_state.metrics.compute().items():\n            metrics_history[f'test_{metric}'].append(value)\n\n        print(f\"train epoch: {(step+1)}, \"\n              f\"loss: {metrics_history['train_loss'][-1]}, \"\n              f\"accuracy: {metrics_history['train_accuracy'][-1] * 100}\")\n        print(f\"test epoch: {(step+1) }, \"\n          f\"loss: {metrics_history['test_loss'][-1]}, \"\n          f\"accuracy: {metrics_history['test_accuracy'][-1] * 100}\")","metadata":{"id":"CaNt9JazHFyw","outputId":"ba447ddf-9940-44a6-f4b2-d27ed78a88c2","execution":{"iopub.status.busy":"2024-06-13T10:53:33.591143Z","iopub.status.idle":"2024-06-13T10:53:33.591604Z","shell.execute_reply.started":"2024-06-13T10:53:33.591372Z","shell.execute_reply":"2024-06-13T10:53:33.591391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt  # Visualization\n\n# Plot loss and accuracy in subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\nax1.set_title('Loss')\nax2.set_title('Accuracy')\nfor dataset in ('train','test'):\n    ax1.plot(metrics_history[f'{dataset}_loss'], label=f'{dataset}_loss')\n    ax2.plot(metrics_history[f'{dataset}_accuracy'], label=f'{dataset}_accuracy')\nax1.legend()\nax2.legend()\nplt.show()\nplt.clf()","metadata":{"id":"Y40JGx1YHFyw","execution":{"iopub.status.busy":"2024-06-13T10:53:33.592999Z","iopub.status.idle":"2024-06-13T10:53:33.593462Z","shell.execute_reply.started":"2024-06-13T10:53:33.593208Z","shell.execute_reply":"2024-06-13T10:53:33.593227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nlogits = fin_model.apply(fin_params, xb, training=False)[0]\nloss = optax.softmax_cross_entropy_with_integer_labels(\n            logits=logits, labels=yb).mean()\n\nprint(loss)","metadata":{"id":"7pJlFXpVHFyw","execution":{"iopub.status.busy":"2024-06-13T10:53:33.594926Z","iopub.status.idle":"2024-06-13T10:53:33.595257Z","shell.execute_reply.started":"2024-06-13T10:53:33.595095Z","shell.execute_reply":"2024-06-13T10:53:33.595108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def generate_text(idx, max_new_tokens, params):\n# # idx is (B, T) array of indices in the current context\n#     for i in range(max_new_tokens):\n#         # crop idx to the last block_size tokens\n#         idx_cond = idx[:, -block_size:]\n#         # get the predictions\n#         logits = fin_model.apply(params, idx_cond)\n#         # focus only on the last time step\n#         logits = logits[:, -1, :] # becomes (B, C)\n\n#         if i == 0:\n#             rng, rng_subkey = jax.random.split(jax.random.PRNGKey(12))\n#         else:\n#             rng, rng_subkey = jax.random.split(rng)\n\n#         idx_next = jax.random.categorical(rng_subkey, logits, axis=-1, shape=(1, 1)) # (B, 1)\n\n\n#         # append sampled index to the running sequence\n#         idx = jnp.concatenate([idx, idx_next], axis=-1) # (B, T+1)\n\n#     return idx","metadata":{"id":"9d28o-dTHFyx","execution":{"iopub.status.busy":"2024-06-13T10:53:33.597106Z","iopub.status.idle":"2024-06-13T10:53:33.597518Z","shell.execute_reply.started":"2024-06-13T10:53:33.597335Z","shell.execute_reply":"2024-06-13T10:53:33.597358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@partial(jax.jit, static_argnames=(\"self\", \"length\"))\ndef generate_text(rng, params, length):\n    def _scan_generate(carry, _):\n        random_key, context = carry\n        logits = fin_model.apply(params, context, training=False)[0]\n        rng, rng_subkey = jax.random.split(random_key)\n        new_token = jax.random.categorical(\n          rng_subkey, logits[:, -1, :], axis=-1, shape=(1, 1)\n        )\n        context = jnp.concatenate([context[:, 1:], new_token], axis=1)\n        return (rng, context), new_token\n\n    _, new_tokens = jax.lax.scan(\n    _scan_generate,\n    (rng, jnp.zeros((1, block_size), dtype=jnp.int32)),\n    (),\n    length=length,\n    )\n    return new_tokens","metadata":{"id":"WB0og7pAHFyx","execution":{"iopub.status.busy":"2024-06-13T10:53:33.599114Z","iopub.status.idle":"2024-06-13T10:53:33.599571Z","shell.execute_reply.started":"2024-06-13T10:53:33.599344Z","shell.execute_reply":"2024-06-13T10:53:33.599363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_tokenz = 1000\nkey, subkey = jax.random.split(jax.random.PRNGKey(156))\n# key, subkey = jax.random.split(key)\n# token_gen = generate_text(jnp.zeros((1,block_size)).astype(jnp.int32), new_tokenz, {'params': state.params})\ntoken_gen = generate_text(key, {'params': state.params}, new_tokenz)[:, 0, 0].tolist()\nprint(token_gen)\nprint(decode(token_gen))","metadata":{"id":"50Vpg2lEHFyx","execution":{"iopub.status.busy":"2024-06-13T10:53:33.601077Z","iopub.status.idle":"2024-06-13T10:53:33.601542Z","shell.execute_reply.started":"2024-06-13T10:53:33.601309Z","shell.execute_reply":"2024-06-13T10:53:33.601329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sdgh  fs","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:53:33.602783Z","iopub.status.idle":"2024-06-13T10:53:33.603217Z","shell.execute_reply.started":"2024-06-13T10:53:33.602988Z","shell.execute_reply":"2024-06-13T10:53:33.603007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"state.params","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:53:33.604923Z","iopub.status.idle":"2024-06-13T10:53:33.605377Z","shell.execute_reply.started":"2024-06-13T10:53:33.605129Z","shell.execute_reply":"2024-06-13T10:53:33.605147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install mamba-ssm","metadata":{"id":"MOw_xjbrHFy0","execution":{"iopub.status.busy":"2024-06-13T10:53:33.607359Z","iopub.status.idle":"2024-06-13T10:53:33.607792Z","shell.execute_reply.started":"2024-06-13T10:53:33.607563Z","shell.execute_reply":"2024-06-13T10:53:33.607581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ones = lambda *size: torch.ones(*size).float().cuda()\nzeros = lambda *size: torch.zeros(*size).float().cuda()\narange = lambda n: torch.arange(n).float().cuda()\nrand = lambda size: torch.rand(*size).abs().float().cuda()\n\ndef create_torch(S = 128, Ba = 2, D = 4, N = 4):\n    x = rand((Ba, 1, D, S))\n    a = -ones((Ba, N, D, 1))\n    b = ones((Ba, N, 1, S)) * 0.1\n    c = rand((Ba, N, 1, S)) * 0.1\n    delta = rand((Ba, 1, D, S)) * 0.1\n    return x, a, b, c, delta","metadata":{"id":"W_PAnYcEOR22","execution":{"iopub.status.busy":"2024-06-13T10:53:33.609048Z","iopub.status.idle":"2024-06-13T10:53:33.609416Z","shell.execute_reply.started":"2024-06-13T10:53:33.609218Z","shell.execute_reply":"2024-06-13T10:53:33.609232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import selective_scan_cuda\n\nxx, aa, bb, cc, ddelta = create_torch()\ny_from_repo = selective_scan_cuda.fwd(xx.squeeze(1), ddelta.squeeze(1), aa[0].squeeze(-1).T, bb.squeeze(-2)[:, None, :, :], cc.squeeze(-2)[:, None, :, :], None, None, None, False)\ny_from_repo","metadata":{"id":"ykh4GTvtOrak","execution":{"iopub.status.busy":"2024-06-13T10:53:33.610849Z","iopub.status.idle":"2024-06-13T10:53:33.611181Z","shell.execute_reply.started":"2024-06-13T10:53:33.611019Z","shell.execute_reply":"2024-06-13T10:53:33.611032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def discretize(a, b, delta):\n    da = delta * a\n    a_ = jnp.exp(da)\n    b_ = b * delta\n    return a_, b_\n\ndef ssm(x, a, b, c, delta):\n    \"Jax Implementation\"\n    y = []\n    h = 0\n    a_, b_ = discretize(a, b, delta)\n    for k in range(x.shape[-1]):\n        h = a_[..., k] * h + b_[..., k] * x[..., k]\n        y.append((c[..., k] * h).sum(1, keepdims=True))\n    return h, jnp.stack(y, -1)\n","metadata":{"id":"NEdG1yPNOtxU","execution":{"iopub.status.busy":"2024-06-13T10:53:33.612075Z","iopub.status.idle":"2024-06-13T10:53:33.612413Z","shell.execute_reply.started":"2024-06-13T10:53:33.612224Z","shell.execute_reply":"2024-06-13T10:53:33.612237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_, y_ = ssm(xx.cpu().numpy(), aa.cpu().numpy(), bb.cpu().numpy(), cc.cpu().numpy(), ddelta.cpu().numpy())","metadata":{"id":"GEjNcZSZPIp_","execution":{"iopub.status.busy":"2024-06-13T10:53:33.613217Z","iopub.status.idle":"2024-06-13T10:53:33.613574Z","shell.execute_reply.started":"2024-06-13T10:53:33.613413Z","shell.execute_reply":"2024-06-13T10:53:33.613427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"tWlqZZOmPnYk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from mamba_ssm import Mamba as Mamba_T\ntorch_mamba = Mamba_T(\n      # This module uses roughly 3 * expand * d_model^2 parameters\n      d_model=n_embd, # Model dimension d_model\n      d_state=16,  # SSM state expansion factor\n      d_conv=4,    # Local convolution width\n      expand=2,    # Block expansion factor\n)","metadata":{"id":"5RHAE_I1Pql9","execution":{"iopub.status.busy":"2024-06-13T10:53:33.614838Z","iopub.status.idle":"2024-06-13T10:53:33.615139Z","shell.execute_reply.started":"2024-06-13T10:53:33.614989Z","shell.execute_reply":"2024-06-13T10:53:33.615002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xm = x = rand((1, 1, n_embd, 32))\nxm.shape","metadata":{"id":"l9zw_M-USrDt","execution":{"iopub.status.busy":"2024-06-13T10:53:33.616176Z","iopub.status.idle":"2024-06-13T10:53:33.616524Z","shell.execute_reply.started":"2024-06-13T10:53:33.616358Z","shell.execute_reply":"2024-06-13T10:53:33.616372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch_mamba(xm.squeeze(1))","metadata":{"id":"gGmA2EWlTCo0","execution":{"iopub.status.busy":"2024-06-13T10:53:33.617386Z","iopub.status.idle":"2024-06-13T10:53:33.617726Z","shell.execute_reply.started":"2024-06-13T10:53:33.617555Z","shell.execute_reply":"2024-06-13T10:53:33.617575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch_mamba.in_proj","metadata":{"id":"73ek9mx9UBBl","execution":{"iopub.status.busy":"2024-06-13T10:53:33.619373Z","iopub.status.idle":"2024-06-13T10:53:33.619688Z","shell.execute_reply.started":"2024-06-13T10:53:33.619529Z","shell.execute_reply":"2024-06-13T10:53:33.619542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import CLIPTokenizer\ntokenizer_1 = CLIPTokenizer.from_pretrained('openai/clip-vit-base-patch32')","metadata":{"id":"P3l_ssIYbiYT","execution":{"iopub.status.busy":"2024-06-13T10:53:33.620765Z","iopub.status.idle":"2024-06-13T10:53:33.621090Z","shell.execute_reply.started":"2024-06-13T10:53:33.620927Z","shell.execute_reply":"2024-06-13T10:53:33.620942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenise_prompts(prompt):\n    inputs = []\n    for tokenizer in [tokenizer_1, tokenizer_2]:\n        text_inputs = tokenizer(\n            positive_prompt,\n            padding=\"max_length\",\n            max_length=tokenizer.model_max_length,\n            truncation=True,\n            return_tensors=\"np\",\n        )\n        inputs.append(text_inputs.input_ids)\n    return jnp.stack(inputs, axis=1)","metadata":{"id":"-X7hXQRMZhl3","execution":{"iopub.status.busy":"2024-06-13T10:53:33.622287Z","iopub.status.idle":"2024-06-13T10:53:33.622613Z","shell.execute_reply.started":"2024-06-13T10:53:33.622451Z","shell.execute_reply":"2024-06-13T10:53:33.622465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CNN(nn.Module):\n    \"\"\"A simple CNN model.\"\"\"\n\n    @nn.compact\n    def __call__(self, x):\n        x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n        print(x.shape)\n        x = nn.relu(x)\n        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n        print(x.shape)\n        x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n        print(x.shape)\n        x = nn.relu(x)\n        x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n        print(x.shape)\n        x = x.reshape((x.shape[0], -1))  # flatten\n        print(x.shape)\n        x = nn.Dense(features=256)(x)\n        print(x.shape)\n        x = nn.relu(x)\n        x = nn.Dense(features=10)(x)\n        print(x.shape)\n        return x","metadata":{"id":"rJhKQ_Oua9Gy","execution":{"iopub.status.busy":"2024-06-13T10:53:33.624082Z","iopub.status.idle":"2024-06-13T10:53:33.624546Z","shell.execute_reply.started":"2024-06-13T10:53:33.624314Z","shell.execute_reply":"2024-06-13T10:53:33.624334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rng = jax.random.PRNGKey(20)\ncnn = CNN()\nparams = cnn.init(rng, jnp.ones([1, 28, 28, 1]))['params']","metadata":{"id":"mzkoYrSVkoJj","execution":{"iopub.status.busy":"2024-06-13T10:53:33.626053Z","iopub.status.idle":"2024-06-13T10:53:33.626516Z","shell.execute_reply.started":"2024-06-13T10:53:33.626259Z","shell.execute_reply":"2024-06-13T10:53:33.626302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Conv1d(nn.Module):\n    \"\"\"A simple 1D CNN model.\"\"\"\n\n    @nn.compact\n    def __call__(self, x):\n        x = nn.Embed(65, 256)(x) + nn.Embed(64, 256)(jnp.arange(64))\n        print(x.shape)\n#         x = nn.Dense(features=512)(x)\n#         print(x.shape)\n        x = nn.Conv(features=256, kernel_size=3, padding=1)(x)\n        print(x.shape)\n        x = nn.avg_pool(x, window_shape=(2,), strides=(2,))\n        print(x.shape)\n        x = nn.Conv(features=512, kernel_size=3, padding=1)(x)\n        print(x.shape)\n        x = nn.avg_pool(x, window_shape=(2,), strides=(2,))\n        print(x.shape)\n        x = nn.ConvTranspose(features=512, kernel_size=2, padding='same')(x)\n        print(x.shape)\n        x = nn.Dense(features=65)(x)\n        print(x.shape)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:53:33.628073Z","iopub.status.idle":"2024-06-13T10:53:33.628534Z","shell.execute_reply.started":"2024-06-13T10:53:33.628305Z","shell.execute_reply":"2024-06-13T10:53:33.628325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rng = jax.random.PRNGKey(204)\nmycon1d = Conv1d()\nparams = mycon1d.init(rng, jnp.ones((1, 64), dtype=jnp.int32))['params']","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:53:33.629564Z","iopub.status.idle":"2024-06-13T10:53:33.629894Z","shell.execute_reply.started":"2024-06-13T10:53:33.629732Z","shell.execute_reply":"2024-06-13T10:53:33.629746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}